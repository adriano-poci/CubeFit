This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
docs/
  CubeFit.html
  CubeFit.md
  CubeFit.pdf
NGC4365/
  figures/
    best_C0207_spax00039.png
    best_C0207_spax00042.png
    best_C0207_spax00051.png
    best_C0207_spax00055.png
    best_C0207_spax01032.png
    best_C0207_spax01058.png
    best_C0207_spax01079.png
    best_C0207_spax01171.png
    best_C0207_spax01259.png
    best_C0207_spax01310.png
    chi2_hist.png
    fit_live_fits.png
    fit_live.png
    modelCube_207_ii_SN100_BaSTI_KB1.30_LW_12.png
    nnlsPatchUsage.png
    orbitSFH_207_ii_SN100_BaSTI_KB1.30_LW_12.png
    patch_spax00000.png
    patch_spax00001.png
    patch_spax00002.png
    patch_spax00003.png
    patch_spax00004.png
    patch_spax00005.png
    patch_spax00006.png
    patch_spax00007.png
    patch_spax00008.png
    patch_spax00009.png
    patch_spax00010.png
    patch_spax00011.png
    patch_spax00012.png
    patch_spax00013.png
    patch_spax00014.png
    patch_spax00015.png
    patch_spax00016.png
    patch_spax00017.png
    patch_spax00018.png
    patch_spax00019.png
    patch_spax00020.png
    patch_spax00021.png
    patch_spax00022.png
    patch_spax00023.png
    patch_spax00024.png
    patch_spax00025.png
    patch_spax00026.png
    patch_spax00027.png
    patch_spax00028.png
    patch_spax00029.png
    patch_spax00030.png
    patch_spax00031.png
    patch_spax00045.png
    patch_spax00090.png
    patch_spax00135.png
    patch_spax00181.png
    patch_spax00226.png
    patch_spax00271.png
    patch_spax00317.png
    patch_spax00362.png
    patch_spax00407.png
    patch_spax00452.png
    patch_spax00498.png
    patch_spax00543.png
    patch_spax00588.png
    patch_spax00634.png
    patch_spax00679.png
    patch_spax00724.png
    patch_spax00769.png
    patch_spax00815.png
    patch_spax00860.png
    patch_spax00905.png
    patch_spax00951.png
    patch_spax00996.png
    patch_spax01041.png
    patch_spax01086.png
    patch_spax01132.png
    patch_spax01177.png
    patch_spax01222.png
    patch_spax01268.png
    patch_spax01313.png
    patch_spax01358.png
    patch_spax01403.png
    patch_spax01449.png
    patch_spax01494.png
    patch_spax01539.png
    patch_spax01585.png
    patch_spax01630.png
    patch_spax01675.png
    patch_spax01720.png
    patch_spax01766.png
    patch_spax01811.png
    patch_spax01856.png
    patch_spax01902.png
    patch_spax01947.png
    patch_spax01992.png
    patch_spax02037.png
    patch_spax02083.png
    patch_spax02128.png
    patch_spax02173.png
    patch_spax02219.png
    patch_spax02264.png
    patch_spax02309.png
    patch_spax02354.png
    patch_spax02400.png
    patch_spax02445.png
    patch_spax02490.png
    patch_spax02536.png
    patch_spax02581.png
    patch_spax02626.png
    patch_spax02671.png
    patch_spax02717.png
    patch_spax02762.png
    patch_spax02807.png
    patch_spax02853.png
    prefit_overlay_from_models_C207.png
    worst_C0207_spax00759.png
    worst_C0207_spax00802.png
    worst_C0207_spax00806.png
    worst_C0207_spax02830.png
    worst_C0207_spax02840.png
    worst_C0207_spax02848.png
    worst_C0207_spax02850.png
    worst_C0207_spax02851.png
    worst_C0207_spax02852.png
    worst_C0207_spax02853.png
    xcp_bar.png
__init__.py
COMMIT_MESSAGE.txt
cube_debug.py
cube_utils.py
CubeFit_zarr.tar.gz
diag_losvd_models.py
environment.yml
fit_tracker.py
hdf5_manager.py
hypercube_builder.py
hypercube_reader.py
kaczmarz_solver_cchunk_mp_nnls.old.py
kaczmarz_solver_cchunk_mp_nnls.py
kz_0All.sh
kz_1Gen.sh
kz_2Fit.sh
kz_3Rio.sh
kz_addqueue.sh
kz_fitSpec.py
kz_initNGC4365.py
kz_rio.py
kz_run.py
live_fit_dashboard.py
logger.py
Makefile
mkdocs.yml
nnls_patch.py
patch.patch
pipeline_runner.py
plotting.py
pre_solve_check.py
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="kaczmarz_solver_cchunk_mp_nnls.old.py">
# -*- coding: utf-8 -*-
r"""
    kaczmarz_solver_cchunk_mp_nnls.py
    Adriano Poci
    University of Oxford
    2025

    Platforms
    ---------
    Unix, Windows

    Synopsis
    --------
    Modestly-MP, chunk-friendly solver aligned to /HyperCube/models chunking.
    `x` is 1-D (length C*P) in/out. Warm start handled by caller; we accept `x0`.
    `orbit_weights` is accepted (optional); free fit if None

    Authors
    -------
    Adriano Poci <adriano.poci@physics.ox.ac.uk>

History
-------
v1.0:   Fixed bug in computing `nprocs`;
        Wrapped entire `solve_global_kaczmarz_cchunk_mp` in `try/except`. 4
            December 2025
v1.1:   Added column-flux scaling bypass (`cp_flux_ref=None`). 5 December 2025
v1.2:   Experimenting with RMSE cap. 11 December 2025
v1.3:   Introduced L2 into Kaczmarz solving to be consistent with NNLS
            initilisation;
        Disabled buggy `w_band` which was implemented incorrectly. 12 December
            2025
v1.4:   Use the seed vector as a numerical prior during the Kaczmarz solving;
        Implement global RMSE evaluation, and keep only globally-best solution;
        Optionally disable seed prior. 13 December 2025
v1.5:   Use RMSE proxy as guard for epoch solution. 15 December 2025
v2.0:   Implemented global Kaczmarz gradient instead of per-tile updates; added
            `_worker_tile_global_grad_band` and
            `solve_global_kaczmarz_global_step_mp`. 16 December 2025
v2.1:   Added tiny-column freeze inside `_worker_tile_global_grad_band`. 18
            December 2025
v2.2:   Consolidated two tiny-column freeze env var names into one;
        Added fairer max-tile rather than bias to brighter spaxels. 25 December
            2025
v2.3:   Stripped global Kaczmarz solver diagnostics to single gradient and NNLS
            constraint. 26 December 2025
v2.4:   Pre-check gradient before each epoch;
        Pre-check RMSE proxy before each epoch to allow for early exit. 28
            December 2025
v2.5:   Implemented backtracking and step-size reduction based on RMSE proxy. 29
            December 2025
v2.6:   Replaced expensive backtracking RMSE evaluations with O(1) quadratic
            coefficients. 30 December 2025
"""

from __future__ import annotations, print_function

import os, sys, traceback
import math
import time
from dataclasses import dataclass
from typing import Iterable, Tuple, Optional, List
from contextlib import contextmanager

import multiprocessing as mp
import numpy as np
import h5py
from tqdm.auto import tqdm

from CubeFit.hdf5_manager import open_h5
from CubeFit.hypercube_builder import read_global_column_energy
from CubeFit import cube_utils as cu

# ----------------------------- Config ---------------------------------

@dataclass
class MPConfig:
    epochs: int = 1
    lr: float = 0.25
    project_nonneg: bool = True
    processes: int = 2              # modest parallelism (2–4 recommended)
    blas_threads: int = 8           # per-process BLAS threads
    apply_mask: bool = True
    # HDF5 *dataset* chunk cache (not RDCC): keep local & harmless
    dset_slots: int = 1_000_003
    dset_bytes: int = 256 * 1024**2
    dset_w0: float = 0.90
    s_tile_override: Optional[int] = None
    pixels_per_aperture: int = 256
    max_tiles: Optional[int] = None

# ------------------------- Norm mode guard ----------------------------

def _assert_norm_mode(h5_path: str, expect: Optional[str] = None) -> str:
    r"""
    Read /HyperCube/norm.mode and optionally assert a specific mode.

    Parameters
    ----------
    h5_path : str
        Path to HDF5 file.
    expect : str, optional
        If given ('data' or 'model'), raise if the stored mode differs.

    Returns
    -------
    str
        The stored normalization mode ('data' or 'model').

    Exceptions
    ----------
    RuntimeError
        If the attribute is missing or does not match `expect`.
    """
    with open_h5(h5_path, role="reader") as f:
        g = f["/HyperCube"]
        mode = str(g.attrs.get("norm.mode", "model")).lower()
    if expect and mode != expect:
        raise RuntimeError(
            f"Hypercube is in norm.mode='{mode}', but solver expects "
            f"'{expect}'. Convert first with convert_hypercube_norm(...)."
        )
    if mode not in ("data", "model"):
        raise RuntimeError(f"Unexpected norm.mode='{mode}'.")
    return mode

# ---------------------- Small pool utilities --------------------------

def _pool_ping() -> int:
    return 1

def _pool_ok(pool, timeout: float = 5.0) -> bool:
    """
    Returns True if a trivial task round-trips within `timeout`.
    If it times out or raises, the pool is considered unhealthy.
    """
    try:
        res = pool.apply_async(_pool_ping)
        return res.get(timeout=timeout) == 1
    except Exception:
        return False

def _worker_init(blas_threads: int) -> None:
    os.environ["OMP_NUM_THREADS"] = str(blas_threads)
    os.environ["OPENBLAS_NUM_THREADS"] = str(blas_threads)
    os.environ["MKL_NUM_THREADS"] = str(blas_threads)
    os.environ["NUMEXPR_NUM_THREADS"] = str(np.max((1, blas_threads // 2)))
    os.environ.setdefault("KMP_INIT_AT_FORK", "FALSE")

# ---------------------- Optional shift diagnostic ---------------------

def _xcorr_int_shift(a: np.ndarray, b: np.ndarray) -> int:
    """
    Integer-lag that best aligns b to a by full cross correlation.
    Positive → b is to the right.
    """
    aa = np.asarray(a, np.float64).ravel()
    bb = np.asarray(b, np.float64).ravel()
    n = int(aa.size)
    fa = np.fft.rfft(aa, n=2*n)
    fb = np.fft.rfft(bb, n=2*n)
    cc = np.fft.irfft(fa * np.conj(fb))
    j = int(np.argmax(cc))
    if j > n:
        j -= 2*n
    return j

# ------------------------------------------------------------------------------

def _choose_tiles_fair_spread(
    s_ranges: list[tuple[int, int]],
    k: int,
    seed: int = 12345,
) -> list[tuple[int, int]]:
    """
    Pick k tile ranges with a roughly uniform spread over the spatial index.

    This avoids brightness bias: it sorts by s0 and stratifies the list into k bins,
    selecting one tile per bin (random within each bin).

    Parameters
    ----------
    s_ranges
        List of (s0, s1) tile ranges.
    k
        Number of tiles to select.
    seed
        RNG seed.

    Returns
    -------
    list[tuple[int, int]]
        Selected tile ranges (k or fewer if s_ranges is smaller).
    """
    if k <= 0 or not s_ranges:
        return []
    if k >= len(s_ranges):
        return list(s_ranges)

    s_sorted = sorted(s_ranges, key=lambda t: int(t[0]))
    n = len(s_sorted)
    k = min(k, n)

    rng = np.random.default_rng(seed)
    edges = np.linspace(0, n, k + 1, dtype=int)

    out: list[tuple[int, int]] = []
    for i in range(k):
        a = int(edges[i])
        b = int(edges[i + 1])
        if b <= a:
            idx = a
        else:
            idx = int(rng.integers(a, b))
        out.append(s_sorted[idx])

    return out

# ------------------------------------------------------------------------------

def _safe_scalar_rmse(val: float, label: str) -> float:
    """
    Ensure a scalar RMSE used for epoch comparison is finite.

    If 'val' is NaN, Inf, or negative, print a warning and return +inf
    so that this value will never be considered an improvement over a
    finite best RMSE.
    """
    if not np.isfinite(val) or val < 0.0:
        print(
            f"[Kaczmarz-MP] WARNING: {label} RMSE={val!r} is non-finite "
            f"or negative; treating as +inf.",
            flush=True,
        )
        return float("inf")
    return float(val)

# ------------------------------------------------------------------------------

def _compute_global_rmse(
    h5_path: str,
    x_CP: np.ndarray,
    *,
    s_ranges: list[tuple[int, int]],
    keep_idx: np.ndarray | None,
    w_lam_sqrt: np.ndarray | None,
    cp_flux_ref: np.ndarray | None,
    inv_cp_flux_ref: np.ndarray | None,
    dset_slots: int,
    dset_bytes: int,
    dset_w0: float,
    weighted: bool = True,
    show_progress: bool = True,
) -> float:
    """
    Compute the global RMSE over the full (masked) cube for a given x_CP.

    This evaluates the same least–squares objective as the Kaczmarz solver
    in a single streaming pass over the cube. It is optimized for the common
    case cp_flux_ref is None (no column–flux normalization).

    Parameters
    ----------
    h5_path : str
        Path to the HDF5 file containing /DataCube and /HyperCube/models.
    x_CP : ndarray, shape (C, P)
        Current solution in the solver's internal basis. If cp_flux_ref is
        not None, this is the normalized basis used by Kaczmarz; otherwise
        it is in the physical basis.
    s_ranges : list of (int, int)
        Spatial tile ranges covering the full spaxel axis S. Should match
        the tiling used by the solver.
    keep_idx : ndarray or None
        Wavelength indices that are kept after applying the mask. If None,
        all wavelengths are used.
    w_lam_sqrt : ndarray or None
        sqrt(λ-weights) on the masked grid. If weighted=True and this is
        not None, residuals are multiplied by w_lam_sqrt before forming
        ||R||^2.
    cp_flux_ref, inv_cp_flux_ref : ndarray or None
        Column–flux scaling arrays of shape (C, P). If cp_flux_ref is None,
        no column-normalization is assumed and x_CP is used directly.
    dset_slots, dset_bytes, dset_w0 : int
        Chunk-cache parameters for /HyperCube/models.
    weighted : bool, optional
        If True (default) and w_lam_sqrt is provided, compute a λ-weighted
        RMSE. If False, compute an unweighted RMSE.
    show_progress : bool, optional
        If True, wrap the tile loop in a tqdm progress bar.

    Returns
    -------
    rmse : float
        Global (optionally weighted) RMSE over all spaxels and masked
        wavelengths.
    """
    # Normalize inputs
    keep_idx = None if keep_idx is None else np.asarray(
        keep_idx, dtype=np.int64
    )
    use_weight = bool(weighted and (w_lam_sqrt is not None))

    with open_h5(h5_path, role="reader") as f:
        DC = f["/DataCube"]           # (S, L)
        M  = f["/HyperCube/models"]   # (S, C, P, L)
        try:
            M.id.set_chunk_cache(dset_slots, dset_bytes, dset_w0)
        except Exception:
            pass

        S, L = map(int, DC.shape)
        _, C, P, Lm = map(int, M.shape)
        if Lm != L:
            raise RuntimeError(
                f"_compute_global_rmse: models L={Lm} vs data L={L}"
            )

        if x_CP.shape != (C, P):
            raise ValueError(
                f"_compute_global_rmse: x_CP shape {x_CP.shape} "
                f"!= (C,P)=({C},{P})"
            )

        # Push any column normalization onto x_CP once; in your current
        # runs cp_flux_ref is None, so this is just a cheap copy.
        if inv_cp_flux_ref is not None:
            x_eff = (
                np.asarray(x_CP, np.float64, order="C")
                * np.asarray(inv_cp_flux_ref, np.float64, order="C")
            )
        else:
            x_eff = np.asarray(x_CP, np.float64, order="C")

        if keep_idx is None:
            Lk = L
        else:
            Lk = int(keep_idx.size)

        if use_weight:
            wvec = np.asarray(w_lam_sqrt, np.float64).ravel()
            if wvec.size != Lk:
                raise RuntimeError(
                    f"_compute_global_rmse: w_lam_sqrt length "
                    f"{wvec.size} != Lk={Lk}"
                )
            # Harden λ-weights once
            if not np.all(np.isfinite(wvec)):
                print(
                    "[RMSE] WARNING: non-finite λ-weights detected; "
                    "cleaning with nan_to_num.",
                    flush=True,
                )
                wvec = np.nan_to_num(
                    wvec, nan=0.0, posinf=0.0, neginf=0.0
                )
        else:
            wvec = None

        num = 0.0
        den = 0  # scalar residual count

        iterator = s_ranges
        pbar = None
        if show_progress:
            pbar = tqdm(
                s_ranges,
                total=len(s_ranges),
                desc="[Kaczmarz-MP] global RMSE",
                mininterval=1.0,
                dynamic_ncols=True,
            )
            iterator = pbar

        for (s0, s1) in iterator:
            s0 = int(s0)
            s1 = int(s1)
            if s0 >= s1:
                continue
            Sblk = s1 - s0

            # Data slice
            Y = np.asarray(DC[s0:s1, :], np.float64, order="C")
            if keep_idx is not None:
                Y = Y[:, keep_idx]  # (Sblk, Lk)

            # Clean any non-finite data just in case
            if not np.all(np.isfinite(Y)):
                Y = np.nan_to_num(
                    Y, nan=0.0, posinf=0.0, neginf=0.0, copy=False
                )

            if wvec is not None:
                Yw = Y * wvec[None, :]
            else:
                Yw = Y

            # Model slice, weighted or unweighted
            yhat_w = np.zeros((Sblk, Lk), dtype=np.float64)

            for c in range(C):
                A = np.asarray(
                    M[s0:s1, c, :, :], dtype=np.float32, order="C"
                )  # (Sblk, P, L)
                if keep_idx is not None:
                    A = A[:, :, keep_idx]  # (Sblk, P, Lk)

                A = A.astype(np.float64, copy=False)
                if wvec is not None:
                    A = A * wvec[None, None, :]  # λ-weights

                # Clean design slice if needed
                if not np.all(np.isfinite(A)):
                    A = np.nan_to_num(
                        A, nan=0.0, posinf=0.0, neginf=0.0, copy=False
                    )

                xc = np.asarray(x_eff[c, :], np.float64, copy=False)

                # yhat_w += Σ_p xc[p] * A[:, p, :]
                yhat_w += np.tensordot(xc, A, axes=(0, 1))

            Rw = Yw - yhat_w

            # Final guard: ensure residuals are finite before accumulation
            if not np.all(np.isfinite(Rw)):
                Rw = np.nan_to_num(
                    Rw, nan=0.0, posinf=0.0, neginf=0.0, copy=False
                )

            num += float(np.sum(Rw * Rw))
            den += int(Rw.size)

            if pbar is not None:
                pbar.update(1)

        if pbar is not None:
            pbar.close()

    if (not np.isfinite(num)) or (not np.isfinite(den)) or (den <= 0):
        print(
            "[RMSE] WARNING: aggregate num/den non-finite or den<=0; "
            "returning +inf RMSE.",
            flush=True,
        )
        return float("inf")

    return float(math.sqrt(num / float(den)))

# ------------------------------------------------------------------------------

def rmse_proxy_subset(
    h5_path,
    x_CP,
    tile_ranges,
    keep_idx,
    inv_cp_flux_ref,
    w_lam_sqrt,
):
    ssq = 0.0
    n = 0

    with open_h5(h5_path, role="reader") as f:
        DC = f["/DataCube"]
        M  = f["/HyperCube/models"]

        for (s0, s1) in tile_ranges:
            Y = np.asarray(DC[s0:s1, :], np.float64)
            if keep_idx is not None:
                Y = Y[:, keep_idx]

            yhat = np.zeros_like(Y)
            for c in range(x_CP.shape[0]):
                A = np.asarray(M[s0:s1, c, :, :], np.float32)
                if keep_idx is not None:
                    A = A[:, :, keep_idx]
                if inv_cp_flux_ref is not None:
                    A = A * inv_cp_flux_ref[c, :][None, :, None]

                yhat += np.tensordot(x_CP[c, :], A, axes=(0, 1))

            R = Y - yhat
            if w_lam_sqrt is not None:
                R = R * w_lam_sqrt

            ssq += float(np.sum(R * R))
            n   += int(R.size)

    return np.sqrt(ssq / max(n, 1))

# ------------------------------------------------------------------------------

def _tiny_col_freeze_inplace(col_denom, grad, rel_zero, abs_zero):
    """
    Freeze numerically tiny / unsupported columns for a given tile-band.

    Parameters
    ----------
    col_denom : (P,) float array
        Per-column denominator computed on the SAME weighted/scaled A used for
        the gradient (e.g. after cp_flux_ref scaling and λ-weighting).
    grad : (P,) float array
        Per-column gradient/numerator (same space as col_denom).
    rel_zero : float
        Relative threshold multiplier applied to median positive denom.
    abs_zero : float
        Absolute floor threshold.
    """
    # robust median on strictly-positive finite entries
    good = np.isfinite(col_denom) & (col_denom > 0.0)
    if np.any(good):
        med_energy = float(np.median(col_denom[good]))
    else:
        med_energy = 0.0

    tiny_col = float(max(abs_zero, rel_zero * med_energy))

    # freeze non-finite or tiny denom
    freeze = (~np.isfinite(col_denom)) | (col_denom <= tiny_col)
    if np.any(freeze):
        grad[freeze] = 0.0
        col_denom[freeze] = np.inf

    return freeze, tiny_col, med_energy

# ---------------------------- Worker ---------------------------------

def _worker_tile_global_grad_band(args):
    r"""
    Compute global Kaczmarz-style gradient contributions for one spatial
    tile and one contiguous band of components.

    This worker does **not** update x or R. It only returns the
    band-local contributions to the global gradient and diagonal
    preconditioner:

        g_band[c,p]  = sum_{s,λ} (√w_λ A_{s,c,p,λ}) * (√w_λ R_{s,λ})
        D_band[c,p]  = sum_{s,λ} (√w_λ A_{s,c,p,λ})^2,

    where R = Y - ŷ is the residual for the current epoch's global x_CP,
    and A already includes any column-flux normalization in the "model"
    basis.

    Additionally, this worker can apply a tile-local "tiny-column freeze"
    safety guard: columns with numerically tiny denom are zeroed in the
    numerator and excluded from the denom accumulator for this tile. This
    prevents large/noisy updates later when the coordinator forms
    invD = 1/max(D, eps).

    Control knobs (env)
    -------------------
    CUBEFIT_ZERO_COL_FREEZE : {0,1} (default 1)
        Enable/disable tile-local freeze.
    CUBEFIT_ZERO_COL_REL : float (default 1e-12)
    CUBEFIT_ZERO_COL_ABS : float (default 1e-24)

    Returns
    -------
    g_band : ndarray, shape (band_size, P), float64
    D_band : ndarray, shape (band_size, P), float64
    """
    (h5_path, s0, s1, keep_idx,
     c_start, c_stop,
     x_band,
     R_tile,
     w_lam_sqrt,
     inv_ref_band,
     dset_slots, dset_bytes, dset_w0) = args

    eps = float(os.environ.get("CUBEFIT_EPS", "1e-12"))

    freeze_enable = os.environ.get(
        "CUBEFIT_ZERO_COL_FREEZE", "1"
    ).lower() not in ("0", "false", "no", "off")
    rel_zero = float(os.environ.get("CUBEFIT_ZERO_COL_REL", "1e-12"))
    abs_zero = float(os.environ.get("CUBEFIT_ZERO_COL_ABS", "1e-24"))

    # Shapes
    R_tile = np.asarray(R_tile, dtype=np.float64, order="C")
    Sblk, Lk = R_tile.shape

    # λ-weights (optional)
    if w_lam_sqrt is not None:
        wvec = np.asarray(w_lam_sqrt, np.float64).ravel()
        if wvec.size != Lk:
            raise RuntimeError(
                f"_worker_tile_global_grad_band: w_lam_sqrt length "
                f"{wvec.size} != Lk={Lk}"
            )
        Rw = R_tile * wvec[None, :]
    else:
        wvec = None
        Rw = R_tile

    band_size, P = x_band.shape

    g_band = np.zeros((band_size, P), dtype=np.float64)
    D_band = np.zeros((band_size, P), dtype=np.float64)

    with open_h5(h5_path, role="reader") as f:
        M = f["/HyperCube/models"]  # (S, C, P, L)
        try:
            M.id.set_chunk_cache(dset_slots, dset_bytes, dset_w0)
        except Exception:
            pass

        for bi, c in enumerate(range(c_start, c_stop)):
            A = np.asarray(
                M[s0:s1, c, :, :], dtype=np.float32, order="C"
            )  # (Sblk, P, L_full)
            if keep_idx is not None:
                A = A[:, :, keep_idx]  # (Sblk, P, Lk)

            # Apply column-flux normalization if present
            if inv_ref_band is not None:
                inv_ref = np.asarray(
                    inv_ref_band[bi, :], dtype=np.float64, copy=False
                )  # (P,)
                A = A * inv_ref[None, :, None]

            # Promote to float64 for safe dot products
            A = A.astype(np.float64, copy=False)

            if wvec is not None:
                A_w = A * wvec[None, None, :]  # (Sblk, P, Lk)
            else:
                A_w = A

            # g_row[p] = Σ_{s,λ} A_w[s,p,λ] * Rw[s,λ]
            g_row = np.tensordot(A_w, Rw, axes=([0, 2], [0, 1]))

            # D_row[p] = Σ_{s,λ} (A_w[s,p,λ])^2
            D_row = np.sum(np.square(A_w, dtype=np.float64), axis=(0, 2))

            # ---- Tiny-column freeze (tile-local) -------------------------
            # Use the existing helper. It sets denom[frozen]=inf; for a
            # global accumulator we instead want to *exclude* this tile’s
            # contribution, so we revert denom[frozen] -> 0 before storing.
            if freeze_enable:
                freeze, _, _ = _tiny_col_freeze_inplace(
                    D_row, g_row, rel_zero, abs_zero
                )
                if np.any(freeze):
                    D_row[freeze] = 0.0

            g_band[bi, :] = g_row
            D_band[bi, :] = D_row

    # Numerical guards
    if not np.all(np.isfinite(g_band)):
        g_band = np.nan_to_num(
            g_band, nan=0.0, posinf=0.0, neginf=0.0, copy=False
        )

    # For D: keep zeros (they are meaningful), but sanitize NaN/Inf.
    if not np.all(np.isfinite(D_band)):
        D_band = np.nan_to_num(
            D_band, nan=0.0, posinf=0.0, neginf=0.0, copy=False
        )

    # Ensure no negative denom from numerical noise
    np.maximum(D_band, 0.0, out=D_band)

    # Optional hard floor (only if you later invert without g-masking);
    # here we do NOT force a floor, because g has been frozen where denom
    # is tiny/invalid.
    if not np.isfinite(eps) or eps <= 0.0:
        eps = 1e-12

    return g_band, D_band

# ---------------------------- Worker ---------------------------------

def _worker_tile_job_with_R(args):
    r"""
    Worker for one S-tile and a contiguous c-band.

    This routine performs a robust, mask/weight-aware Kaczmarz-style update
    over a band of components for a single spatial tile. It is designed to be
    numerically stable and inclusive of common data challenges (NaNs/Infs,
    near-zero columns, masked wavelengths), while preserving physical zeros in
    the design matrix (no input normalization).

    Key behaviors
    -------------
    • Sanitize inputs in-place: non-finite values in `A` and `R` → 0.
    • Freeze columns whose (tile-local, weighted) energy is numerically tiny.
    • Diagonal preconditioning:
        dx_p = lr * g_p / sum_s,λ ( (√w_λ A_{s,p,λ})^2 ).
    • Trust region on the model change:
        || √w ⊙ (Δy) || ≤ τ · || √w ⊙ R ||  with τ≈0.7 by default.
    • Backtracking on the step scale α using an **O(1) quadratic evaluation**
      of the weighted RMSE:
         ||Rw + a·ΔRw||² = rr + 2a·cr + a²·rd,
      where rr=⟨Rw,Rw⟩, rd=⟨ΔRw,ΔRw⟩, cr=⟨Rw,ΔRw⟩. This avoids repeated
      array sums over Sblk×Lk during line search.
    • Blend local denominators with global column energy:
        denom ← max(denom, β · E_global[c, :]).
    • Return per-band global update-energy estimates for a global step cap.
    • Propose a small set of high-score columns per band for optional,
      tile-local NNLS polishing upstream (the actual polish is coordinated
      outside this worker).

    Parameters
    ----------
    args : tuple
        (h5_path, s0, s1, keep_idx, c_start, c_stop, x_band, lr,
         project_nonneg, R, w_band, dset_slots, dset_bytes, dset_w0,
         E_global_band, beta_blend, w_lam_sqrt)

        See caller for shapes and semantics. In brief:
        - R is the current (unweighted) residual for the tile, shape (Sblk, Lk).
        - w_lam_sqrt is √(λ-weights) on the (masked) λ-grid or None.
        - x_band is a view of the current weights for this band (c_start:c_stop).
        - E_global_band supplies per-(band,pop) global energies for blending.

    Returns
    -------
    R_delta : ndarray, shape (Sblk, Lk), float64
        The **unweighted** residual update accumulated over the band
        (i.e., what should be added to R by the coordinator, after an
        optional outer backtracking / global cap).
    dx_list : list of tuples
        [(c, dx_c, upd_energy_sq), ...] where dx_c has shape (P,) and
        `upd_energy_sq = Σ_p (dx_c[p]^2 · E_global[c,p])` for global
        step capping upstream.
    nnls_candidates : list of (global_cp_index, score)
        Proposed (component·P + pop) indices and their scores for a
        later tile-local NNLS polish step.

    Notes
    -----
    • All λ-weighting is applied symmetrically to `A` and `R` via `√w` for
      gradient/denominator and trust-region logic. The returned `R_delta`
      remains **unweighted** so that the coordinator can combine band deltas
      and apply any further global backtracking consistently.
    • Nonnegativity projection is respected. If projection clips some
      entries, ΔR is recomputed for the *final* dx on this band to keep the
      update self-consistent.
    """
    (h5_path, s0, s1, keep_idx,
     c_start, c_stop, x_band, x_prior_band,
     lr, project_nonneg, R,
     dset_slots, dset_bytes, dset_w0,
     E_global_band, beta_blend,
     w_lam_sqrt,
     inv_ref_band) = args   # (band_size, P)


    bt_steps   = int(os.environ.get("CUBEFIT_BT_STEPS", "3"))
    bt_factor  = float(os.environ.get("CUBEFIT_BT_FACTOR", "0.5"))
    tau_trust  = float(os.environ.get("CUBEFIT_TRUST_TAU", "0.7"))
    eps        = float(os.environ.get("CUBEFIT_EPS", "1e-12"))
    rel_zero   = float(os.environ.get("CUBEFIT_ZERO_COL_REL", "1e-12"))
    abs_zero   = float(os.environ.get("CUBEFIT_ZERO_COL_ABS", "1e-24"))
    dbg        = os.environ.get("CUBEFIT_DEBUG_SAFE", "0") == "1"
    nnls_prop_per_band = int(os.environ.get("CUBEFIT_NNLS_PROP_PER_BAND", "6"))
    nnls_l2 = float(os.environ.get("CUBEFIT_NNLS_L2", "0.0"))
    if not np.isfinite(nnls_l2) or nnls_l2 < 0.0:
        nnls_l2 = 0.0

    bt_steps = np.max((0, bt_steps))
    beta_blend = float(beta_blend)

    def _finite_inplace(arr, name, stats_dict):
        bad = ~np.isfinite(arr)
        if bad.any():
            arr[bad] = 0.0
            if stats_dict is not None:
                stats_dict[name] = int(np.sum(bad))

    with open_h5(h5_path, role="reader") as f:
        M = f["/HyperCube/models"]  # (S,C,P,L)
        try:
            M.id.set_chunk_cache(dset_slots, dset_bytes, dset_w0)
        except Exception:
            pass

        Sblk = int(s1 - s0)
        P    = int(M.shape[2])
        L    = int(M.shape[3])
        Lk   = L if keep_idx is None else int(keep_idx.size)

        R = np.asarray(R, dtype=np.float64, order="C")  # (Sblk,Lk)
        stats = {}
        _finite_inplace(R, "R_nonfinite", stats)

        # Weighted residual (√w * R) for LS computations
        if w_lam_sqrt is not None:
            wvec = np.asarray(w_lam_sqrt, np.float64).ravel()
            if wvec.size != R.shape[1]:
                raise RuntimeError(
                    f"w_lam_sqrt length {wvec.size} != Lk={R.shape[1]}"
                )
            Rw = R * wvec[None, :]
        else:
            Rw = R

        rmse_before_tile = float(np.sqrt(np.mean(R * R)))  # unweighted (for info)
        R_delta = np.zeros((Sblk, Lk), dtype=np.float64)
        dx_list = []
        nnls_candidates = []

        for bi, c in enumerate(range(c_start, c_stop)):
            A = np.asarray(M[s0:s1, c, :, :], np.float32, order="C")
            if keep_idx is not None:
                A = A[:, :, keep_idx]  # (Sblk,P,Lk)

            inv_ref = (
                np.asarray(inv_ref_band[bi, :], dtype=np.float64)
                if inv_ref_band is not None
                else np.ones((P,), dtype=np.float64)
            )
            # Broadcast over (Sblk, P, Lk)
            A = A * inv_ref[None, :, None]
            _finite_inplace(A, f"A_nonfinite_c{c}", stats)

            # λ-weighted view for gradient/denominator
            if w_lam_sqrt is not None:
                A_w = A * w_lam_sqrt[None, None, :]
            else:
                A_w = A

            # Gradient g = (√w A)^T (√w R)
            g = np.zeros((P,), dtype=np.float64)
            for s in range(Sblk):
                g += A_w[s, :, :].astype(np.float64, copy=False) @ Rw[s, :]

            # Per-column denom on the same weighted view
            col_denom = np.sum(np.square(A_w, dtype=np.float64), axis=(0, 2))

            # Freeze numerically tiny columns (tile-local)
            if np.any(col_denom > 0):
                med_energy = float(np.median(col_denom[col_denom > 0]))
            else:
                med_energy = 0.0
            tiny_col = np.max((abs_zero, rel_zero * med_energy))
            freeze = col_denom <= tiny_col
            if freeze.any():
                g[freeze] = 0.0
                col_denom = np.where(freeze, np.inf, col_denom)

            # Blend with global column energy (stabilizes sparse tiles)
            if E_global_band is not None:
                Eg_row = np.asarray(E_global_band[bi, :], dtype=np.float64)
                if Eg_row.size == col_denom.size:
                    col_denom = np.maximum(col_denom, beta_blend * Eg_row)

            # Optional L2 term.
            #
            # If x_prior_band is not None:
            #   J_L2 = 0.5 * nnls_l2 * ||x_c - x_prior_c||^2   (seed as prior)
            # If x_prior_band is None:
            #   J_L2 = 0.5 * nnls_l2 * ||x_c||^2               (ridge to 0)
            #
            # Our g currently carries +A^T W R, i.e. the negative gradient
            # of the data term. The L2 term contributes
            #   -nnls_l2 * (x_c - x_prior_c)   or   -nnls_l2 * x_c
            # to this same descent direction, restricted to non-frozen cols.
            if nnls_l2 > 0.0:
                x_c = np.asarray(
                    x_band[bi, :], dtype=np.float64, copy=False
                )

                if x_prior_band is not None:
                    # pull toward prior
                    x_prior_c = np.asarray(
                        x_prior_band[bi, :],
                        dtype=np.float64,
                        copy=False,
                    )
                    delta = x_c - x_prior_c
                    if freeze.any():
                        mask = ~freeze
                        g[mask] -= nnls_l2 * delta[mask]
                    else:
                        g -= nnls_l2 * delta
                else:
                    # No prior supplied: pure ridge to zero
                    if freeze.any():
                        mask = ~freeze
                        g[mask] -= nnls_l2 * x_c[mask]
                    else:
                        g -= nnls_l2 * x_c

            invD = 1.0 / np.maximum(col_denom, eps)
            dx_c = float(lr) * (g * invD)  # (P,)

            # Propose NNLS candidates (optional polish)
            if nnls_prop_per_band > 0:
                score = np.abs(g) / (np.sqrt(np.maximum(col_denom, eps)) + eps)
                if np.any(freeze):
                    score[freeze] = 0.0
                k_keep = int(np.min((nnls_prop_per_band, int(np.count_nonzero(score > 0.0)))))
                if k_keep > 0:
                    top_p = np.argpartition(score, -k_keep)[-k_keep:]
                    ordr  = np.argsort(score[top_p])[::-1]
                    top_p = top_p[ordr]
                    top_global = (int(c) * P + top_p).astype(np.int64)
                    for j in range(top_p.size):
                        nnls_candidates.append(
                            (int(top_global[j]), float(score[top_p[j]]))
                        )

            # ΔR for α=1, computed in the **unweighted** space
            R_delta_band = np.zeros((Sblk, Lk), dtype=np.float64)
            for s in range(Sblk):
                R_delta_band[s, :] -= (
                    A[s, :, :].astype(np.float64, copy=False).T @ dx_c
                )

            # Trust region & backtracking in the **weighted** space
            if w_lam_sqrt is not None:
                Rw_delta_band = R_delta_band * wvec[None, :]
            else:
                Rw_delta_band = R_delta_band

            # Trust-region cap on α based on norms
            rd_norm = float(np.linalg.norm(Rw_delta_band))
            if rd_norm > 0.0:
                r_norm = float(np.linalg.norm(Rw))
                alpha_max = np.min((1.0, (tau_trust * r_norm) / rd_norm))
            else:
                alpha_max = 1.0

            # ---- Quadratic, O(1) evaluation for line search ----
            # ||Rw + a*Δ||^2 = rr + 2a*cr + a^2*rd
            rr  = float(np.dot(Rw.ravel(),           Rw.ravel()))
            rd2 = float(np.dot(Rw_delta_band.ravel(), Rw_delta_band.ravel()))
            cr  = float(np.dot(Rw.ravel(),           Rw_delta_band.ravel()))
            den = float(Rw.size) or 1.0

            def _rmse_w_at(a: float) -> float:
                val = rr + 2.0 * a * cr + (a * a) * rd2
                if val < 0.0:  # numerical safety
                    val = 0.0
                return math.sqrt(val / den)

            rmse_before_w = math.sqrt(rr / den)
            alpha = float(alpha_max)
            rmse_after_w = _rmse_w_at(alpha)

            if not (rmse_after_w < rmse_before_w):
                a = alpha
                for _ in range(bt_steps):
                    a *= bt_factor
                    if a <= 0.0:
                        break
                    if _rmse_w_at(a) < rmse_before_w:
                        alpha = a
                        break
                else:
                    alpha = a  # may be 0

            # finalize dx; if we clip nonnegativity, recompute ΔR / ΔRw for final dx
            dx_c = alpha * dx_c
            if project_nonneg:
                over_neg = dx_c < -x_band[bi, :]
                if np.any(over_neg):
                    dx_c[over_neg] = -x_band[bi, :][over_neg]
                    R_delta_band.fill(0.0)
                    for s in range(Sblk):
                        R_delta_band[s, :] -= (
                            A[s, :, :].astype(np.float64, copy=False).T @ dx_c
                        )
                    if w_lam_sqrt is not None:
                        Rw_delta_band = R_delta_band * wvec[None, :]
            else:
                if alpha != 1.0:
                    R_delta_band  *= alpha
                    Rw_delta_band *= alpha

            # Per-band global update-energy estimate (for a global cap)
            Eg_row = None
            if E_global_band is not None:
                Eg_row = np.asarray(E_global_band[bi, :], np.float64)
            if Eg_row is not None:
                upd_energy_sq = float(
                    np.sum((dx_c.astype(np.float64) ** 2) * Eg_row)
                )
            else:
                upd_energy_sq = 0.0

            R_delta += R_delta_band
            dx_list.append((c, dx_c, upd_energy_sq))

        if dbg and stats:
            try:
                print("[SAFE]", {k: int(v) for k, v in stats.items()})
            except Exception:
                pass

        return R_delta, dx_list, nnls_candidates

# ------------------------------------------------------------------------------

def _canon_orbit_weights(h5_path: str,
                         orbit_weights,
                         C: int,
                         P: int) -> np.ndarray | None:
    """
    Return a (C,) float64 prior vector for components, or None if unavailable.
    Accepts:
      - orbit_weights == None: try reading '/CompWeights' from HDF5.
      - orbit_weights shape == (C,): use as-is.
      - orbit_weights shape == (C*P,): sum over populations -> (C,).
    Raises if a provided vector has incompatible size.
    """
    w = None
    if orbit_weights is not None:
        w = np.asarray(orbit_weights, dtype=np.float64).ravel(order="C")
    else:
        with open_h5(h5_path, role="reader") as f:
            if "/CompWeights" in f:
                w = np.asarray(f["/CompWeights"][...], dtype=np.float64).ravel(order="C")
            else:
                return None  # no prior available

    if w.size == C:
        pass
    elif w.size == C * P:
        w = w.reshape(C, P).sum(axis=1)
    else:
        raise ValueError(f"orbit_weights length {w.size} incompatible with C={C}, P={P}. "
                         f"Expected C or C*P.")
    # normalize to a comparable scale (optional, keeps magnitudes sane)
    s = np.sum(w)
    if np.isfinite(s) and s > 0.0:
        w = w / s
    return w

# ------------------------------------------------------------------------------

def softbox_params_smooth(eq: int, E: int) -> tuple[float, float]:
    """
    Cosine ramp starting at epoch 2 (1-based):
      eq = 1 → (band, step) = (0.30, 0.20)
      eq = E → (band, step) = (0.15, 0.30)
      2..E ramps smoothly between the two.
    """
    eq = int(eq)
    E  = int(max(2, E))

    if eq <= 1:
        return 0.30, 0.20

    # t=0 at eq=2, t=1 at eq=E
    t = np.clip((eq - 2) / max(1, (E - 2)), 0.0, 1.0)
    s = 0.5 - 0.5 * np.cos(np.pi * t)

    band = (1.0 - s) * 0.30 + s * 0.15   # 0.30 → 0.15
    step = (1.0 - s) * 0.20 + s * 0.30   # 0.20 → 0.30
    return float(band), float(step)

# ------------------------------------------------------------------------------

def solve_global_kaczmarz_cchunk_mp(
    h5_path: str,
    cfg: MPConfig,
    *,
    orbit_weights: Optional[np.ndarray] = None,   # (C,) or None
    x0: Optional[np.ndarray] = None,              # 1-D (C*P,) or None
    tracker: Optional[object] = None,             # FitTracker or None
    ratio_cfg: cu.RatioCfg | None = None,
) -> tuple[np.ndarray, dict]:

    pool = None
    try:

        def _set_chunk_cache(dset, cfg_obj):
            try:
                dset.id.set_chunk_cache(
                    cfg_obj.dset_slots, cfg_obj.dset_bytes, cfg_obj.dset_w0
                )
            except Exception:
                pass

        # ---- guard: ensure HyperCube normalization mode is declared
        mode = _assert_norm_mode(h5_path, expect=None)
        print(f"[Kaczmarz-MP] HyperCube norm.mode = '{mode}'", flush=True)

        # ------------------- read dims, chunks, mask -------------------
        t0 = time.perf_counter()
        with open_h5(h5_path, role="reader") as f:
            DC = f["/DataCube"]           # (S,L) float64
            M  = f["/HyperCube/models"]   # (S,C,P,L) float32
            _set_chunk_cache(M, cfg)

            S, L = map(int, DC.shape)
            _, C, P, Lm = map(int, M.shape)
            if Lm != L:
                raise RuntimeError(f"L mismatch: models L={Lm} vs data L={L}.")

            mask = cu._get_mask(f) if cfg.apply_mask else None
            keep_idx_lam = np.flatnonzero(mask) if mask is not None else None
            Lk = int(keep_idx_lam.size) if keep_idx_lam is not None else L
            # ------------------- column-flux scaling -------------------
            # cp_flux_ref[c,p] is the reference flux for column (c,p) in the *physical* basis
            cp_flux_ref = cu._ensure_cp_flux_ref(h5_path, keep_idx=keep_idx_lam)  # shape (C,P) or None
            cp_flux_ref = None

            if cp_flux_ref is not None:
                print("[Kaczmarz-MP] Using column-flux scaling.", flush=True)
                print(f"[Kaczmarz-MP] {'cp_flux_ref'}.min={np.min(cp_flux_ref):.3e}, "
                      f"max={np.max(cp_flux_ref):.3e}, "
                      f"median={np.median(cp_flux_ref[cp_flux_ref>0]):.3e}",
                      flush=True)
                cp_flux_ref = np.asarray(cp_flux_ref, np.float64).reshape(C, P)
                inv_cp_flux_ref = 1.0 / np.maximum(cp_flux_ref, 1.0e-30)
            else:
                # no scaling; everything stays physical
                cp_flux_ref = None
                inv_cp_flux_ref = None

            # ------------------- λ-weights (feature emphasis) -------------
            lamw_enable = os.environ.get(
                "CUBEFIT_LAMBDA_WEIGHTS_ENABLE", "1"
            ).lower() not in ("0", "false", "no", "off")
            lamw_dset   = os.environ.get(
                "CUBEFIT_LAMBDA_WEIGHTS_DSET", "/HyperCube/lambda_weights"
            )
            lamw_floor  = float(os.environ.get("CUBEFIT_LAMBDA_MIN_W", "1e-6"))
            lamw_auto   = os.environ.get(
                "CUBEFIT_LAMBDA_WEIGHTS_AUTO", "1"
            ).lower() not in ("0", "false", "no", "off")

            if lamw_enable:
                print(
                    f"[Kaczmarz-MP] Reading λ-weights from '{lamw_dset}' "
                    f"(floor={lamw_floor}, auto={lamw_auto})...", flush=True
                )
                try:
                    w_full = cu.read_lambda_weights(
                        h5_path, dset_name=lamw_dset, floor=lamw_floor
                    )
                except Exception:
                    w_full = cu.ensure_lambda_weights(
                        h5_path, dset_name=lamw_dset
                    ) if lamw_auto else np.ones(L, dtype=np.float64)
                print(
                    "[Kaczmarz-MP] λ-weights min={:.3e}, max={:.3e}, mean={:.3e}"
                    .format(float(np.min(w_full)),
                            float(np.max(w_full)),
                            float(np.mean(w_full))),
                    flush=True
                )
                w_lam = w_full[keep_idx_lam] if keep_idx_lam is not None else w_full
                # Use sqrt(w) on both A and R (weighted LS)
                w_lam_sqrt = np.sqrt(np.clip(w_lam, lamw_floor, None)).astype(
                    np.float64, order="C"
                )
            else:
                w_lam_sqrt = None

            # Spaxel tiling (follow /HyperCube/models chunking)
            s_tile = int(M.chunks[0]) if (M.chunks and M.chunks[0] > 0) else 128
            s_ranges = [(s0, np.min((S, s0 + s_tile))) for s0 in range(0, S, s_tile)]

            # ordering + global ||Y|| while we touch DC
            norms = []
            Y_glob_norm2 = 0.0
            for (ss0, ss1) in s_ranges:
                Yt = np.asarray(DC[ss0:ss1, :], np.float64)
                if keep_idx_lam is not None:
                    Yt = Yt[:, keep_idx_lam]
                norms.append(float(np.linalg.norm(Yt)))
                Y_glob_norm2 += float(np.sum(Yt * Yt))

        print(
            f"[Kaczmarz-MP] DataCube S={S}, L={L} (kept Lk={Lk}), "
            f"Hypercube C={C}, P={P}, s_tile={s_tile}, "
            f"epochs={cfg.epochs}, lr={cfg.lr}, "
            f"processes={cfg.processes}, blas_threads={cfg.blas_threads}",
            flush=True
        )

        # --- Persist orbit_weights and push into tracker ---
        print("[Kaczmarz-MP] Preparing orbit weights...", flush=True)
        print("[Kaczmarz-MP] _canon_orbit_weights(...)...", flush=True)
        w_prior = _canon_orbit_weights(h5_path, orbit_weights, C, P)
        print("[Kaczmarz-MP] done.", flush=True)
        if (w_prior is not None) and (tracker is not None) and hasattr(tracker, "set_orbit_weights"):
            print("[Kaczmarz-MP] Setting orbit weights in tracker...", flush=True)
            tracker.set_orbit_weights(w_prior)  # this mirrors your SP path behavior
            print("[Kaczmarz-MP] done.", flush=True)

        # sort by descending norm (hard / bright tiles first)
        paired = sorted(zip(norms, s_ranges), key=lambda t: -t[0])
        norms_sorted = [t[0] for t in paired]
        s_ranges_sorted = [t[1] for t in paired]

        # Optional tile budget for quick polish runs
        print("[Kaczmarz-MP] Applying max_tiles constraint...", flush=True)
        max_tiles = cfg.max_tiles
        if max_tiles is not None:
            max_tiles = int(max_tiles)
            if 0 < max_tiles < len(s_ranges_sorted):
                rng = np.random.default_rng(
                    int(os.environ.get("CUBEFIT_POLISH_SEED", "12345"))
                )

                k_bright = max(1, int(math.ceil(0.7 * max_tiles)))
                k_rand   = max(0, max_tiles - k_bright)

                bright_idx = np.arange(k_bright, dtype=int)
                tail_idx = np.arange(k_bright, len(s_ranges_sorted), dtype=int)
                if k_rand > 0 and tail_idx.size > 0:
                    rng.shuffle(tail_idx)
                    rand_idx = tail_idx[:k_rand]
                    keep_tiles_idx = np.concatenate([bright_idx, rand_idx])
                else:
                    keep_tiles_idx = bright_idx

                keep_tiles_idx = np.sort(keep_tiles_idx)
                s_ranges = [s_ranges_sorted[i] for i in keep_tiles_idx]
                norms_sorted = [norms_sorted[i] for i in keep_tiles_idx]
            else:
                s_ranges = s_ranges_sorted
        else:
            s_ranges = s_ranges_sorted
        print(f"[Kaczmarz-MP] Using {len(s_ranges)} tiles for fitting.", flush=True)

        Y_glob_norm = float(np.sqrt(Y_glob_norm2))


        # ------------------- global column energy & knobs -----------------
        print("[Kaczmarz-MP] Reading global column energy...", flush=True)
        E_global = read_global_column_energy(h5_path)  # (C,P) float64
        print("[Kaczmarz-MP] done.", flush=True)
        tau_global = float(os.environ.get("CUBEFIT_GLOBAL_TAU", "0.5"))
        beta_blend = float(os.environ.get("CUBEFIT_GLOBAL_ENERGY_BLEND", "1e-2"))

        # ------------------- x init --------------------------------------
        if x0 is None:
            # Start from zero *in the normalized basis*
            x_CP = np.zeros((C, P), dtype=np.float64, order="C")
        else:
            x0 = np.asarray(x0, np.float64).ravel(order="C")
            if x0.size != C * P:
                raise ValueError(f"x0 length {x0.size} != C*P={C*P}.")

            X_phys = x0.reshape(C, P)  # x0 is always in the physical basis

            if cp_flux_ref is not None:
                # Encode physical weights into the normalized basis the solver uses
                x_CP = X_phys * cp_flux_ref   # x_norm = x_phys * cp_flux_ref
            else:
                x_CP = X_phys.copy(order="C")

        # Tiny symmetry breaking (optional)
        sym_eps  = float(os.environ.get("CUBEFIT_SYMBREAK_EPS", "1e-6"))
        sym_mode = os.environ.get("CUBEFIT_SYMBREAK_MODE", "qr").lower()
        if sym_eps > 0.0 and sym_mode != "off" and (
            x0 is None or np.count_nonzero(x_CP) == 0
        ):
            print("[Kaczmarz-MP] Applying symmetry breaking...", flush=True)
            rng = np.random.default_rng(int(os.environ.get("CUBEFIT_SEED",
                                                        "12345")))
            if sym_mode == "qr":
                Rmat = rng.standard_normal((C, P))
                Q, _ = np.linalg.qr(Rmat.T, mode="reduced")
                Q = Q[:, :C].T  # (C,P)
                x_CP += sym_eps * np.abs(Q)
            else:
                x_CP += sym_eps * rng.random((C, P))
            if cfg.project_nonneg:
                np.maximum(x_CP, 0.0, out=x_CP)
            print("[Kaczmarz-MP] done.", flush=True)
        # x_CP holds the normalized weights actually updated by Kaczmarz.
        # Optionally keep a fixed prior (e.g. NNLS seed) for L2.
        use_prior = os.environ.get(
            "CUBEFIT_USE_NNLS_PRIOR", "1"
        ).lower() not in ("0", "false", "no", "off")

        if use_prior:
            print("[Kaczmarz-MP] Using NNLS seed as prior.", flush=True)
            x_CP_prior = x_CP.copy(order="C")
        else:
            print("[Kaczmarz-MP] Seed prior disabled.", flush=True)
            x_CP_prior = None

        # ----- global RMSE configuration (weighted/unweighted & guard) -----
        rmse_weighted = os.environ.get(
            "CUBEFIT_RMSE_WEIGHTED", "1"
        ).lower() not in ("0", "false", "no", "off")

        # Tolerance for "best epoch" tracking (used only for logging).
        rmse_guard_tol = float(
            os.environ.get("CUBEFIT_RMSE_GUARD_TOL", "0.0")
        )

        print(
            f"[Kaczmarz-MP] Global RMSE will be computed "
            f"{'with' if rmse_weighted else 'without'} λ-weights.",
            flush=True,
        )

        # # ------------------- global RMSE for the seed ----------------------
        # print("[Kaczmarz-MP] Computing global RMSE for the seed...", flush=True)
        # rmse_seed_raw = _compute_global_rmse(
        #     h5_path,
        #     x_CP,
        #     s_ranges=s_ranges,
        #     keep_idx=keep_idx,
        #     w_lam_sqrt=w_lam_sqrt,
        #     cp_flux_ref=cp_flux_ref,
        #     inv_cp_flux_ref=inv_cp_flux_ref,
        #     dset_slots=cfg.dset_slots,
        #     dset_bytes=cfg.dset_bytes,
        #     dset_w0=cfg.dset_w0,
        #     weighted=rmse_weighted,
        # )
        # rmse_seed = _safe_scalar_rmse(rmse_seed_raw, "seed")
        # print(
        #     f"[Kaczmarz-MP] seed global RMSE = {rmse_seed:.6e}",
        #     flush=True,
        # )

        # best_rmse = float(rmse_seed)
        # best_x_CP = x_CP.copy(order="C")

        best_rmse_proxy = np.inf
        best_x_CP       = x_CP.copy()

        # ------------------- ratio penalty (argument-driven) ------------------
        print(f"[Kaczmarz-MP] ratio_cfg pre-normalization: {type(ratio_cfg)}", flush=True)
        try:
            if ratio_cfg is not None and not isinstance(ratio_cfg, cu.RatioCfg):
                # Be conservative: only accept actual dict-like configs
                ratio_cfg = cu.RatioCfg(**dict(ratio_cfg))
            print(f"[Kaczmarz-MP] ratio_cfg post-normalization: {type(ratio_cfg)}",
                flush=True)
        except Exception as e:
            # If anything goes wrong, disable ratio and log
            print(f"[Kaczmarz-MP] ratio_cfg normalization failed: {e!r}. "
                f"Disabling ratio regularizer.", flush=True)
            ratio_cfg = None
        rc = ratio_cfg

        have_ratio = (orbit_weights is not None and rc is not None and rc.use)
        print(f"[Kaczmarz-MP] have_ratio = {have_ratio}", flush=True)
        if have_ratio:
            print("[Kaczmarz-MP] Preparing ratio regularizer...", flush=True)
            w_in = np.asarray(orbit_weights, np.float64).ravel(order="C")
            if w_in.size != C:
                raise ValueError(f"orbit_weights length {w_in.size} != C={C}.")

            # Base target mixture (normalize with a floor)
            w_target = np.maximum(w_in, rc.minw)
            w_target = w_target / np.maximum(np.sum(w_target), 1.0)

            # Optional anchor: mixture implied by the input x0 (also in physical units)
            if x0 is not None and x0.size == C * P:
                x0_mat = np.asarray(x0, np.float64).reshape(C, P, order="C")
                s0 = np.sum(np.maximum(x0_mat, 0.0), axis=1)  # physical mass per component
                S0 = np.sum(s0)
                t_x0 = (s0 / np.maximum(S0, 1.0)) if S0 > 0.0 else w_target
            else:
                t_x0 = None

            rng = np.random.default_rng()

            # Choose which target mixture to use at each update; for now just w_target.
            # You can later blend w_target and t_x0 if rc supports that.
            def _get_target_mix() -> np.ndarray:
                return w_target

            def _ratio_update_in_place(x_norm_mat: np.ndarray) -> None:
                """
                Mass-preserving multiplicative update toward the target mixture
                in *physical* space. x_norm_mat is the solver's internal normalized
                weights (x_norm = x_phys * cp_flux_ref).
                """
                # 1) Convert to physical weights
                if cp_flux_ref is not None:
                    x_phys = x_norm_mat * inv_cp_flux_ref     # x_phys = x_norm / cp_flux_ref
                else:
                    x_phys = x_norm_mat

                # 2) Compute physical mixture per component
                s = np.sum(x_phys, axis=1)                    # (C,)
                S = np.sum(s)
                if not np.isfinite(S) or S <= 0.0:
                    return

                sh = s / np.maximum(S, 1.0)                   # current physical mixture ŝ

                t_vec = _get_target_mix()                     # target physical mixture (sum=1)

                active = np.isfinite(sh) & np.isfinite(t_vec) & (t_vec > 0.0)
                idx = np.nonzero(active)[0]
                if idx.size == 0:
                    return

                # Optional stochastic subsampling
                if rc.prob < 1.0:
                    keep = rng.random(idx.size) < rc.prob
                    idx = idx[keep]
                    if idx.size == 0:
                        return
                if (rc.batch > 0) and (idx.size > rc.batch):
                    idx = rng.choice(idx, size=int(rc.batch), replace=False)

                # 3) Compute multiplicative factors (in log space) to move ŝ → t_vec
                e = np.log(np.maximum(sh[idx], rc.minw)) - np.log(
                    np.maximum(t_vec[idx], rc.minw)
                )
                f_idx = np.exp(-rc.eta * e)
                f_idx = np.clip(f_idx, 1.0 / rc.gamma, rc.gamma)

                F = np.ones((C,), dtype=np.float64)
                F[idx] = f_idx

                denom = np.sum(sh * F)
                if not np.isfinite(denom) or denom <= 0.0:
                    return

                # Renormalize to keep total mass fixed
                F = F / np.maximum(denom, 1.0e-30)

                # 4) Apply update in *physical* space, then convert back to normalized
                x_phys *= F[:, None]
                if cfg.project_nonneg:
                    np.maximum(x_phys, 0.0, out=x_phys)

                if cp_flux_ref is not None:
                    x_norm_mat[:, :] = x_phys * cp_flux_ref   # back to normalized basis
                else:
                    x_norm_mat[:, :] = x_phys

            print("[Kaczmarz-MP] Ratio regularizer ready.", flush=True)

        # ------------------- bands & pool -------------------
        print(f"[Kaczmarz-MP] Spinning up workers with {cfg.processes} processes...",
            flush=True)

        # Compute band layout from the requested process count
        print("[Kaczmarz-MP] Spinning up workers (entry)...", flush=True)
        nprocs_req = max(1, int(cfg.processes))
        print(f"[Kaczmarz-MP] nprocs_req = {nprocs_req}", flush=True)
        band_size = np.ceil(C / nprocs_req).astype(int)
        print(f"[Kaczmarz-MP] band_size = {band_size}", flush=True)
        bands: List[Tuple[int, int]] = []
        c0 = 0
        for i in range(nprocs_req):
            c1 = np.min((C, c0 + band_size))
            print(f"[Kaczmarz-MP] band loop i={i}, c0={c0}, c1={c1}", flush=True)
            if c1 > c0:
                bands.append((c0, c1))
            c0 = c1
        nprocs = len(bands)
        print(f"[Kaczmarz-MP] Using {nprocs} processes, band_size={band_size}.",
            flush=True)

        print(
            f"[Kaczmarz-MP] Using {nprocs} processes, band_size={band_size}.",
            flush=True,
        )

        # If we only have one band, we can skip mp.Pool entirely. This is both
        # a useful fallback and a very clean way to see if the hang is MP-only.
        use_pool = nprocs > 1

        if not use_pool:
            print(
                "[Kaczmarz-MP] Single-process mode "
                "(no multiprocessing pool will be used).",
                flush=True,
            )
            pool = None
        else:
            ctx_name = os.environ.get("CUBEFIT_MP_CTX", "forkserver")
            print(
                f"[Kaczmarz-MP] Using multiprocessing context '{ctx_name}'",
                flush=True,
            )
            try:
                ctx = mp.get_context(ctx_name)
            except ValueError:
                print(
                    "[Kaczmarz-MP] Context '{ctx_name}' unavailable; "
                    "falling back to 'spawn'.",
                    flush=True,
                )
                ctx = mp.get_context("spawn")

            # Anti-deadlock knobs in the parent
            os.environ.setdefault("KMP_INIT_AT_FORK", "FALSE")
            os.environ.setdefault("OMP_NUM_THREADS", str(cfg.blas_threads))
            os.environ.setdefault("OPENBLAS_NUM_THREADS", str(cfg.blas_threads))
            os.environ.setdefault("MKL_NUM_THREADS", str(cfg.blas_threads))

            max_tasks = int(os.environ.get("CUBEFIT_WORKER_MAXTASKS", "0"))
            ping_timeout = float(
                os.environ.get("CUBEFIT_POOL_PING_TIMEOUT", "5.0")
            )
            renew_each_epoch = os.environ.get(
                "CUBEFIT_POOL_RENEW_EVERY_EPOCH", "0"
            ).lower() not in ("0", "false", "no", "off")

            def _make_pool():
                print(
                    f"[Kaczmarz-MP] Creating pool with {nprocs} workers...",
                    flush=True,
                )
                ppool = ctx.Pool(
                    processes=nprocs,
                    initializer=_worker_init,
                    initargs=(int(cfg.blas_threads),),
                    maxtasksperchild=(None if max_tasks <= 0 else max_tasks),
                )
                print("[Kaczmarz-MP] Pool created.", flush=True)
                return ppool

            pool = _make_pool()
            print(
                "[Kaczmarz-MP] Workers ready. Starting epochs...",
                flush=True,
            )

        # Optional integer-shift diagnostic (cheap sampling)
        want_shift_diag = os.environ.get(
            "CUBEFIT_SHIFT_DIAG", "0"
        ).lower() not in ("0", "false", "no", "off")

        x = x_CP.ravel(order="C")   # view; keep and reuse everywhere

        try:
            for ep in range(cfg.epochs):
                if use_pool:
                    for ep in range(cfg.epochs):
                        if use_pool:
                            if (not _pool_ok(pool, timeout=ping_timeout)) or renew_each_epoch:
                                try:
                                    pool.close()
                                    pool.join()
                                except Exception:
                                    try:
                                        pool.terminate()
                                    except Exception:
                                        pass
                                pool = _make_pool()

                pbar = tqdm(total=len(s_ranges),
                            desc=f"[Kaczmarz-MP] epoch {ep+1}/{cfg.epochs}",
                            mininterval=2.0, dynamic_ncols=True)
                pbar.refresh()

                # Per-epoch target mixture t_norm (sum=1)
                if have_ratio:
                    if rc.anchor == "x0" and (t_x0 is not None):
                        t_epoch = t_x0.copy()
                    elif rc.anchor == "auto" and (t_x0 is not None):
                        t_epoch = t_x0.copy()
                    else:
                        t_epoch = w_target.copy()

                    if rc.epoch_renorm:
                        # keep a small floor to avoid zeros
                        t_epoch = np.maximum(t_epoch, rc.minw)
                        t_epoch = t_epoch / np.maximum(np.sum(t_epoch), 1.0)

                    t_norm = t_epoch
                else:
                    t_norm = None

                # --------- epoch-level RMSE proxy accumulator --------------
                # We sum ||R||^2 over all tiles using the *pre-update* R
                # (unweighted, masked wavelengths) and track the total count.
                rmse_sum_sq = 0.0
                rmse_count  = 0

                for tile_idx, (s0, s1) in enumerate(s_ranges):
                    Sblk = s1 - s0

                    # ---------- Build residual R = Y - yhat ----------
                    with open_h5(h5_path, role="reader") as f:
                        DC = f["/DataCube"]; M  = f["/HyperCube/models"]
                        try:
                            M.id.set_chunk_cache(cfg.dset_slots,
                                                cfg.dset_bytes, cfg.dset_w0)
                        except Exception:
                            pass

                        Y = np.asarray(DC[s0:s1, :], np.float64, order="C")
                        if keep_idx is not None:
                            Y = Y[:, keep_idx]

                        yhat = np.zeros((Sblk, Lk), np.float64)
                        for c in range(C):
                            A = np.asarray(M[s0:s1, c, :, :], np.float32, order="C")
                            if keep_idx is not None:
                                A = A[:, :, keep_idx]
                            if cp_flux_ref is not None:
                                # same cp normalization here
                                A = A * (inv_cp_flux_ref[c, :][None, :, None])
                            xc_norm = x_CP[c, :].astype(np.float64, copy=False)
                            for s in range(Sblk):
                                yhat[s, :] += xc_norm @ A[s, :, :]
                        R = Y - yhat


                    # --------- aggregate epoch RMSE proxy (pre-updates) -----
                    if not np.all(np.isfinite(R)):
                        bad = ~np.isfinite(R)
                        n_bad = int(bad.sum())
                        print(
                            f"[Kaczmarz-MP] WARNING: non-finite residuals on "
                            f"tile {tile_idx} (bad={n_bad}); zeroing.",
                            flush=True,
                        )
                        R = np.nan_to_num(
                            R, nan=0.0, posinf=0.0, neginf=0.0, copy=False
                        )
                    rmse_sum_sq += float(np.sum(R * R))
                    rmse_count  += int(R.size)

                    # Optional: light shift diagnostic on 1 spaxel in the tile
                    if want_shift_diag and Sblk > 0:
                        s_pick = s0
                        try:
                            y_obs = np.asarray(DC[s_pick, :], np.float64)
                            if keep_idx is not None:
                                y_obs = y_obs[keep_idx]
                            y_fit = yhat[s_pick - s0, :]
                            sh = _xcorr_int_shift(y_obs, y_fit)
                            if sh != 0:
                                print(f"[diag] spaxel {s_pick}: "
                                    f"data↔model integer shift = {sh} px")
                        except Exception:
                            pass

                    # ---------- RMSE BEFORE ANY UPDATES ----------
                    rmse_before = float(np.sqrt(np.mean(R * R)))

                    # Optional hard guard:
                    rmse_cap = float(os.environ.get("CUBEFIT_RMSE_ABORT",
                        "1e7"))
                    if (rmse_cap > 0.0) and (rmse_before > rmse_cap):
                        if tracker is not None:
                            tracker.on_batch_rmse(rmse_cap)
                        # Skip updates for this tile; keep x_CP and R as-is
                        continue

                    if tracker is not None:
                        tracker.on_batch_rmse(rmse_before)

                    # Cosine LR decay across tiles
                    if cfg.epochs >= 1:
                        frac = (tile_idx + 1) / len(s_ranges)
                        lr_tile = float(cfg.lr) * (0.5 + 0.5 * np.cos(
                            np.pi * frac
                        ))
                    else:
                        lr_tile = float(cfg.lr)

                    # ---------- Workers ----------
                    jobs = []
                    for (c_start, c_stop) in bands:
                        x_band = x_CP[c_start:c_stop, :].copy()
                        if x_CP_prior is not None:
                            x_prior_band = x_CP_prior[c_start:c_stop, :].copy()
                        else:
                            x_prior_band = None
                        E_band = E_global[c_start:c_stop, :]  # (band_size, P)
                        inv_ref_band = (
                            inv_cp_flux_ref[c_start:c_stop, :]
                            if cp_flux_ref is not None else None
                        )

                        jobs.append(
                            (
                                h5_path,
                                s0,
                                s1,
                                keep_idx,
                                c_start,
                                c_stop,
                                x_band,
                                x_prior_band,
                                float(lr_tile),
                                bool(cfg.project_nonneg),
                                R.copy(),
                                cfg.dset_slots,
                                cfg.dset_bytes,
                                cfg.dset_w0,
                                E_band,
                                float(beta_blend),
                                w_lam_sqrt,
                                inv_ref_band,
                            )
                        )

                    if use_pool:
                        results = pool.map(_worker_tile_job_with_R, jobs)
                    else:
                        # Single-process fallback: call the worker directly
                        results = [
                            _worker_tile_job_with_R(job) for job in jobs
                        ]

                    # --------------- aggregate & tile backtracking ---------------
                    R_delta_agg = np.zeros_like(R)
                    band_updates = []
                    cand_pairs = []
                    upd_energy_sq_total = 0.0

                    for item in results:
                        if isinstance(item, tuple) and len(item) == 3:
                            R_delta, dx_list, cands = item
                            if cands:
                                cand_pairs.extend(cands)
                        else:
                            R_delta, dx_list = item
                        R_delta_agg += R_delta
                        for tup in dx_list:
                            if len(tup) == 3:
                                c, dx_c, e2 = tup
                                upd_energy_sq_total += float(e2)
                            else:
                                c, dx_c = tup
                            band_updates.append((c, dx_c))

                    bt_steps_tile  = int(
                        np.max((0, int(os.environ.get("CUBEFIT_TILE_BT_STEPS", "6"))))
                    )
                    bt_factor_tile = float(
                        os.environ.get("CUBEFIT_TILE_BT_FACTOR", "0.5")
                    )

                    alpha = 1.0
                    rmse_after = float(
                        np.sqrt(np.mean((R + alpha * R_delta_agg) ** 2))
                    )
                    for _ in range(bt_steps_tile):
                        if rmse_after < rmse_before:
                            break
                        alpha *= bt_factor_tile
                        if alpha <= 0.0:
                            break
                        rmse_after = float(
                            np.sqrt(np.mean((R + alpha * R_delta_agg) ** 2))
                        )

                    if not np.isfinite(rmse_after):
                        alpha *= 0.0
                        rmse_after = rmse_before
                    if tracker is not None:
                        tracker.on_batch_rmse(rmse_after)

                    # -------- GLOBAL trust region (global energy & ||Y||) -------
                    if (upd_energy_sq_total > 0.0) and (Y_glob_norm > 0.0):
                        global_step_norm = float(
                            np.sqrt(upd_energy_sq_total)
                        ) * alpha
                        cap = float(tau_global * Y_glob_norm)
                        if global_step_norm > cap:
                            alpha *= float(cap / np.max((1e-12, global_step_norm)))

                    # ----------------- apply α ONCE to x and R -------------------
                    # ----------------- APPLY α (NaN-hardened) -----------------
                    # If backtracking produced nonsense, neutralize it.
                    if not np.isfinite(alpha):
                        alpha = 0.0

                    # R update: sanitize the aggregated residual step if needed.
                    if not np.all(np.isfinite(R_delta_agg)):
                        R_delta_agg = np.nan_to_num(
                            R_delta_agg, nan=0.0, posinf=0.0, neginf=0.0
                        )

                    R += alpha * R_delta_agg

                    # x updates: sanitize each band’s dx before applying.
                    for (c, dx_c) in band_updates:
                        if not np.all(np.isfinite(dx_c)):
                            # Zero-out any NaN/Inf in the step; keep shape, no copy.
                            dx_c = np.nan_to_num(dx_c, nan=0.0, posinf=0.0, neginf=0.0, copy=False)
                        x_CP[c, :] += alpha * dx_c
                        if cfg.project_nonneg:
                            np.maximum(x_CP[c, :], 0.0, out=x_CP[c, :])

                    # Safety net: if any NaNs still leaked into x_CP, zero them.
                    if not np.all(np.isfinite(x_CP)):
                        bad = ~np.isfinite(x_CP)
                        x_CP[bad] = 0.0

                    # ---------- optional ratio penalty --------------------------
                    if have_ratio and ((tile_idx % rc.tile_every) == 0):
                        _ratio_update_in_place(x_CP)

                    # ---------- Optional: tile-local NNLS polish ----------------
                    nnls_enable = os.environ.get(
                        "CUBEFIT_NNLS_ENABLE", "1"
                    ).lower() not in ("0", "false", "no", "off")
                    nnls_every = int(os.environ.get("CUBEFIT_NNLS_EVERY", "5"))
                    nnls_max_cols = int(
                        os.environ.get("CUBEFIT_NNLS_MAX_COLS", "256")
                    )
                    nnls_max_bytes = int(os.environ.get(
                        "CUBEFIT_NNLS_MAX_BYTES", str(1_000_000_000)
                    ))
                    nnls_sub_L = int(os.environ.get("CUBEFIT_NNLS_SUB_L", "0"))
                    nnls_solver = os.environ.get(
                        "CUBEFIT_NNLS_SOLVER", "nnls"
                    ).lower()  # "pg"|"fista"|"nnls"|"lsq"
                    nnls_max_iter = int(
                        os.environ.get("CUBEFIT_NNLS_MAX_ITER", "200")
                    )
                    nnls_min_improve = float(os.environ.get("CUBEFIT_NNLS_MIN_IMPROVE", "0.9995"))
                    nnls_l2 = float(os.environ.get("CUBEFIT_NNLS_L2", "0.0"))
                    if not np.isfinite(nnls_l2) or nnls_l2 < 0.0:
                        nnls_l2 = 0.0
                    # allow a small worsening of ratio misfit when polishing (>=1 means allow)
                    # e.g. 1.02 allows up to +2% worse local ratio misfit; 1.00 = never worsen.
                    nnls_ratio_worsen = float(os.environ.get("CUBEFIT_NNLS_RATIO_WORSEN", "1.02"))

                    do_nnls = (
                        nnls_enable and ((tile_idx % nnls_every) == 0)
                        and (len(cand_pairs) > 0)
                    )

                    if do_nnls:
                        # Deduplicate by best score per global index
                        idxs = np.asarray(
                            [int(t[0]) for t in cand_pairs], np.int64
                        )
                        scrs = np.asarray(
                            [float(t[1]) for t in cand_pairs], np.float64
                        )
                        best = {}
                        for i, sc in zip(idxs, scrs):
                            if (i not in best) or (sc > best[i]):
                                best[i] = sc
                        uniq_idx = np.fromiter(best.keys(), dtype=np.int64)
                        uniq_scr = np.fromiter(best.values(), dtype=np.float64)
                        order = np.argsort(uniq_scr)[::-1]
                        uniq_idx = uniq_idx[order]

                        Lk_loc = int(R.shape[1])
                        if (nnls_sub_L > 0) and (Lk_loc > nnls_sub_L):
                            rng = np.random.default_rng(12345 + tile_idx)
                            lam_sel = np.sort(
                                rng.choice(Lk_loc, size=int(nnls_sub_L),
                                        replace=False)
                            )
                        else:
                            lam_sel = np.arange(Lk_loc, dtype=np.int64)

                        rows = int(Sblk * lam_sel.size)
                        bytes_per_col = int(rows * 8)  # float64
                        cap_by_mem = int(np.max((1, nnls_max_bytes // np.max((1, bytes_per_col)))))
                        K_use = int(np.min((uniq_idx.size, nnls_max_cols, cap_by_mem)))

                        if (K_use >= 2) and (rows >= 2):
                            W_idx = uniq_idx[:K_use]

                            # λ-weights on the subsample
                            if w_lam_sqrt is not None:
                                wlam_sel = w_lam_sqrt[lam_sel].astype(
                                    np.float64, copy=False
                                )
                            else:
                                wlam_sel = np.ones(lam_sel.size, np.float64)
                            sqrt_w_rows = np.tile(wlam_sel, Sblk).astype(
                                np.float64
                            )  # (rows,)

                            # Build B and xW on the subsample
                            with open_h5(h5_path, role="reader") as f:
                                M = f["/HyperCube/models"]
                                PP = int(M.shape[2])
                                groups = {}
                                for j, g in enumerate(W_idx):
                                    cc = int(g // PP); pp = int(g % PP)
                                    groups.setdefault(cc, []).append((pp, j))

                                B = np.empty((rows, K_use), dtype=np.float64)
                                xW = np.zeros((K_use,), dtype=np.float64)
                                xW_prior = np.zeros(
                                    (K_use,), dtype=np.float64
                                )

                                for cc, plist in groups.items():
                                    A_c = np.asarray(M[s0:s1, cc, :, :],
                                                    np.float32, order="C")
                                    if keep_idx is not None:
                                        A_c = A_c[:, :, keep_idx]
                                    A_c = A_c[:, :, lam_sel]
                                    for pp, j in plist:
                                        col = np.asarray(
                                            M[s0:s1, cc, pp, :],
                                            dtype=np.float64,
                                        )
                                        if keep_idx is not None:
                                            col = col[:, keep_idx]
                                        if cp_flux_ref is not None:
                                            col *= float(
                                                inv_cp_flux_ref[int(cc),
                                                                int(pp)]
                                            )
                                        B[:, int(j)] = col
                                        xW[int(j)] = float(
                                            x_CP[int(cc), int(pp)]
                                        )
                                        xW_prior[int(j)] = float(
                                            x_CP_prior[int(cc), int(pp)]
                                        )


                            r_sub = R[:, lam_sel].reshape(rows, order="C")
                            y_rhs = B @ xW + r_sub

                            # Weighted system
                            B_w     = B * sqrt_w_rows[:, None]
                            y_rhs_w = y_rhs * sqrt_w_rows

                            # Optional L2 via Tikhonov augmentation:
                            # J(x) = 0.5||B_w x - y_rhs_w||^2
                            #      + 0.5*nnls_l2*||x||^2
                            if nnls_l2 > 0.0:
                                lam_sqrt = float(np.sqrt(nnls_l2))
                                # Augment with sqrt(λ) I and 0 target
                                B_aug = np.vstack([
                                    B_w,
                                    lam_sqrt * np.eye(K_use, dtype=np.float64)
                                ])
                                y_aug = np.concatenate([
                                    y_rhs_w,
                                    np.zeros(K_use, dtype=np.float64)
                                ])
                            else:
                                B_aug = B_w
                                y_aug = y_rhs_w

                            xW_new = None
                            if nnls_solver in ("pg", "fista"):
                                # Column-normalize local system (conditioning)
                                col_norm = np.linalg.norm(B_w, axis=0)
                                col_norm = np.where(
                                    col_norm > 0.0, col_norm, 1.0
                                )
                                Bn = B_w / col_norm
                                x = xW / col_norm

                                if nnls_l2 > 0.0:
                                    x_prior_norm = xW_prior / col_norm
                                else:
                                    x_prior_norm = np.zeros_like(x)

                                # FISTA with backtracking (fast & robust)
                                if K_use <= 2048:
                                    L_est = float(
                                        np.linalg.norm(Bn, ord=2) ** 2
                                        + nnls_l2
                                    )
                                else:
                                    L_est = float(
                                        (Bn ** 2).sum(axis=0).max()
                                        + nnls_l2
                                    )

                                t = 1.0
                                z = np.maximum(0.0, x.copy())
                                x_old = z.copy()

                                def _f_and_grad(zvec):
                                    r = Bn @ zvec - y_rhs_w
                                    if nnls_l2 > 0.0:
                                        diff = zvec - x_prior_norm
                                        f = (
                                            0.5 * float(r @ r)
                                            + 0.5 * nnls_l2
                                            * float(diff @ diff)
                                        )
                                        g = Bn.T @ r + nnls_l2 * diff
                                    else:
                                        f = 0.5 * float(r @ r)
                                        g = Bn.T @ r
                                    return f, g, r

                                fz, gz, rz = _f_and_grad(z)
                                step = 1.0 / max(L_est, 1e-6)

                                for _it in range(nnls_max_iter):
                                    # Armijo backtracking
                                    while True:
                                        x_try = z - step * gz
                                        np.maximum(x_try, 0.0, out=x_try)
                                        r_try = Bn @ x_try - y_rhs_w
                                        if nnls_l2 > 0.0:
                                            diff_try = x_try - x_prior_norm
                                            f_try = (
                                                0.5 * float(r_try @ r_try)
                                                + 0.5 * nnls_l2
                                                * float(diff_try @ diff_try)
                                            )
                                        else:
                                            f_try = 0.5 * float(r_try @ r_try)

                                        if (
                                            f_try
                                            <= fz
                                            - 1e-4 * step * float(gz @ gz)
                                            or step < 1e-12
                                        ):
                                            break
                                        step *= 0.5

                                    # FISTA momentum
                                    t_new = 0.5 * (
                                        1.0 + np.sqrt(1.0 + 4.0 * t * t)
                                    )
                                    z = x_try + ((t - 1.0) / t_new) * (
                                        x_try - x_old
                                    )
                                    x_old = x_try
                                    t = t_new
                                    fz, gz, rz = _f_and_grad(z)
                                    if float(rz @ rz) < 1e-12:
                                        break

                                xW_new = x_try * col_norm

                            elif nnls_solver == "lsq":
                                try:
                                    from scipy.optimize import lsq_linear
                                    res = lsq_linear(
                                        B_aug, y_aug,
                                        bounds=(0.0, np.inf),
                                        method="trf",
                                        max_iter=nnls_max_iter,
                                        verbose=0,
                                    )
                                    xW_new = np.maximum(0.0, res.x)
                                except Exception:
                                    xW_new = None
                            elif nnls_solver == "nnls":
                                try:
                                    from scipy.optimize import nnls as _scipy_nnls
                                    xW_new, _ = _scipy_nnls(B_aug, y_aug)
                                except Exception:
                                    xW_new = None

                            # Commit only if we actually solved something and it helped enough
                            if xW_new is not None:
                                dxW = xW_new - xW
                                if np.any(dxW != 0.0):

                                    # --- Weighted RMSE accept criterion (unchanged) ---
                                    def _wrmse(vec_1d: np.ndarray) -> float:
                                        w2 = sqrt_w_rows * sqrt_w_rows
                                        num = float(np.dot(w2, vec_1d * vec_1d))
                                        den = float(np.sum(w2)) or 1.0
                                        return math.sqrt(num / den)

                                    r_after = r_sub - (B @ dxW)   # NOTE: unweighted residual update
                                    rmse_before = _wrmse(r_sub)
                                    rmse_after  = _wrmse(r_after)
                                    ok_rmse = (rmse_after / max(1e-12, rmse_before)) < nnls_min_improve

                                    # --- ratio-drift guard vs global prior w_prior ---
                                    ok_ratio = True
                                    if have_ratio:
                                        # totals before
                                        s_before = x_CP.sum(axis=1)             # (C,)
                                        S_b = float(np.sum(s_before))
                                        if S_b > 0.0:
                                            mix_b = s_before / S_b

                                            # apply dxW only to the touched (c,p) indices
                                            comp_of_j = (W_idx // P).astype(np.int64)
                                            dS = np.zeros(C, dtype=np.float64)
                                            np.add.at(dS, comp_of_j, dxW)

                                            s_after = s_before + dS
                                            S_a = float(np.sum(s_after))
                                            if S_a > 0.0:
                                                mix_a = s_after / S_a
                                                # L1 deviation from target mixture w_prior
                                                dev_b = float(np.sum(np.abs(mix_b - w_prior)))
                                                dev_a = float(np.sum(np.abs(mix_a - w_prior)))
                                                ok_ratio = (dev_a <= dev_b * nnls_ratio_worsen + 1e-18)

                                    if ok_rmse and ok_ratio:
                                        # Commit to x_CP and to the *unweighted* residual R
                                        for j, g in enumerate(W_idx):
                                            cc = int(g // P); pp = int(g % P)
                                            x_CP[cc, pp] = float(max(0.0, x_CP[cc, pp] + dxW[j]))
                                        R[:, lam_sel] = r_after.reshape(Sblk, lam_sel.size, order="C")
                                    # else: reject this NNLS polish

                    # ---------- end NNLS polish ----------

                    if tracker is not None:
                        tracker.on_progress(
                            epoch=ep + 1,
                            spax_done=tile_idx + 1,
                            spax_total=len(s_ranges),
                            rmse_ewma=None
                        )
                        tracker.maybe_snapshot_x(x_CP, epoch=ep,
                                                rmse=rmse_after)

                    pbar.update(1)
                    pbar.refresh()

                pbar.close()

                # --------- finalize epoch-level RMSE proxy -----------------
                if rmse_count > 0:
                    mean_sq = rmse_sum_sq / float(rmse_count)
                else:
                    mean_sq = 0.0

                if (not np.isfinite(mean_sq)) or (mean_sq < 0.0):
                    print(
                        f"[Kaczmarz-MP] WARNING: epoch {ep+1} RMSE(proxy) "
                        f"mean_sq={mean_sq!r} non-finite or negative; "
                        f"setting RMSE(proxy)=+inf.",
                        flush=True,
                    )
                    rmse_epoch_proxy = float("inf")
                else:
                    rmse_epoch_proxy = float(math.sqrt(mean_sq))

                print(
                    f"[Kaczmarz-MP] epoch {ep+1} RMSE(proxy) = "
                    f"{rmse_epoch_proxy:.6e}",
                    flush=True,
                )

                if tracker is not None:
                    try:
                        tracker.on_epoch_end(
                            ep + 1,
                            {"rmse_epoch_proxy": rmse_epoch_proxy},
                            block=False,
                        )
                    except TypeError:
                        tracker.on_epoch_end(
                            ep + 1,
                            {"rmse_epoch_proxy": rmse_epoch_proxy},
                        )

                if w_prior is not None:
                    t0_sb = time.perf_counter()
                    orbBand, orbStep = softbox_params_smooth(
                        eq=ep, E=cfg.epochs)
                    cu.apply_component_softbox_energy(
                        x_CP, E_global,
                        (orbit_weights if orbit_weights is not None else np.ones(x_CP.shape[0])),
                        band=float(orbBand), step=float(orbStep),
                        min_target=1e-10
                    )

                    dt_sb = time.perf_counter() - t0_sb
                    print(f"[softbox] epoch {ep+1}: took {dt_sb:.4f}s",
                        flush=True)

                # ---------- optional orbit weights enforcement -----------
                t0_ob = time.perf_counter()
                if orbit_weights is not None and rc is not None and rc.epoch_project:
                    cu.project_to_component_weights(
                        x_CP,
                        orbit_weights,
                        E_cp=E_global,     # (C,P) from /HyperCube/col_energy
                        minw=1e-10,
                        beta=rc.epoch_beta  # e.g. 0.1–0.3
                    )

                    dt_ob = time.perf_counter() - t0_ob
                    print(f"[orbit-weights] epoch {ep+1}: took {dt_ob:.4f}s",
                        flush=True)
                    t0_ob = time.perf_counter()
                    x[:] = x_CP.ravel(order="C")  # keep your flattened view in sync
                    dt_ob = time.perf_counter() - t0_ob
                    print(f"[orbit-weights] epoch {ep+1}: sync took {dt_ob:.4f}s",
                        flush=True)

                    t0_usage = time.perf_counter()
                    # --- global energy L1-to-target diagnostic (robust) ---
                    try:
                        if E_global is not None:
                            print("[global energy] computing L1-to-target...", flush=True)
                            X64 = np.asarray(x_CP, dtype=np.float64)
                            E64 = np.asarray(E_global, dtype=np.float64)

                            # Sanitize E_global and X64 to avoid NaN/Inf poisoning
                            bad_E = ~np.isfinite(E64)
                            bad_X = ~np.isfinite(X64)
                            n_bad_E = int(bad_E.sum())
                            n_bad_X = int(bad_X.sum())
                            if n_bad_E or n_bad_X:
                                print(f"[global energy] WARNING: non-finite entries detected "
                                    f"in X/E (X bad={n_bad_X}, E bad={n_bad_E}); zeroing.",
                                    flush=True)
                                if n_bad_E:
                                    E64[bad_E] = 0.0
                                if n_bad_X:
                                    X64[bad_X] = 0.0

                            # Energy–weighted usage per component
                            s = (X64 * E64).sum(axis=1)  # (C,)
                        else:
                            # Plain usage
                            X64 = np.asarray(x_CP, dtype=np.float64)
                            bad_X = ~np.isfinite(X64)
                            if bad_X.any():
                                print(f"[global energy] WARNING: non-finite entries in X "
                                    f"(bad={int(bad_X.sum())}); zeroing.", flush=True)
                                X64[bad_X] = 0.0
                            s = X64.sum(axis=1)

                        # Now compute L1 safely
                        s = np.maximum(s, 0.0)
                        S = float(np.sum(s))
                        if not np.isfinite(S) or S <= 0.0:
                            print("[global energy] S is non-finite or <=0; skipping L1 diagnostic.",
                                flush=True)
                        else:
                            s_frac = s / S

                            # t is your target mix; make sure it's finite too
                            t_vec = (t_x0 if (rc.anchor == "x0" and t_x0 is not None) else w_target)
                            t = np.asarray(t_vec, np.float64).ravel(order="C")
                            if t.size == C * P:
                                t = t.reshape(C, P, order="C").sum(axis=1)
                            elif t.size != C:
                                raise ValueError(f"[ratio] target len {t.size} not in {{C, C*P}}")

                            t = np.maximum(np.nan_to_num(t, nan=0.0, posinf=0.0,
                                neginf=0.0),
                                rc.minw if rc is not None else 1e-10)
                            T = float(np.sum(t))
                            if not np.isfinite(T) or T <= 0.0:
                                print("[global energy] target sum non-finite or <=0; skipping L1.",
                                    flush=True)
                            else:
                                t_frac = t / T
                                diff = s_frac - t_frac
                                # clean diff just in case
                                diff = np.nan_to_num(diff, nan=0.0, posinf=0.0, neginf=0.0)
                                l1 = float(np.sum(np.abs(diff)))
                                print(f"[ratio] epoch L1-to-target = {l1:.5e}", flush=True)

                    except Exception as e:
                        # Last-resort guard so the solver never 'hangs' here
                        print(f"[global energy] ERROR while computing L1-to-target: {e!r}",
                            flush=True)
                    dt_usage = time.perf_counter() - t0_usage
                    print(f"[ratio] epoch {ep+1}: usage calc took {dt_usage:.4f}s",
                        flush=True)

                print(f"[Kaczmarz-MP] epoch {ep+1}/{cfg.epochs} snapshotting...",
                    flush=True)

                try:
                    if tracker is not None:
                        tracker.maybe_snapshot_x(x_CP, epoch=ep+1,
                            rmse=rmse_after, force=True)
                except Exception:
                    pass

                # # ------------------- global RMSE for this epoch -------------
                # print(
                #     f"[Kaczmarz-MP] Computing global RMSE for "
                #     f"epoch {ep+1}...",
                #     flush=True,
                # )
                # rmse_epoch_raw = _compute_global_rmse(
                #     h5_path,
                #     x_CP,
                #     s_ranges=s_ranges,
                #     keep_idx=keep_idx,
                #     w_lam_sqrt=w_lam_sqrt,
                #     cp_flux_ref=cp_flux_ref,
                #     inv_cp_flux_ref=inv_cp_flux_ref,
                #     dset_slots=cfg.dset_slots,
                #     dset_bytes=cfg.dset_bytes,
                #     dset_w0=cfg.dset_w0,
                #     weighted=rmse_weighted,
                # )
                # rmse_epoch = _safe_scalar_rmse(rmse_epoch_raw, "epoch")
                # print(
                #     f"[Kaczmarz-MP] epoch {ep+1} global RMSE = "
                #     f"{rmse_epoch:.6e}",
                #     flush=True,
                # )

                # Track best epoch by global RMSE (proxy)
                if rmse_epoch_proxy + rmse_guard_tol < best_rmse_proxy:
                    best_rmse_proxy = rmse_epoch_proxy
                    best_x_CP = x_CP.copy()
                    print(
                        f"[Kaczmarz-MP] New best RMSE "
                        f"{best_rmse_proxy:.6e} at epoch {ep+1}.",
                        flush=True,
                    )
                else:
                    print(
                        f"[Kaczmarz-MP] RMSE did not improve over "
                        f"best={best_rmse_proxy:.6e}.",
                        flush=True,
                    )

                print(f"[Kaczmarz-MP] epoch {ep+1}/{cfg.epochs} housekeeping "
                    f"done.", flush=True)   

            elapsed = time.perf_counter() - t0

            if np.isfinite(best_rmse_proxy):
                x_CP[:, :] = best_x_CP

            # Sanitize the final solution and convert to physical basis
            # np.nan_to_num(
            #     x_CP, nan=0.0, posinf=0.0, neginf=0.0, copy=False
            # )
            np.nan_to_num(
                best_x_CP, nan=0.0, posinf=0.0, neginf=0.0, copy=False
            )

            if cp_flux_ref is not None:
                # Decode normalized solution back to physical weights
                X_norm = best_x_CP
                X_phys = X_norm * inv_cp_flux_ref   # x_phys = x_norm / cp_flux_ref
            else:
                X_phys = best_x_CP

            x_out = np.asarray(
                X_phys, np.float64, order="C"
            ).ravel(order="C")

            return x_out, dict(
                epochs=cfg.epochs,
                elapsed_sec=elapsed,
                # rmse_seed=rmse_seed,
                rmse_best=best_rmse_proxy,
            )
        finally:
            if pool is not None:
                try:
                    pool.close()
                    pool.join()
                except Exception:
                    try:
                        pool.terminate()
                    except Exception:
                        pass

    except Exception as e:
        print(
            "[Kaczmarz-MP] FATAL exception in "
            "solve_global_kaczmarz_cchunk_mp:",
            flush=True,
        )
        print(traceback.format_exc(), flush=True)
        # re-raise so the pipeline still fails loudly
        raise

# ------------------------------------------------------------------------------

def solve_global_kaczmarz_global_step_mp(
    h5_path: str,
    cfg: MPConfig,
    *,
    orbit_weights: Optional[np.ndarray] = None,
    x0: Optional[np.ndarray] = None,
    tracker: Optional[object] = None,
    ratio_cfg: cu.RatioCfg | None = None,
) -> tuple[np.ndarray, dict]:
    """
    Global-step Kaczmarz/gradient solver over the full HyperCube.

    This variant treats the whole cube as a single least-squares problem
    with objective

        J(x) = 0.5 * sum_{s,λ} w_λ (Y_{s,λ} - yhat_{s,λ}(x))^2
               + 0.5 * λ * ||x||^2,

    where x is the component–population weight matrix in the solver's
    normalized basis (x_CP). Each epoch does:

    1. Sweep over all spatial tiles, using the current x_CP to build the
       residual R_tile = Y_tile - yhat_tile(x_CP).
    2. For each tile and band of components, accumulate local contributions
       to the global gradient g_CP and diagonal preconditioner D_CP using
       `_worker_tile_global_grad_band`.
    3. After all tiles, take a single global projected step

           dx = lr * (g_eff / D_eff),
           x_CP <- max(0, x_CP + dx),

       with an optional global trust-region cap based on E_global and
       ||Y||.

    There are **no per-tile Kaczmarz updates** in this solver, no tile-level
    line search, and no tile-local NNLS polish. The only objective it
    optimizes is the global LS (plus optional L2), making its behaviour
    much easier to interpret relative to the NNLS seed.

    Parameters
    ----------
    h5_path : str
        Path to the HDF5 cube with /DataCube and /HyperCube/models.
    cfg : MPConfig
        Configuration for tiling, multiprocessing, epochs, LR, etc.
    orbit_weights : ndarray or None, optional
        Ignored in the pure-LS global step, but kept for API parity.
        Global orbit/ratio regularization is not applied here.
    x0 : ndarray or None, optional
        Initial weights in the **physical** basis, shape (C*P,). If None,
        start from zeros in the solver's normalized basis.
    tracker : object or None, optional
        Optional FitTracker-like object with methods:
            - on_batch_rmse(rmse: float)
            - on_progress(epoch, spax_done, spax_total, rmse_ewma=None)
            - on_epoch_end(epoch, stats_dict, block=True/False)
            - maybe_snapshot_x(x_CP, epoch, rmse, force=False)
    ratio_cfg : cu.RatioCfg or None, optional
        Ignored in this pure-LS solver. Present only for API compatibility.

    Returns
    -------
    x_out : ndarray, shape (C*P,), float64
        Final physical component–population weights, flattened in C-major
        order.
    info : dict
        Dictionary with basic run metadata, currently:
            - epochs : int
            - elapsed_sec : float

    Notes
    -----
    • This solver assumes `mode='model'` normalization on the HyperCube.
    • Column-flux normalization (`cp_flux_ref`) is allowed but disabled in
      your current runs; when present it is handled in the "normalized"
      basis internally and decoded back to physical at the end.
    • All global orbit/ratio/softbox regularization is intentionally
      disabled here to focus purely on the LS objective. You can layer it
      back later if this behaves well.
    """
    try:
        t0 = time.perf_counter()

        def _set_chunk_cache(dset, cfg_obj):
            try:
                dset.id.set_chunk_cache(
                    cfg_obj.dset_slots, cfg_obj.dset_bytes, cfg_obj.dset_w0
                )
            except Exception:
                pass

        # Normalization mode sanity check
        mode = _assert_norm_mode(h5_path, expect=None)
        print(f"[Kaczmarz-GLOBAL] HyperCube norm.mode = '{mode}'",
              flush=True)

        # ------------------- read dims, mask, cp_flux_ref -------------------
        with open_h5(h5_path, role="reader") as f:
            DC = f["/DataCube"]           # (S, L) float64
            M  = f["/HyperCube/models"]   # (S, C, P, L) float32
            _set_chunk_cache(M, cfg)

            S, L = map(int, DC.shape)
            _, C, P, Lm = map(int, M.shape)
            if Lm != L:
                raise RuntimeError(
                    f"L mismatch: models L={Lm} vs data L={L}."
                )

            mask = cu._get_mask(f) if cfg.apply_mask else None
            keep_idx = np.flatnonzero(mask) if mask is not None else None
            Lk = int(keep_idx.size) if keep_idx is not None else L

            # Column-flux scaling (model basis), explicitly gated by env
            cp_enable = os.environ.get(
                "CUBEFIT_CP_FLUX_ENABLE", "0"
            ).lower() not in ("0", "false", "no", "off")

            if cp_enable:
                cp_flux_ref = cu._ensure_cp_flux_ref(
                    h5_path, keep_idx=keep_idx
                )  # shape (C, P) or None
            else:
                cp_flux_ref = None

            if cp_flux_ref is not None:
                print(
                    "[Kaczmarz-GLOBAL] Using column-flux scaling.",
                    flush=True,
                )
                cp_flux_ref = np.asarray(
                    cp_flux_ref, np.float64
                ).reshape(C, P)
                print(
                    "[Kaczmarz-GLOBAL] cp_flux_ref: "
                    f"min={np.min(cp_flux_ref):.3e}, max={np.max(cp_flux_ref):.3e}, median={np.median(cp_flux_ref[cp_flux_ref > 0.0]):.3e}",
                    flush=True,
                )
                inv_cp_flux_ref = 1.0 / np.maximum(
                    cp_flux_ref, 1.0e-30
                )
            else:
                cp_flux_ref = None
                inv_cp_flux_ref = None

            # λ-weights (feature emphasis)
            lamw_enable = os.environ.get(
                "CUBEFIT_LAMBDA_WEIGHTS_ENABLE", "1"
            ).lower() not in ("0", "false", "no", "off")
            lamw_dset = os.environ.get(
                "CUBEFIT_LAMBDA_WEIGHTS_DSET",
                "/HyperCube/lambda_weights",
            )
            lamw_floor = float(
                os.environ.get("CUBEFIT_LAMBDA_MIN_W", "1e-6")
            )
            lamw_auto = os.environ.get(
                "CUBEFIT_LAMBDA_WEIGHTS_AUTO", "1"
            ).lower() not in ("0", "false", "no", "off")

            if lamw_enable:
                print(
                    "[Kaczmarz-GLOBAL] Reading λ-weights from "
                    f"'{lamw_dset}' (floor={lamw_floor}, auto={lamw_auto})",
                    flush=True,
                )
                try:
                    w_full = cu.read_lambda_weights(
                        h5_path, dset_name=lamw_dset, floor=lamw_floor
                    )
                except Exception:
                    w_full = (
                        cu.ensure_lambda_weights(
                            h5_path, dset_name=lamw_dset
                        )
                        if lamw_auto
                        else np.ones(L, dtype=np.float64)
                    )
                print(
                    "[Kaczmarz-GLOBAL] λ-weights "
                    f"min={np.min(w_full):.3e}, max={np.max(w_full):.3e}, mean={np.mean(w_full):.3e}",
                    flush=True,
                )
                w_lam = w_full[keep_idx] if keep_idx is not None else w_full
                w_lam_sqrt = np.sqrt(
                    np.clip(w_lam, lamw_floor, None)
                ).astype(np.float64, order="C")
            else:
                w_lam_sqrt = None

            # Spaxel tiling from HyperCube chunking
            s_tile = int(M.chunks[0]) if (M.chunks and M.chunks[0] > 0) \
                else 128
            s_ranges = [
                (s0, int(np.min((S, s0 + s_tile))))
                for s0 in range(0, S, s_tile)
            ]

            # Tile norms and global ||Y||
            norms = []
            Y_glob_norm2 = 0.0
            for (ss0, ss1) in s_ranges:
                Yt = np.asarray(DC[ss0:ss1, :], np.float64)
                if keep_idx is not None:
                    Yt = Yt[:, keep_idx]
                norms.append(float(np.linalg.norm(Yt)))
                Y_glob_norm2 += float(np.sum(Yt * Yt))

        print(
            f"[Kaczmarz-GLOBAL] DataCube S={S}, L={L} (kept Lk={Lk}), "
            f"Hypercube C={C}, P={P}, s_tile={s_tile}, epochs={cfg.epochs}, lr={cfg.lr}, "
            f"processes={cfg.processes}, blas_threads={cfg.blas_threads}",
            flush=True,
        )

        # # Sort tiles by descending norm (brightest first)
        # paired = sorted(
        #     zip(norms, s_ranges), key=lambda t: -t[0]
        # )
        # norms_sorted = [t[0] for t in paired]
        # s_ranges_sorted = [t[1] for t in paired]

        # # Optional tile budget (max_tiles)
        # max_tiles = cfg.max_tiles
        # print("[Kaczmarz-GLOBAL] Applying max_tiles constraint...",
        #       flush=True)
        # if max_tiles is not None:
        #     max_tiles = int(max_tiles)
        #     if 0 < max_tiles < len(s_ranges_sorted):
        #         import math
        #         rng_sel = np.random.default_rng(
        #             int(os.environ.get(
        #                 "CUBEFIT_POLISH_SEED", "12345"
        #             ))
        #         )

        #         k_bright = max(1, int(math.ceil(0.7 * max_tiles)))
        #         k_rand = max(0, max_tiles - k_bright)

        #         bright_idx = np.arange(k_bright, dtype=int)
        #         tail_idx = np.arange(
        #             k_bright, len(s_ranges_sorted), dtype=int
        #         )
        #         if k_rand > 0 and tail_idx.size > 0:
        #             rng_sel.shuffle(tail_idx)
        #             rand_idx = tail_idx[:k_rand]
        #             keep_idx_tiles = np.concatenate(
        #                 [bright_idx, rand_idx]
        #             )
        #         else:
        #             keep_idx_tiles = bright_idx

        #         keep_idx_tiles = np.sort(keep_idx_tiles)
        #         s_ranges = [s_ranges_sorted[i] for i in keep_idx_tiles]
        #         norms_sorted = [norms_sorted[i] for i in keep_idx_tiles]
        #     else:
        #         s_ranges = s_ranges_sorted
        # else:
        #     s_ranges = s_ranges_sorted

        print("[Kaczmarz-MP] Applying max_tiles constraint...", flush=True)
        max_tiles = cfg.max_tiles
        # ---- Build tile list ----
        s_ranges_all = list(s_ranges)  # keep the full list for fairness

        # ---------------- probe tiles for accept/backtracking ----------------
        n_probe = int(os.environ.get("CUBEFIT_PROBE_TILES", "6"))
        n_probe = max(1, min(n_probe, len(s_ranges_all)))

        probe_seed = int(os.environ.get("CUBEFIT_PROBE_SEED", "24680"))
        probe_tiles = _choose_tiles_fair_spread(
            s_ranges, n_probe, seed=probe_seed
        )

        probe_growth_max = float(
            os.environ.get("CUBEFIT_PROBE_GROWTH_MAX", "1.10")
        )
        if (not np.isfinite(probe_growth_max)) or (probe_growth_max <= 1.0):
            probe_growth_max = 1.10

        bt_max = int(os.environ.get("CUBEFIT_PROBE_BT_MAX", "8"))
        bt_max = max(1, bt_max)

        bt_shrink = float(os.environ.get("CUBEFIT_PROBE_BT_SHRINK", "0.5"))
        if (not np.isfinite(bt_shrink)) or (bt_shrink <= 0.0) or (bt_shrink >= 1.0):
            bt_shrink = 0.5

        allow_sign_flip = os.environ.get(
            "CUBEFIT_PROBE_SIGN_FLIP", "1"
        ).lower() not in ("0", "false", "no", "off")

        tile_norms: list[tuple[float, tuple[int, int]]] = []
        with open_h5(h5_path, role="reader") as f:
            DC = f["/DataCube"]              # (S, L)
            for (s0, s1) in s_ranges_all:
                Y = np.asarray(DC[s0:s1, :], np.float64, order="C")
                if keep_idx is not None:
                    Y = Y[:, keep_idx]       # (Sblk, Lk)
                tile_norms.append((float(np.linalg.norm(Y)), (s0, s1)))

        # Brightness ordering (optional, but fine as an iteration order)
        tile_norms.sort(key=lambda t: t[0], reverse=True)

        if max_tiles is not None and max_tiles < len(s_ranges_all):
            # Fair subset in space, then keep brightness ordering within that subset.
            fair_keep = _choose_tiles_fair_spread(
                s_ranges_all,
                int(max_tiles),
                seed=int(os.environ.get("CUBEFIT_GLOBAL_TILE_SEED", "12345")),
            )
            fair_keep_set = set(fair_keep)
            s_ranges = [t for _, t in tile_norms if t in fair_keep_set]
        else:
            s_ranges = [t for _, t in tile_norms]

        print(
            f"[Kaczmarz-GLOBAL] Using {len(s_ranges)} tiles for fitting.",
            flush=True,
        )

        Y_glob_norm = float(np.sqrt(Y_glob_norm2))

        # Global column energy (for trust-region scaling)
        print("[Kaczmarz-GLOBAL] Reading global column energy...",
            flush=True)
        E_global = read_global_column_energy(h5_path)  # (C, P) float64
        print("[Kaczmarz-GLOBAL] done.", flush=True)

        # ------------------- optional orbit_weights prior -------------------
        # Here we only prepare the target mix; the actual use is an optional
        # projection after each global step (see below).
        w_prior = None
        orbit_weights_arr = None
        if orbit_weights is not None:
            orbit_weights_arr = np.asarray(
                orbit_weights, np.float64
            ).ravel(order="C")
            if orbit_weights_arr.size != C:
                raise ValueError(
                    f"orbit_weights length {orbit_weights_arr.size} != C={C}."
                )
            w_prior = orbit_weights_arr.copy()

            if tracker is not None and hasattr(
                tracker, "set_orbit_weights"
            ):
                try:
                    tracker.set_orbit_weights(w_prior)
                except Exception:
                    pass

        tau_global = float(
            os.environ.get("CUBEFIT_GLOBAL_TAU", "0.5")
        )
        beta_blend = float(
            os.environ.get("CUBEFIT_GLOBAL_ENERGY_BLEND", "1e-2")
        )

        # ------------------- x initialization -------------------
        if x0 is None:
            x_CP = np.zeros((C, P), dtype=np.float64, order="C")
        else:
            x0 = np.asarray(x0, np.float64).ravel(order="C")
            if x0.size != C * P:
                raise ValueError(
                    f"x0 length {x0.size} != C*P={C*P}."
                )
            X_phys = x0.reshape(C, P)  # physical basis
            if cp_flux_ref is not None:
                x_CP = X_phys * cp_flux_ref
            else:
                x_CP = X_phys.copy(order="C")

        # Small symmetry breaking if starting from exact zero
        sym_eps = float(
            os.environ.get("CUBEFIT_SYMBREAK_EPS", "1e-6")
        )
        sym_mode = os.environ.get(
            "CUBEFIT_SYMBREAK_MODE", "qr"
        ).lower()
        if (
            sym_eps > 0.0
            and sym_mode != "off"
            and (x0 is None or np.count_nonzero(x_CP) == 0)
        ):
            print("[Kaczmarz-GLOBAL] Applying symmetry breaking...",
                  flush=True)
            rng_sb = np.random.default_rng(
                int(os.environ.get("CUBEFIT_SEED", "12345"))
            )
            if sym_mode == "qr":
                Rmat = rng_sb.standard_normal((C, P))
                Q, _ = np.linalg.qr(Rmat.T, mode="reduced")
                Q = Q[:, :C].T  # (C, P)
                x_CP += sym_eps * np.abs(Q)
            else:
                x_CP += sym_eps * rng_sb.random((C, P))
            if cfg.project_nonneg:
                np.maximum(x_CP, 0.0, out=x_CP)
            print("[Kaczmarz-GLOBAL] done.", flush=True)

        # This global-step solver ignores orbit_weights / ratio_cfg for now
        have_ratio = False
        w_prior = None

        # ------------------- multiprocessing setup -------------------
        print(
            f"[Kaczmarz-GLOBAL] Spinning up workers with "
            f"{cfg.processes} processes...",
            flush=True,
        )

        nprocs_req = max(1, int(cfg.processes))
        band_size = int(np.ceil(C / nprocs_req))
        bands: list[tuple[int, int]] = []
        c0 = 0
        for _i in range(nprocs_req):
            c1 = int(np.min((C, c0 + band_size)))
            if c1 > c0:
                bands.append((c0, c1))
            c0 = c1
        nprocs = len(bands)
        print(
            f"[Kaczmarz-GLOBAL] Using {nprocs} processes, "
            f"band_size={band_size}.",
            flush=True,
        )

        use_pool = nprocs > 1
        if not use_pool:
            print(
                "[Kaczmarz-GLOBAL] Single-process mode "
                "(no multiprocessing pool).",
                flush=True,
            )
            pool = None
        else:
            ctx_name = os.environ.get(
                "CUBEFIT_MP_CTX", "forkserver"
            )
            print(
                f"[Kaczmarz-GLOBAL] Using multiprocessing context "
                f"'{ctx_name}'",
                flush=True,
            )
            try:
                ctx = mp.get_context(ctx_name)
            except ValueError:
                print(
                    "[Kaczmarz-GLOBAL] Context '{ctx_name}' unavailable; "
                    "falling back to 'spawn'.",
                    flush=True,
                )
                ctx = mp.get_context("spawn")

            os.environ.setdefault("KMP_INIT_AT_FORK", "FALSE")
            os.environ.setdefault(
                "OMP_NUM_THREADS", str(cfg.blas_threads)
            )
            os.environ.setdefault(
                "OPENBLAS_NUM_THREADS", str(cfg.blas_threads)
            )
            os.environ.setdefault(
                "MKL_NUM_THREADS", str(cfg.blas_threads)
            )

            max_tasks = int(
                os.environ.get("CUBEFIT_WORKER_MAXTASKS", "0")
            )
            ping_timeout = float(
                os.environ.get(
                    "CUBEFIT_POOL_PING_TIMEOUT", "5.0"
                )
            )
            renew_each_epoch = os.environ.get(
                "CUBEFIT_POOL_RENEW_EVERY_EPOCH", "0"
            ).lower() not in ("0", "false", "no", "off")

            def _make_pool():
                print(
                    f"[Kaczmarz-GLOBAL] Creating pool with {nprocs} "
                    f"workers...",
                    flush=True,
                )
                ppool = ctx.Pool(
                    processes=nprocs,
                    initializer=_worker_init,
                    initargs=(int(cfg.blas_threads),),
                    maxtasksperchild=(
                        None if max_tasks <= 0 else max_tasks
                    ),
                )
                print("[Kaczmarz-GLOBAL] Pool created.", flush=True)
                return ppool

            pool = _make_pool()

        want_shift_diag = os.environ.get(
            "CUBEFIT_SHIFT_DIAG", "0"
        ).lower() not in ("0", "false", "no", "off")

        # Kacz-style global step hyperparameters
        eps      = float(os.environ.get("CUBEFIT_EPS", "1e-12"))
        rel_zero = float(os.environ.get("CUBEFIT_ZERO_COL_REL", "1e-12"))
        abs_zero = float(os.environ.get("CUBEFIT_ZERO_COL_ABS", "1e-24"))

        # Allow disabling the tiny-column freeze for diagnostics.
        # If CUBEFIT_ZERO_COL_FREEZE in {0, false, no, off}, no columns are
        # frozen based on (rel_zero, abs_zero).
        zero_col_freeze = os.environ.get(
            "CUBEFIT_ZERO_COL_FREEZE", "1"
        ).lower() not in ("0", "false", "no", "off")

        kacz_l2 = float(os.environ.get("CUBEFIT_KACZ_L2", "0.0"))
        if not np.isfinite(kacz_l2) or kacz_l2 < 0.0:
            kacz_l2 = 0.0

        # Optional orbit-mix projection strength (0 = off)
        orbit_beta = float(
            os.environ.get("CUBEFIT_ORBIT_BETA", "0.0")
        )
        if not np.isfinite(orbit_beta) or orbit_beta < 0.0:
            orbit_beta = 0.0

        rmse_cap = float(
            os.environ.get("CUBEFIT_RMSE_ABORT", "0.0")
        )

        frac_bad_prev = None
        dx_base_norm_prev = None

        try:
            for ep in range(cfg.epochs):
                if use_pool:
                    if (not _pool_ok(pool, timeout=ping_timeout)) or \
                       renew_each_epoch:
                        try:
                            pool.close()
                            pool.join()
                        except Exception:
                            try:
                                pool.terminate()
                            except Exception:
                                pass
                        pool = _make_pool()

                # Global accumulators for this epoch
                g_tot = np.zeros((C, P), dtype=np.float64)
                D_tot = np.zeros((C, P), dtype=np.float64)
                rmse_sum_sq = 0.0
                rmse_count = 0

                pbar = tqdm(
                    total=len(s_ranges),
                    desc=(
                        f"[Kaczmarz-GLOBAL] epoch "
                        f"{ep+1}/{cfg.epochs}"
                    ),
                    mininterval=2.0,
                    dynamic_ncols=True,
                )
                pbar.refresh()

                for tile_idx, (s0, s1) in enumerate(s_ranges):
                    Sblk = s1 - s0

                    # ---------- Build residual R = Y - yhat ----------
                    with open_h5(h5_path, role="reader") as f:
                        DC = f["/DataCube"]
                        M  = f["/HyperCube/models"]
                        try:
                            M.id.set_chunk_cache(
                                cfg.dset_slots,
                                cfg.dset_bytes,
                                cfg.dset_w0,
                            )
                        except Exception:
                            pass

                        Y = np.asarray(
                            DC[s0:s1, :], np.float64, order="C"
                        )
                        if keep_idx is not None:
                            Y = Y[:, keep_idx]

                        # Model prediction with current global x_CP
                        yhat = np.zeros((Sblk, Lk), np.float64)
                        for c in range(C):
                            A = np.asarray(
                                M[s0:s1, c, :, :],
                                np.float32,
                                order="C",
                            )
                            if keep_idx is not None:
                                A = A[:, :, keep_idx]
                            if cp_flux_ref is not None:
                                A = (
                                    A
                                    * inv_cp_flux_ref[c, :][
                                        None, :, None
                                    ]
                                )
                            xc_norm = x_CP[c, :].astype(
                                np.float64, copy=False
                            )
                            # yhat[s, :] += xc_norm @ A[s, :, :]
                            yhat += np.tensordot(
                                xc_norm, A, axes=(0, 1)
                            )

                        R = Y - yhat

                    # Optional per-tile shift diagnostic
                    if want_shift_diag and Sblk > 0:
                        s_pick = s0
                        try:
                            y_obs = np.asarray(
                                DC[s_pick, :], np.float64
                            )
                            if keep_idx is not None:
                                y_obs = y_obs[keep_idx]
                            y_fit = yhat[s_pick - s0, :]
                            sh = _xcorr_int_shift(y_obs, y_fit)
                            if sh != 0:
                                print(
                                    f"[diag] spaxel {s_pick}: "
                                    "data↔model integer shift "
                                    f"= {int(sh)} px",
                                    flush=True,
                                )
                        except Exception:
                            pass

                    # NaN/Inf guard on R
                    if not np.all(np.isfinite(R)):
                        bad = ~np.isfinite(R)
                        n_bad = int(bad.sum())
                        print(
                            "[Kaczmarz-GLOBAL] WARNING: non-finite "
                            f"residuals on tile {tile_idx} "
                            f"(bad={n_bad}); zeroing.",
                            flush=True,
                        )
                        R = np.nan_to_num(
                            R,
                            nan=0.0,
                            posinf=0.0,
                            neginf=0.0,
                            copy=False,
                        )

                    # Per-tile RMSE (unweighted) before this global step
                    rmse_before = float(
                        np.sqrt(np.mean(R * R))
                    )

                    if (rmse_cap > 0.0) and (rmse_before > rmse_cap):
                        if tracker is not None:
                            tracker.on_batch_rmse(rmse_cap)
                        # Skip this tile in gradient accumulation
                        pbar.update(1)
                        pbar.refresh()
                        continue

                    if tracker is not None:
                        tracker.on_batch_rmse(rmse_before)

                    # Accumulate into the epoch-level global RMSE proxy
                    rmse_sum_sq += float(np.sum(R * R))
                    rmse_count += int(R.size)

                    # ---------- Gradient jobs for this tile ----------
                    jobs = []
                    inv_ref_band_full = (
                        inv_cp_flux_ref
                        if cp_flux_ref is not None
                        else None
                    )
                    for (c_start, c_stop) in bands:
                        x_band = x_CP[c_start:c_stop, :].copy(order="C")
                        inv_ref_band = (
                            inv_ref_band_full[c_start:c_stop, :]
                            if inv_ref_band_full is not None
                            else None
                        )
                        jobs.append(
                            (
                                h5_path,
                                int(s0),
                                int(s1),
                                keep_idx,
                                int(c_start),
                                int(c_stop),
                                x_band,
                                R.copy(order="C"),
                                w_lam_sqrt,
                                inv_ref_band,
                                cfg.dset_slots,
                                cfg.dset_bytes,
                                cfg.dset_w0,
                            )
                        )

                    if use_pool:
                        results = pool.map(
                            _worker_tile_global_grad_band, jobs
                        )
                    else:
                        results = [
                            _worker_tile_global_grad_band(job)
                            for job in jobs
                        ]

                    # Aggregate band contributions into global g_tot, D_tot
                    for (c_start, c_stop), (g_band, D_band) in zip(
                        bands, results
                    ):
                        c_start = int(c_start)
                        c_stop = int(c_stop)
                        g_tot[c_start:c_stop, :] += g_band
                        D_tot[c_start:c_stop, :] += D_band

                    if tracker is not None and False:
                        tracker.on_progress(
                            epoch=ep + 1,
                            spax_done=tile_idx + 1,
                            spax_total=len(s_ranges),
                            rmse_ewma=None,
                        )

                    pbar.update(1)
                    # pbar.refresh()

                pbar.close()

                # --------- finalize epoch-level RMSE proxy ----------
                if rmse_count > 0:
                    mean_sq = rmse_sum_sq / float(rmse_count)
                else:
                    mean_sq = 0.0

                if (not np.isfinite(mean_sq)) or (mean_sq < 0.0):
                    print(
                        "[Kaczmarz-GLOBAL] WARNING: epoch "
                        f"{ep+1} RMSE(proxy) mean_sq={mean_sq!r} "
                        "non-finite or negative; setting +inf.",
                        flush=True,
                    )
                    rmse_epoch_proxy = float("inf")
                else:
                    rmse_epoch_proxy = float(np.sqrt(mean_sq))

                print(
                    f"[Kaczmarz-GLOBAL] epoch {ep + 1} RMSE(proxy) = "
                    f"{rmse_epoch_proxy:.6e}",
                    flush=True,
                )

                if tracker is not None:
                    try:
                        tracker.on_epoch_end(
                            ep + 1,
                            {"rmse_epoch_proxy": rmse_epoch_proxy},
                            block=False,
                        )
                    except TypeError:
                        tracker.on_epoch_end(
                            ep + 1,
                            {"rmse_epoch_proxy": rmse_epoch_proxy},
                        )

                # --------- global Kaczmarz-style update in x_CP -----
                # Freeze numerically tiny columns based on global D_tot
                D = np.asarray(D_tot, dtype=np.float64, order="C")
                g = np.asarray(g_tot, dtype=np.float64, order="C")

                # Freeze numerically tiny columns globally (optional)
                if zero_col_freeze and np.any(D > 0):
                    med_energy = float(np.median(D[D > 0]))
                    tiny_col = np.max((abs_zero, rel_zero * med_energy))
                    freeze = D <= tiny_col
                    if freeze.any():
                        g[freeze] = 0.0
                        D = np.where(freeze, np.inf, D)
                else:
                    tiny_col = 0.0  # not used, kept for potential logging

                # Blend with global column energy to stabilize empty tiles
                if E_global is not None:
                    Eg = np.asarray(E_global, np.float64)
                    if Eg.shape == D.shape:
                        D = np.maximum(D, beta_blend * Eg)

                # Optional L2 term in the normalized basis
                if kacz_l2 > 0.0:
                    g -= kacz_l2 * x_CP

                # ---- Robust denom floor to prevent huge invD from tiny-but-nonzero D ----
                D_pos = D[np.isfinite(D) & (D > 0.0)]
                if D_pos.size:
                    D_scale = float(np.percentile(D_pos, 90.0))
                else:
                    D_scale = 0.0

                D_floor_frac = float(os.environ.get("CUBEFIT_DENOM_FLOOR_FRAC", "1e-8"))
                if (not np.isfinite(D_floor_frac)) or (D_floor_frac <= 0.0):
                    D_floor_frac = 1e-8

                D_floor = float(max(abs_zero, D_floor_frac * D_scale))
                n_clamped = 0
                if D_floor > 0.0:
                    n_clamped = int(np.count_nonzero(D < D_floor))
                    np.maximum(D, D_floor, out=D)
                print(
                    f"[Kaczmarz-GLOBAL] epoch {ep+1} denom floor = "
                    f"{D_floor:.3e} (clamped {n_clamped} cols), "
                    f" D_p90={D_scale:.3e}",
                    flush=True,
                )

                invD = 1.0 / np.maximum(D, eps)

                # ---- Adaptive effective learning rate for this epoch ----
                # Base step for lr=1.0
                dx_base = g * invD

                # ------------------------------------------------------------
                # Guard A: gradient geometry sanity check (pre-epoch)
                # ------------------------------------------------------------
                if cfg.project_nonneg:
                    frac_bad = float(np.mean(dx_base < -x_CP))
                else:
                    frac_bad = 0.0

                dx_base_norm = float(np.linalg.norm(dx_base))

                if frac_bad_prev is not None and dx_base_norm_prev is not None:
                    if (frac_bad > frac_bad_prev * 1.2) and (dx_base_norm > dx_base_norm_prev * 10.0):
                        print(
                            f"[Kaczmarz-GLOBAL] epoch {ep+1} ABORTED EARLY: "
                            f"active-set shock "
                            f"(frac_bad {frac_bad:.2f}, "
                            f"dx_norm {dx_base_norm:.2e})",
                            flush=True,
                        )
                        break

                frac_bad_prev = frac_bad
                dx_base_norm_prev = dx_base_norm

                # ------------------------------------------------------------
                # Descent-guaranteed global projected step (probe quadratic)
                # ------------------------------------------------------------
                lr = float(cfg.lr)
                x_before = x_CP.copy(order="C")

                # Start from the raw preconditioned direction.
                dx_dir = lr * dx_base

                # Enforce a *feasible projected-gradient* direction so small steps remain
                # feasible without needing "project full step once".
                if cfg.project_nonneg:
                    # Do not step negative at the boundary x=0.
                    dx_dir = np.where((x_before > 0.0) | (dx_dir > 0.0), dx_dir, 0.0)

                # Apply global trust cap to the direction (BT only scales down).
                step_scale_cap = 1.0
                if (Y_glob_norm > 0.0) and (E_global is not None):
                    Eg = np.asarray(E_global, np.float64)
                    step_energy = float(np.sum((dx_dir * dx_dir) * Eg))
                    cap = float(tau_global * Y_glob_norm)
                    if (step_energy > 0.0) and (cap > 0.0):
                        step_norm = float(np.sqrt(step_energy))
                        if step_norm > cap:
                            step_scale_cap = cap / max(step_norm, 1.0e-12)
                            dx_dir *= step_scale_cap


                def _probe_quadratic_terms(dx: np.ndarray) -> tuple[float, float, float, int]:
                    rr = 0.0
                    cr = 0.0
                    rd2 = 0.0
                    den = 0

                    with open_h5(h5_path, role="reader") as f:
                        DC = f["/DataCube"]
                        M = f["/HyperCube/models"]

                        for (ps0, ps1) in probe_tiles:
                            Y = np.asarray(DC[ps0:ps1, :], np.float64, order="C")
                            if keep_idx is not None:
                                Y = Y[:, keep_idx]

                            Sblk = int(ps1 - ps0)
                            yhat0 = np.zeros((Sblk, Lk), dtype=np.float64)
                            dyhat = np.zeros((Sblk, Lk), dtype=np.float64)

                            for c in range(C):
                                A = np.asarray(M[ps0:ps1, c, :, :], np.float32, order="C")
                                if keep_idx is not None:
                                    A = A[:, :, keep_idx]
                                if cp_flux_ref is not None:
                                    A = A * inv_cp_flux_ref[c, :][None, :, None]

                                A = A.astype(np.float64, copy=False)

                                yhat0 += np.tensordot(x_before[c, :], A, axes=(0, 1))
                                dyhat += np.tensordot(dx[c, :], A, axes=(0, 1))

                            if w_lam_sqrt is not None:
                                wv = w_lam_sqrt[None, :]
                                Rw = (Y - yhat0) * wv
                                dRw = (-dyhat) * wv
                            else:
                                Rw = (Y - yhat0)
                                dRw = (-dyhat)

                            r = Rw.ravel(order="C")
                            d = dRw.ravel(order="C")

                            rr += float(np.dot(r, r))
                            cr += float(np.dot(r, d))
                            rd2 += float(np.dot(d, d))
                            den += int(r.size)

                    return rr, cr, rd2, den


                def _rmse_from_quad(rr: float, cr: float, rd2: float, den: int,
                                    a: float) -> float:
                    val = rr + 2.0 * a * cr + (a * a) * rd2
                    if val < 0.0:
                        val = 0.0
                    return float(np.sqrt(val / max(den, 1)))


                # Compute probe quadratic once.
                rr, cr, rd2, den = _probe_quadratic_terms(dx_dir)
                rmse_probe_before = float(np.sqrt(rr / max(den, 1)))

                # Directional derivative sanity: for a descent direction we need cr < 0.
                # If it is not, flip the direction once and re-evaluate.
                step_sign = 1.0
                if allow_sign_flip and np.isfinite(cr) and (cr >= 0.0):
                    step_sign = -1.0
                    dx_dir = -dx_dir
                    if cfg.project_nonneg:
                        dx_dir = np.where((x_before > 0.0) | (dx_dir > 0.0), dx_dir, 0.0)
                    rr, cr, rd2, den = _probe_quadratic_terms(dx_dir)
                    rmse_probe_before = float(np.sqrt(rr / max(den, 1)))

                # Closed-form best step along this direction (quadratic is exact here).
                a_star = 0.0
                if np.isfinite(rd2) and (rd2 > 0.0) and np.isfinite(cr):
                    a_star = float(np.clip(-cr / rd2, 0.0, 1.0))

                # Backtrack only to enforce improvement.
                accepted = False
                bt_used = 0
                step_scale = 0.0
                rmse_probe_after = rmse_probe_before

                improve_tol = float(os.environ.get("CUBEFIT_PROBE_IMPROVE_TOL", "1e-12"))
                target = rmse_probe_before * (1.0 - improve_tol)

                for bt_used in range(bt_max):
                    a = a_star * float(bt_shrink ** bt_used)
                    if a <= 0.0:
                        break
                    rmse_a = _rmse_from_quad(rr, cr, rd2, den, a)
                    if np.isfinite(rmse_a) and (rmse_a < target):
                        accepted = True
                        step_scale = a
                        rmse_probe_after = rmse_a
                        break

                if accepted:
                    dx = step_scale * dx_dir
                    x_trial = x_before + dx
                    if cfg.project_nonneg:
                        np.maximum(x_trial, 0.0, out=x_trial)
                else:
                    dx = np.zeros_like(dx_dir)
                    x_trial = x_before

                x_CP[:, :] = x_trial
                dx_applied = dx

                # Diagnostics: fraction of entries that would violate NNLS without projection
                if cfg.project_nonneg:
                    frac_clipped = float(np.mean(dx_dir < -x_before))
                else:
                    frac_clipped = 0.0

                dx_norm = float(np.linalg.norm(dx))
                dx_app_norm = float(np.linalg.norm(dx_applied))
                x_norm = float(np.linalg.norm(x_CP))
                x_min = float(np.min(x_CP))
                x_max = float(np.max(x_CP))

                if E_global is not None:
                    Eg = np.asarray(E_global, np.float64)
                    dx_E = float(np.sqrt(np.sum((dx.astype(np.float64) ** 2) * Eg)))
                    dx_app_E = float(
                        np.sqrt(np.sum((dx_applied.astype(np.float64) ** 2) * Eg))
                    )
                else:
                    dx_E = float("nan")
                    dx_app_E = float("nan")

                print(
                    f"[Kaczmarz-GLOBAL] epoch {ep + 1} "
                    f"rmse_proxy={rmse_epoch_proxy:.3e} "
                    f"probe_before={rmse_probe_before:.3e} "
                    f"probe_after={rmse_probe_after:.3e} "
                    f"accepted={int(accepted)} bt={bt_used} sign={step_sign:+.0f} "
                    f"frac_clipped={frac_clipped:.3f} "
                    f"| dx_norm={dx_norm:.3e} dx_app_norm={dx_app_norm:.3e} "
                    f"step_scale={step_scale:.3e} "
                    f"| dx_E_norm={dx_E:.3e} dx_app_E_norm={dx_app_E:.3e} "
                    f"| x_norm={x_norm:.3e} x_min={x_min:.3e} x_max={x_max:.3e}",
                    flush=True,
                )

                if not np.all(np.isfinite(x_CP)):
                    bad = ~np.isfinite(x_CP)
                    print(
                        "[Kaczmarz-GLOBAL] WARNING: non-finite entries "
                        f"in x_CP after update (bad={int(bad.sum())}); "
                        "zeroing.",
                        flush=True,
                    )
                    x_CP[bad] = 0.0

                # Optional global projection toward orbit_weights
                if (
                    orbit_beta > 0.0
                    and orbit_weights_arr is not None
                    and E_global is not None
                ):
                    try:
                        cu.project_to_component_weights(
                            x_CP,
                            orbit_weights_arr,
                            E_cp=E_global,
                            minw=1.0e-10,
                            beta=float(orbit_beta),
                        )
                    except Exception as e:
                        print(
                            "[Kaczmarz-GLOBAL] WARNING: "
                            "orbit_weights projection failed: "
                            f"{e!r}",
                            flush=True,
                        )

                # Snapshot after the global step
                if tracker is not None:
                    try:
                        tracker.maybe_snapshot_x(
                            x_CP,
                            epoch=ep + 1,
                            rmse=rmse_epoch_proxy,
                            force=True,
                        )
                    except Exception:
                        pass

                print(
                    f"[Kaczmarz-GLOBAL] epoch {ep + 1}/{cfg.epochs} done.",
                    flush=True,
                )

            elapsed = time.perf_counter() - t0

            # Convert back to physical basis
            np.nan_to_num(
                x_CP,
                nan=0.0,
                posinf=0.0,
                neginf=0.0,
                copy=False,
            )

            if cp_flux_ref is not None:
                X_norm = x_CP
                X_phys = X_norm * inv_cp_flux_ref
            else:
                X_phys = x_CP

            x_out = np.asarray(
                X_phys, np.float64, order="C"
            ).ravel(order="C")

            return x_out, dict(
                epochs=cfg.epochs,
                elapsed_sec=elapsed,
            )
        finally:
            if use_pool and (pool is not None):
                try:
                    pool.close()
                    pool.join()
                except Exception:
                    try:
                        pool.terminate()
                    except Exception:
                        pass
    except Exception:
        print(
            "[Kaczmarz-GLOBAL] FATAL exception in "
            "solve_global_kaczmarz_global_step_mp:",
            flush=True,
        )
        print(traceback.format_exc(), flush=True)
        raise

# ------------------------------------------------------------------------------

def probe_kaczmarz_tile(
    h5_path: str,
    s0: int | None = None,
    s1: int | None = None,
    c: int | None = None,
    lr: float = 0.25,
    x_source: str = "auto",   # "auto" | "zeros"
    project_nonneg: bool = True,
):
    """
    Single-band probe that mirrors the worker math on one component.
    Uses the same λ-weighting and global energy blend, so scale matches.
    """

    bt_steps   = int(np.max((0, int(os.environ.get("CUBEFIT_BT_STEPS", "3")))))
    bt_factor  = float(os.environ.get("CUBEFIT_BT_FACTOR", "0.5"))
    tau_trust  = float(os.environ.get("CUBEFIT_TRUST_TAU", "0.7"))
    eps        = float(os.environ.get("CUBEFIT_EPS", "1e-12"))
    rel_zero   = float(os.environ.get("CUBEFIT_ZERO_COL_REL", "1e-12"))
    abs_zero   = float(os.environ.get("CUBEFIT_ZERO_COL_ABS", "1e-24"))
    tau_global = float(os.environ.get("CUBEFIT_GLOBAL_TAU", "0.5"))
    beta_blend = float(os.environ.get("CUBEFIT_GLOBAL_ENERGY_BLEND", "1e-2"))

    with h5py.File(h5_path, "r") as f:
        M  = f["/HyperCube/models"]  # (S,C,P,L)
        DC = f["/DataCube"]          # (S,L)
        S, Ctot, P, L = map(int, M.shape)
        chunks = M.chunks or (S, 1, P, L)
        S_chunk = int(chunks[0])

        if s0 is None or s1 is None:
            s0 = 0
            s1 = int(np.min((S, S_chunk)))
        if c is None:
            c = int(Ctot // 2)

        keep_idx = None
        if "/Mask" in f:
            m = np.asarray(f["/Mask"][...], bool).ravel()
            keep_idx = np.flatnonzero(m)
        Lk = int(L if keep_idx is None else keep_idx.size)

        # x source
        if x_source == "auto" and "/X_global" in f:
            x1d = np.asarray(f["/X_global"][...], np.float64, order="C")
            x_CP = x1d.reshape(Ctot, P)
        else:
            x_CP = np.zeros((Ctot, P), np.float64)

        # Y (tile), global ||Y||
        Y = np.asarray(DC[s0:s1, :], np.float64, order="C")
        if keep_idx is not None:
            Y = Y[:, keep_idx]  # (Sblk, Lk)
        Sblk = int(s1 - s0)

        Yglob2 = 0.0
        for t0 in range(0, S, S_chunk):
            t1 = int(np.min((S, t0 + S_chunk)))
            Yt = np.asarray(DC[t0:t1, :], np.float64, order="C")
            if keep_idx is not None:
                Yt = Yt[:, keep_idx]
            Yglob2 += float(np.sum(Yt * Yt))
        Y_glob_norm = float(np.sqrt(Yglob2))

        # yhat (tile) exactly like the solver
        yhat = np.zeros((Sblk, Lk), np.float64)
        for cc in range(Ctot):
            A_cc = np.asarray(M[s0:s1, cc, :, :], np.float32, order="C")
            if keep_idx is not None:
                A_cc = A_cc[:, :, keep_idx]
            xc = x_CP[cc, :].astype(np.float64, copy=False)
            for s in range(Sblk):
                yhat[s, :] += xc @ A_cc[s, :, :]

        R = Y - yhat

        # ---- worker-like band update on component c ----
        A = np.asarray(M[s0:s1, c, :, :], np.float32, order="C")
        if keep_idx is not None:
            A = A[:, :, keep_idx]  # (Sblk, P, Lk)
        cp_flux_ref = cu._ensure_cp_flux_ref(h5_path, keep_idx=None if keep_idx is None else np.arange(L)[keep_idx])
        A = A * (1.0 / cp_flux_ref[int(c), :])[None, :, None]

        # sanitize
        badR = ~np.isfinite(R); R[badR] = 0.0
        badA = ~np.isfinite(A); A[badA] = 0.0

        # λ-weights (mirror main solver)
        lamw_enable = os.environ.get(
            "CUBEFIT_LAMBDA_WEIGHTS_ENABLE", "1"
        ).lower() not in ("0", "false", "no", "off")
        if lamw_enable and "/HyperCube/lambda_weights" in f:
            w_full = np.asarray(f["/HyperCube/lambda_weights"][...],
                                np.float64)
            if keep_idx is not None:
                w_lam_sqrt = np.sqrt(np.maximum(w_full[keep_idx], 1e-6))
            else:
                w_lam_sqrt = np.sqrt(np.maximum(w_full, 1e-6))
        else:
            w_lam_sqrt = None

        # gradient (weighted)
        if w_lam_sqrt is not None:
            A_w = A * w_lam_sqrt[None, None, :]
            Rw  = R * w_lam_sqrt[None, :]
        else:
            A_w = A; Rw = R

        g = np.zeros((P,), np.float64)
        for s in range(Sblk):
            g += A_w[s, :, :].astype(np.float64, copy=False) @ Rw[s, :]

        # local per-column denom (weighted)
        col_denom = np.sum(np.square(A_w, dtype=np.float64), axis=(0, 2))

        # freeze near-zero columns (tile-local)
        med_energy = float(np.median(col_denom[col_denom > 0])) if np.any(col_denom > 0) else 0.0
        tiny_col = np.max((abs_zero, rel_zero * med_energy))
        freeze = col_denom <= tiny_col
        if np.any(freeze):
            g[freeze] = 0.0
            col_denom = np.where(freeze, np.inf, col_denom)

        # --- global energy blend
        E_global = read_global_column_energy(h5_path)  # (C,P)
        Eg_row = np.asarray(E_global[int(c), :], np.float64)  # (P,)
        col_denom = np.maximum(col_denom, float(beta_blend) * Eg_row)

        invD = 1.0 / np.maximum(col_denom, eps)
        dx_c = float(lr) * (g * invD)  # (P,)

        # ΔR for alpha=1 (unweighted)
        R_delta = np.zeros((Sblk, Lk), np.float64)
        for s in range(Sblk):
            R_delta[s, :] -= (
                A[s, :, :].astype(np.float64, copy=False).T @ dx_c
            )

        # trust region (tile, weighted)
        if w_lam_sqrt is not None:
            Rw_delta = R_delta * w_lam_sqrt[None, :]
            rn = float(np.linalg.norm(R * w_lam_sqrt[None, :]))
        else:
            Rw_delta = R_delta
            rn = float(np.linalg.norm(R))
        rd = float(np.linalg.norm(Rw_delta))
        alpha_max = 1.0 if rd == 0.0 else min(1.0, (tau_trust * rn) / rd)

        # backtracking
        alpha = alpha_max
        def _rmse_w(MAT):  # weighted RMSE helper
            if w_lam_sqrt is None:
                return float(np.sqrt(np.mean(MAT * MAT)))
            Z = MAT * w_lam_sqrt[None, :]
            return float(np.sqrt(np.mean(Z * Z)))

        rmse_before = _rmse_w(R)
        rmse_after  = _rmse_w(R + alpha * R_delta)
        if not (rmse_after < rmse_before):
            a = alpha
            for _ in range(bt_steps):
                a *= bt_factor
                if a <= 0.0:
                    break
                rmse_after = _rmse_w(R + a * R_delta)
                if rmse_after < rmse_before:
                    alpha = a
                    break
            else:
                alpha = a

        # global cap
        upd_energy_sq = float(np.sum((dx_c.astype(np.float64) ** 2) * Eg_row))
        if (upd_energy_sq > 0.0) and (Y_glob_norm > 0.0):
            step_norm_global = float(np.sqrt(upd_energy_sq)) * alpha
            cap = float(tau_global * Y_glob_norm)
            if step_norm_global > cap:
                alpha *= float(np.minimum(1.0, cap / np.maximum(1e-12, step_norm_global)))

        dx_c *= alpha
        if project_nonneg:
            over_neg = dx_c < -x_CP[c, :]
            if np.any(over_neg):
                dx_c[over_neg] = -x_CP[c, :][over_neg]
                R_delta.fill(0.0)
                for s in range(Sblk):
                    R_delta[s, :] -= (
                        A[s, :, :].astype(np.float64, copy=False).T @ dx_c
                    )
        else:
            if alpha != 1.0:
                R_delta *= alpha

        R_after = R + R_delta
        yhat_norm = float(np.linalg.norm(yhat))
        yhat_next_norm = float(np.linalg.norm(yhat - R_delta))

        out = {
            "rmse_before": float(np.sqrt(np.mean(R * R))),
            "rmse_after":  float(np.sqrt(np.mean(R_after * R_after))),
            "y_norm":      float(np.linalg.norm(Y)),
            "yhat_norm":   yhat_norm,
            "yhat_next_norm": yhat_next_norm,
            "g_norm":      float(np.linalg.norm(g)),
            "dx_norm":     float(np.linalg.norm(dx_c)),
            "global_upd_norm": float(np.sqrt(np.maximum(0.0, upd_energy_sq)) * alpha),
            "Y_glob_norm": Y_glob_norm,
            "Sblk":        Sblk,
            "Lk":          Lk,
            "c":           int(c),
            "alpha":       float(alpha),
            "frozen_cols": int(np.count_nonzero(freeze)),
        }
        print("[Probe]", out)
        return out
</file>

<file path="__init__.py">
# -*- coding: utf-8 -*-
r"""
    CubeFit: Spectral Cube Fitting Toolkit
    --------------------------------------

    Platforms
    ---------
    Unix, Windows

    Synopsis
    --------
    High-level CubeFit pipeline and utilities:
    - ModelCube: Fast generation of rebinned synthetic spectra and convolutions
    - ZarrManager: Zarr-based I/O for large, chunked data cubes and HyperCube storage
    - PipelineRunner: Orchestrates full fitting pipeline (Kaczmarz, BCD, batch NNLS)
    - KaczmarzSolver: Parallel, batched, blockwise Kaczmarz NNLS solvers
    - cube_bcd: Parallel block coordinate descent solvers and cluster/mean NNLS init
    - cube_utils: Generic scientific helpers (mean/cluster NNLS init, loss, batching)
    - plotting: Flexible plotting for spectra, fits, and diagnostics
    - logger: Robust CubeFit logging and formatted progress messages
    - parallel: Batch and multiprocessing helpers
    - reference: Reference fit, diagnostics, and comparison routines

    Authors
    -------
    Adriano Poci <adriano.poci@physics.ox.ac.uk>
"""

# Define __all__ for explicit package API
__all__ = [
    "ModelCube",
    "ZarrManager",
    "PipelineRunner",
    "KaczmarzSolver",
    "comp_loss_for_total",
    "flatten_x",
    "unflatten_x",
    "random_subset_indices",
    "plot_aperture_fit",
    "get_logger",
    "CubeFitLogger",
]
</file>

<file path="diag_losvd_models.py">
#!/usr/bin/env python3
# diag_losvd_models.py
from __future__ import annotations
import argparse, os, pathlib as plp
import numpy as np
import matplotlib.pyplot as plt

from CubeFit.hdf5_manager import open_h5
from CubeFit.hypercube_builder import read_global_column_energy

def _pick_spaxels(S: int, s_sel: str | None) -> np.ndarray:
    if s_sel is None:
        return np.arange(min(1, S), dtype=np.int64)
    if ":" in s_sel:
        a, b = s_sel.split(":")
        start = max(0, min(S - 1, int(a)))
        count = max(1, int(b))
        end = max(start, min(S, start + count))
        return np.arange(start, end, dtype=np.int64)
    idx = np.array([int(x) for x in s_sel.split(",") if x.strip() != ""],
                   dtype=np.int64)
    idx = np.unique(idx)
    return idx[(idx >= 0) & (idx < S)]

def _choose_pops(E: np.ndarray, K: int, comps: np.ndarray | None) -> list[np.ndarray]:
    C, P = map(int, E.shape)
    if comps is None:
        comps = np.arange(C, dtype=np.int64)
    picks: list[np.ndarray] = []
    for c in range(C):
        if c in set(comps):
            order = np.argsort(E[c, :])[::-1]
            picks.append(order[:min(K, P)].astype(np.int64))
        else:
            picks.append(np.array([], dtype=np.int64))
    return picks

def main():
    ap = argparse.ArgumentParser(
        description="Inspect LOSVD, templates, and stored model columns."
    )
    ap.add_argument("h5", help="Path to CubeFit HDF5")
    ap.add_argument("--spax", default="0", help="'i' or 'start:count'")
    ap.add_argument("--comps", default=None,
                    help="components like '0,1' (default: all)")
    ap.add_argument("--k-per-comp", type=int, default=3,
                    help="top-K populations per component by global energy")
    ap.add_argument("--pops", default=None,
                    help="explicit pops e.g. '5,12,42' (overrides K)")
    ap.add_argument("--use-mask", action="store_true",
                    help="plot only masked λ")
    ap.add_argument("--out", default="diag_out", help="output directory")
    args = ap.parse_args()

    plp.Path(args.out).mkdir(parents=True, exist_ok=True)

    with open_h5(args.h5, role="reader") as f:
        S, L = map(int, f["/DataCube"].shape)
        T = int(f["/Templates"].shape[1])
        C = int(f["/LOSVD"].shape[2])
        P = int(f["/Templates"].shape[0])

        s_idx = _pick_spaxels(S, args.spax)
        if s_idx.size != 1:
            raise SystemExit("Please select exactly one spaxel (e.g., --spax 24).")
        s = int(s_idx[0])

        comps = None
        if args.comps:
            comps = np.array([int(x) for x in args.comps.split(",")], np.int64)

        # grids
        vgrid = np.asarray(f["/VelPix"][...], dtype=np.float64)           # (V,)
        tempix = np.asarray(f["/TemPix"][...], dtype=np.float64)          # (T,)
        obspix = np.asarray(f["/ObsPix"][...], dtype=np.float64)          # (L,)

        # mask (optional)
        if args.use_mask and "/Mask" in f:
            mask = np.asarray(f["/Mask"][...], dtype=bool).ravel()
            if mask.size != L:
                mask = np.ones(L, bool)
        else:
            mask = np.ones(L, bool)

        # picks
        if args.pops:
            pops = np.array([int(x) for x in args.pops.split(",")], np.int64)
            picks = [pops if (comps is None or c in set(comps)) else
                     np.array([], np.int64) for c in range(C)]
        else:
            E = read_global_column_energy(args.h5)                        # (C,P)
            picks = _choose_pops(E, int(args.k_per_comp), comps)

        # pull arrays once
        LOS = np.asarray(f["/LOSVD"][s, :, :], dtype=np.float64)          # (V,C)
        TPL = np.asarray(f["/Templates"][...], dtype=np.float64)          # (P,T)
        MOD = np.asarray(f["/HyperCube/models"][s, :, :, :], dtype=np.float32) # (C,P,L)

        # quick stats
        los_sums = LOS.sum(axis=0)
        los_trapz = np.trapz(LOS, vgrid, axis=0)
        print(f"[diag] spaxel {s}")
        print(f"[diag] LOSVD sum per comp:   min/med/max "
              f"{los_sums.min():.3g}/{np.median(los_sums):.3g}/"
              f"{los_sums.max():.3g}")
        print(f"[diag] LOSVD trapz per comp: min/med/max "
              f"{los_trapz.min():.3g}/{np.median(los_trapz):.3g}/"
              f"{los_trapz.max():.3g}")

        # per-(c,p) column norms on stored models (masked and unmasked)
        colnorm = {}
        for c in range(C):
            if picks[c].size == 0:
                continue
            for p in picks[c]:
                A = MOD[c, p, :]
                colnorm[(c, int(p))] = (
                    float(np.linalg.norm(A)),
                    float(np.linalg.norm(A[mask]))
                )

        if len(colnorm):
            u = np.array([v[0] for v in colnorm.values()])
            m = np.array([v[1] for v in colnorm.values()])
            print(f"[diag] ||model||2 (unmasked)  min/med/max "
                  f"{u.min():.3g}/{np.median(u):.3g}/{u.max():.3g}")
            print(f"[diag] ||model||2 (masked)    min/med/max "
                  f"{m.min():.3g}/{np.median(m):.3g}/{m.max():.3g}")

        # ----- plots -----
        # 1) LOSVD per component
        fig = plt.figure(figsize=(9, 3.2))
        ax = fig.add_subplot(111)
        for c in range(C):
            if comps is not None and c not in set(comps):
                continue
            ax.plot(vgrid, LOS[:, c], lw=1.0, label=f"c={c}")
        ax.set_title(f"LOSVD histograms @ spaxel {s}")
        ax.set_xlabel("velocity (km/s)")
        ax.set_ylabel("count / weight")
        ax.legend(fontsize=8, ncol=min(C, 4))
        fig.tight_layout()
        fig.savefig(os.path.join(args.out, f"losvd_spax{s:05d}.png"), dpi=130)
        plt.close(fig)

        # 2) Raw templates (selected pops)
        for c in range(C):
            if picks[c].size == 0:
                continue
            plist = picks[c]
            fig = plt.figure(figsize=(9.5, 3.2))
            ax = fig.add_subplot(111)
            for p in plist:
                ax.plot(tempix, TPL[p, :], lw=0.9, label=f"p={int(p)}")
            ax.set_title(f"Raw templates (P)  —  comps used: c={c}")
            ax.set_xlabel("log λ (TemPix)")
            ax.set_ylabel("template flux")
            ax.legend(fontsize=7, ncol=5)
            fig.tight_layout()
            fig.savefig(os.path.join(args.out, f"templates_c{c}_spax{s:05d}.png"),
                        dpi=130)
            plt.close(fig)

        # 3) Stored model columns (convolved+rebinned)
        for c in range(C):
            if picks[c].size == 0:
                continue
            plist = picks[c]
            fig = plt.figure(figsize=(9.5, 3.2))
            ax = fig.add_subplot(111)
            for p in plist:
                y = MOD[c, p, :]
                y_plot = y[mask] if mask is not None else y
                x_plot = obspix[mask] if mask is not None else obspix
                ax.plot(x_plot, y_plot, lw=0.9,
                        label=f"c={c}, p={int(p)}, "
                              f"||·||2={np.linalg.norm(y_plot):.2g}")
            ax.set_title(f"Stored models @ spaxel {s}  (convolved + rebinned)")
            ax.set_xlabel("log λ (ObsPix)")
            ax.set_ylabel("model flux")
            ax.legend(fontsize=7, ncol=3)
            fig.tight_layout()
            fig.savefig(os.path.join(args.out, f"models_c{c}_spax{s:05d}.png"),
                        dpi=130)
            plt.close(fig)

        print(f"[diag] wrote figures to: {plp.Path(args.out).resolve()}")

if __name__ == "__main__":
    main()
</file>

<file path="environment.yml">
name: cubefit-env
channels:
  - conda-forge
dependencies:
  - python>=3.8
  - numpy
  - matplotlib
  - zarr
  - numcodecs
  - tqdm
</file>

<file path="kz_addqueue.sh">
#!/bin/bash

while getopts ":g:" arg; do
  case $arg in
    g) galax=$OPTARG;;
  esac
done

curdir="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
QUEUE="cmb"
NodeMem=273
nCPU=32

memPerCPU=$(echo "scale=2; $NodeMem*0.99 / $nCPU" | bc)
#memPerCPU=$(echo "scale=2; 110.0 / $nCPU" | bc)
# memPerCPU=10
printf "Running ${nCPU} cores on ${QUEUE},\n using ${memPerCPU} GB per core.\n\n"

addqueue --sbatch --requeue --serial -n "${nCPU}" -q "${QUEUE}" \
    -m "${memPerCPU}" -c "genAper_${galax}" -g "CubeFit${galax}" \
    -o "${curdir}/log.log" "${curdir}/kz_rio.sh"
    # -o "${curdir}/log.log" "${curdir}/kz_run.sh"
</file>

<file path="mkdocs.yml">
site_name: CubeFit
theme:
  name: material
nav:
  - Home: index.md
  - CubeFit: CubeFit.md
docs_dir: docs
</file>

<file path="patch.patch">
diff --git a/CubeFit/kaczmarz_solver.py b/CubeFit/kaczmarz_solver.py
--- a/CubeFit/kaczmarz_solver.py
+++ b/CubeFit/kaczmarz_solver.py
@@ -18,6 +18,7 @@
 from typing import Optional, Tuple
 import numpy as np
 import time
+from tqdm import tqdm
 
 try:
     from threadpoolctl import threadpool_limits
@@ def solve_global_kaczmarz(reader, cfg: SolverCfg, orbit_weights: Optional[np.ndarray] = None,
-    with _blas_ctx(cfg.blas_threads):
-        for ep in range(cfg.epochs):
-            if cfg.verbose:
-                print(f"[Kaczmarz] epoch {ep+1}/{cfg.epochs}")
+    with _blas_ctx(cfg.blas_threads):
+        for ep in range(cfg.epochs):
+            # Epoch header (kept, matches your logs)
+            if cfg.verbose:
+                print(f"[Kaczmarz] epoch {ep+1}/{cfg.epochs}")
+            # Per‑epoch progress bar over spaxels
+            pbar = tqdm(
+                total=reader.nSpat,
+                desc=f"[Kaczmarz] epoch {ep+1}/{cfg.epochs}",
+                unit="spax",
+                dynamic_ncols=True,
+                leave=(ep == cfg.epochs - 1),
+                disable=not cfg.verbose,
+            )
             # iterate spaxels in small tiles to preserve locality if s_tile>1
             for s0, s1 in reader.spaxel_tiles():
                 for s in range(s0, s1):
                     A_f32, y = reader.read_spaxel_plane(s)    # (N,L_eff) float32, (L_eff,) float64
                     L_eff = A_f32.shape[1]
                     K = min(cfg.pixels_per_aperture, L_eff)
@@
                     for l in idx:
                         # Promote only this row to float64 for math
                         a = A_f32[:, l].astype(np.float64, copy=False)  # promote *only* this row
                         if w is not None:
                             a = a * w
                         r = y[l] - np.dot(a, x)          # residual
                         denom = np.dot(a, a) + 1e-18     # safe
                         x += cfg.lr * (r / denom) * a
+                    # update bar once per spaxel
+                    pbar.update(1)
 
-            if cfg.project_nonneg:
+            if cfg.project_nonneg:
                 np.maximum(x, 0, out=x)
+            # close bar at end of epoch
+            pbar.close()
 
     elapsed = time.perf_counter() - t0
     stats = {
</file>

<file path="plotting.py">
# -*- coding: utf-8 -*-
r"""
    plotting.py
    Adriano Poci
    University of Oxford
    2025

    Platforms
    ---------
    Unix, Windows

    Synopsis
    --------
    Diagnostic and summary plotting for CubeFit results: spectra, white-light
    images, residual maps, and convergence history.

    Authors
    -------
    Adriano Poci <adriano.poci@physics.ox.ac.uk>

History
-------
v1.0:   Spectrum/white-light/residual plotting. 2025
v1.1:   Integrated convergence and comparison plots. 2025
"""

from __future__ import annotations

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection

from CubeFit.logger import get_logger
from dynamics.IFU.Constants import UnitStr

UTS = UnitStr()
logger = get_logger()

# ------------------------------------------------------------------------------

def plot_aperture_fit(
    y_obs: np.ndarray,
    y_model: np.ndarray,
    obs_pix: np.ndarray,
    aperture_index: int | str = 0,
    mask: np.ndarray | None = None,
    show_residual: bool = True,
    wavelength_str: str | None = None,
) -> None:
    """
    Plot observed and model spectra (with optional mask), handling both single
    and stacked (multiple) spectra.

    Notes
    -----
    - If inputs are stacked (len(y_obs) is a multiple of len(obs_pix)),
      spectra are drawn as separate curves with NaN breaks (no cross-aperture
      line joins).
    - `mask` should match the shape of the stacked data (i.e., tiled).
    - Residuals are shown on a separate panel if requested.

    Parameters
    ----------
    y_obs : ndarray
        Observed flux, shape (nLSpec,) or (N_stack * nLSpec,).
    y_model : ndarray
        Model flux, same shape as y_obs.
    obs_pix : ndarray
        Wavelength array, shape (nLSpec,).
    aperture_index : int | str
        Title label (e.g., aperture index or "stacked").
    mask : ndarray or None
        Boolean mask for good pixels. If stacking, tile to (N_stack * nLSpec,).
    show_residual : bool
        Whether to include residuals panel.
    wavelength_str : str or None
        X-axis label. Defaults to "Wavelength [$\\AA$]".
    """
    if wavelength_str is None:
        wavelength_str = r"Wavelength [$\AA$]"

    n_spec = obs_pix.size
    n_total = y_obs.size
    is_stacked = (n_total % n_spec == 0) and (n_total > n_spec)

    print(is_stacked, n_total, n_spec)

    if show_residual:
        fig, (ax0, ax1) = plt.subplots(
            2, 1, figsize=(10, 6), sharex=True,
            gridspec_kw={"height_ratios": [2, 1]},
        )
    else:
        fig, ax0 = plt.subplots(figsize=(10, 4))

    if is_stacked:
        N_stack = n_total // n_spec
        y_obs_2d   = y_obs.reshape(N_stack, n_spec)
        y_model_2d = y_model.reshape(N_stack, n_spec)

        if mask is not None and mask.size == n_total:
            mask_2d = mask.reshape(N_stack, n_spec)
        else:
            mask_2d = None

        # Build segments for LineCollection: one segment per spectrum
        # Observed
        obs_segs = []
        mod_segs = []
        for i in range(N_stack):
            if mask_2d is not None:
                mg = mask_2d[i]
                x_i = obs_pix[mg]
                yo  = y_obs_2d[i, mg]
                ym  = y_model_2d[i, mg]
            else:
                x_i = obs_pix
                yo  = y_obs_2d[i]
                ym  = y_model_2d[i]
            if x_i.size >= 2:  # need at least two points to draw a line
                obs_segs.append(np.column_stack([x_i, yo]))
                mod_segs.append(np.column_stack([x_i, ym]))

        # Draw without cross-spectrum joins
        if obs_segs:
            lc_obs = LineCollection(obs_segs, linewidths=1.2, alpha=0.7, label="Observed")
            ax0.add_collection(lc_obs)
        if mod_segs:
            lc_mod = LineCollection(mod_segs, linewidths=1.2, alpha=0.7, label="Model")
            ax0.add_collection(lc_mod)

        # Axis limits
        x_min, x_max = obs_pix[0], obs_pix[-1]
        x_pad = 0.01 * (x_max - x_min)
        ax0.set_xlim(x_min - x_pad, x_max + x_pad)

        # y-limits from the data we plotted
        def _stack_minmax(segs):
            if not segs: return (0.0, 1.0)
            y_all = np.concatenate([s[:, 1] for s in segs])
            return float(np.nanmin(y_all)), float(np.nanmax(y_all))

        y_min_obs, y_max_obs = _stack_minmax(obs_segs)
        y_min_mod, y_max_mod = _stack_minmax(mod_segs)
        y_min = min(y_min_obs, y_min_mod)
        y_max = max(y_max_obs, y_max_mod)
        yr = y_max - y_min
        y_pad = 0.05 * yr if yr > 0 else 0.05 * abs(y_min if y_min else 1.0)
        ax0.set_ylim(y_min - y_pad, y_max + y_pad)

        # Residual panel: plot each stacked residual as gray line segments too
        if show_residual:
            resid_segs = []
            for i in range(N_stack):
                if mask_2d is not None:
                    mg = mask_2d[i]
                    x_i = obs_pix[mg]
                    r_i = (y_obs_2d[i, mg] - y_model_2d[i, mg])
                else:
                    x_i = obs_pix
                    r_i = (y_obs_2d[i] - y_model_2d[i])
                if x_i.size >= 2:
                    resid_segs.append(np.column_stack([x_i, r_i]))
            if resid_segs:
                lc_res = LineCollection(resid_segs, linewidths=1.0, alpha=0.6, color="gray")
                ax1.add_collection(lc_res)
                ax1.set_xlim(x_min - x_pad, x_max + x_pad)
                r_all = np.concatenate([s[:, 1] for s in resid_segs]) if resid_segs else np.array([0.0])
                rmin, rmax = float(np.nanmin(r_all)), float(np.nanmax(r_all))
                rr = rmax - rmin
                rpad = 0.05 * rr if rr > 0 else 0.05 * abs(rmin if rmin else 1.0)
                ax1.set_ylim(rmin - rpad, rmax + rpad)
                ax1.set_ylabel("Residual")
                ax1.set_xlabel(wavelength_str)

    else:
        if mask is not None:
            mask = mask.astype(bool, copy=False)
            x_good      = obs_pix[mask]
            y_obs_good  = y_obs[mask]
            y_mod_good  = y_model[mask]
            x_masked    = obs_pix[~mask]
            y_obs_masked = y_obs[~mask]
            y_mod_masked = y_model[~mask]
        else:
            x_good = obs_pix
            y_obs_good = y_obs
            y_mod_good = y_model
            x_masked = y_obs_masked = y_mod_masked = None

        ax0.plot(x_good, y_obs_good, label="Observed", alpha=0.7)
        ax0.plot(x_good, y_mod_good, label="Model", alpha=0.7)

        x_pad = 0.01 * (x_good[-1] - x_good[0])
        y_min = min(np.min(y_obs_good), np.min(y_mod_good))
        y_max = max(np.max(y_obs_good), np.max(y_mod_good))
        y_rng = y_max - y_min
        y_pad = 0.05 * y_rng if y_rng > 0 else 0.05 * abs(y_min if y_min else 1.0)
        ax0.set_xlim(x_good[0] - x_pad, x_good[-1] + x_pad)
        ax0.set_ylim(y_min - y_pad, y_max + y_pad)

        # Masked dots, clipped to y-limits
        if mask is not None and np.any(~mask):
            ylim = ax0.get_ylim()
            in_y_obs = (y_obs_masked > ylim[0]) & (y_obs_masked < ylim[1])
            in_y_mod = (y_mod_masked > ylim[0]) & (y_mod_masked < ylim[1])
            if np.any(in_y_obs):
                ax0.plot(x_masked[in_y_obs], y_obs_masked[in_y_obs],
                         '.', color="gray", alpha=0.3, markersize=6,
                         label="Masked (data)", zorder=1)
            if np.any(in_y_mod):
                ax0.plot(x_masked[in_y_mod], y_mod_masked[in_y_mod],
                         '.', color="orange", alpha=0.2, markersize=6,
                         label="Masked (model)", zorder=1)

        if show_residual:
            resid = y_obs_good - y_mod_good
            r_min, r_max = np.min(resid), np.max(resid)
            r_rng = r_max - r_min
            r_pad = 0.05 * r_rng if r_rng > 0 else 0.05 * abs(r_min if r_min else 1.0)
            ax1.plot(x_good, resid, color="gray")
            ax1.set_xlim(x_good[0] - x_pad, x_good[-1] + x_pad)
            ax1.set_ylim(r_min - r_pad, r_max + r_pad)
            ax1.set_ylabel("Residual")
            ax1.set_xlabel(wavelength_str)
            if mask is not None and np.any(~mask):
                ylim = ax1.get_ylim()
                resid_masked = (y_obs - y_model)[~mask]
                x_masked_resid = obs_pix[~mask]
                in_ylim = (resid_masked > ylim[0]) & (resid_masked < ylim[1])
                if np.any(in_ylim):
                    ax1.plot(x_masked_resid[in_ylim], resid_masked[in_ylim],
                             '.', color="gray", alpha=0.15, markersize=4,
                             zorder=1, clip_on=False)

    ax0.set_title(f"Aperture {aperture_index}")
    ax0.set_ylabel("Flux")
    ax0.legend()

# ------------------------------------------------------------------------------

def plot_white_light_images(
    data_cube: np.ndarray,
    model_cube: np.ndarray,
    save_path: str | None = None,
) -> None:
    """
    Summation along spectral axis → "white-light" images.

    Parameters
    ----------
    data_cube  : ndarray, (ny, nx, nPix)
    model_cube : ndarray, same shape
    save_path  : str | None
        If provided, PNG is written to this path; otherwise shown on screen.
    """
    wl_data   = data_cube.sum(-1)
    wl_model  = model_cube.sum(-1)
    wl_resid  = wl_data - wl_model

    fig, axes = plt.subplots(1, 3, figsize=(12, 4))
    for ax, img, title in zip(
        axes,
        (wl_data, wl_model, wl_resid),
        ("Data (white-light)", "Model (white-light)", "Residual"),
    ):
        im = ax.imshow(img, origin="lower", cmap="gray")
        ax.set_title(title)
        ax.axis("off")
        fig.colorbar(im, ax=ax, fraction=0.046)

    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=150)
        plt.close(fig)
    else:
        plt.show()

# ------------------------------------------------------------------------------

def plot_model_decomposition(
    y_obs: np.ndarray,          # Observed spectrum (full, not masked)
    obs_pix: np.ndarray,        # Wavelength array
    mask: np.ndarray,           # Boolean mask array (same shape)
    x_ref: np.ndarray,          # Fit solution vector
    A0: np.ndarray,             # Full design matrix (all columns)
    nComp: int, nPop: int,      # Number of components and populations
    C: np.ndarray | None = None,# Continuum matrix (nLSpec, nContinuum) or None
    aperture_index: int | str = "stacked",
    wavelength_str: str | None = None,
    show_residual: bool = True,
) -> None:
    """
    Plot observed, main model, velocity shift, continuum, and residual.

    This is primarily for *reference NNLS* diagnostics (single or stacked
    spectrum). It assumes your A0 column order:
        [templates | velshift | continuum]

    Parameters
    ----------
    y_obs, obs_pix, mask : arrays
        Full (unmasked) spectrum, wavelengths, and boolean mask.
    x_ref : ndarray
        Full solution vector from NNLS (including velshift/continuum if present).
    A0 : ndarray
        Design matrix, shape (nLSpec_total, nCols).
    nComp, nPop : int
        Numbers of components and populations.
    C : ndarray or None
        Continuum basis used in the fit. If None, continuum is omitted.
    """
    if wavelength_str is None:
        wavelength_str = r"Wavelength [$\AA$]"

    nTemplates = nComp * nPop
    # Block order: [templates | velshift | continuum]
    w_templates = x_ref[:nTemplates]
    w_velshift  = x_ref[nTemplates:2 * nTemplates]
    has_cont    = (C is not None and C.shape[1] > 0)
    w_cont      = x_ref[2 * nTemplates:] if has_cont else np.array([])

    model_main   = A0[:, :nTemplates] @ w_templates
    model_vshift = A0[:, nTemplates:2 * nTemplates] @ w_velshift
    model_cont   = (A0[:, 2 * nTemplates:] @ w_cont) if has_cont else 0.0
    model_total  = model_main + model_vshift + model_cont

    # Figure
    if show_residual:
        from matplotlib.gridspec import GridSpec
        gs = GridSpec(2, 1, height_ratios=[3, 1])
        ax0 = plt.subplot(gs[0])
    else:
        plt.figure(figsize=(10, 5))
        ax0 = plt.gca()

    # Plot only unmasked in solid lines
    mask = mask.astype(bool, copy=False)
    x_good = obs_pix[mask]
    ax0.plot(x_good, y_obs[mask], label="Observed", color="k", lw=1)
    ax0.plot(x_good, model_main[mask], label="Model (no vshift/cont)",
             color="b", lw=1)
    ax0.plot(x_good, (model_main + model_vshift)[mask],
             label="Model + vshift", color="r", lw=1)
    if has_cont:
        ax0.plot(x_good, model_total[mask],
                 label="Full model (+ continuum)", color="g", lw=1)

    # Velocity/continuum trends across all pixels (dashed)
    ax0.plot(obs_pix, model_vshift, '--', color="orange", lw=1,
             label="Velocity shift (all pixels)", alpha=0.7, zorder=2)
    if has_cont:
        ax0.plot(obs_pix, model_cont, '--', color="purple", lw=1,
                 label="Continuum (all pixels)", alpha=0.7, zorder=3)

    # Limits
    x_pad = 0.01 * (x_good[-1] - x_good[0])
    ax0.set_xlim(x_good[0] - x_pad, x_good[-1] + x_pad)
    lines = [y_obs[mask], model_main[mask], (model_main + model_vshift)[mask]]
    if has_cont:
        lines.append(model_total[mask])
    y_min = np.min([np.min(l) for l in lines])
    y_max = np.max([np.max(l) for l in lines])
    y_rng = y_max - y_min
    y_pad = 0.05 * y_rng if y_rng > 0 else 0.05 * abs(y_min if y_min else 1.0)
    ax0.set_ylim(y_min - y_pad, y_max + y_pad)
    ylim = ax0.get_ylim()

    # Masked dots, clipped
    if np.any(~mask):
        x_masked = obs_pix[~mask]
        y_obs_masked = y_obs[~mask]
        y_model_masked = model_total[~mask] if has_cont else \
                         (model_main + model_vshift)[~mask]
        in_y_obs = (y_obs_masked > ylim[0]) & (y_obs_masked < ylim[1])
        in_y_mod = (y_model_masked > ylim[0]) & (y_model_masked < ylim[1])
        if np.any(in_y_obs):
            ax0.plot(x_masked[in_y_obs], y_obs_masked[in_y_obs],
                     '.', color="gray", alpha=0.3, markersize=6,
                     label="Masked (data)", zorder=1)
        if np.any(in_y_mod):
            ax0.plot(x_masked[in_y_mod], y_model_masked[in_y_mod],
                     '.', color="lime", alpha=0.2, markersize=6,
                     label="Masked (model)", zorder=1)

    ax0.set_title(f"Aperture {aperture_index}")
    ax0.set_ylabel("Flux")
    ax0.legend(fontsize=8, loc="best")

    # Residuals panel
    if show_residual:
        ax1 = plt.subplot(gs[1])
        resid = y_obs[mask] - model_total[mask]
        ax1.plot(x_good, resid, color="gray")
        r_min, r_max = np.min(resid), np.max(resid)
        r_rng = r_max - r_min
        r_pad = 0.05 * r_rng if r_rng > 0 else 0.05 * abs(r_min if r_min else 1.0)
        ax1.set_xlim(x_good[0] - x_pad, x_good[-1] + x_pad)
        ax1.set_ylim(r_min - r_pad, r_max + r_pad)
        ax1.set_ylabel("Residual")
        ax1.set_xlabel(wavelength_str)
        if np.any(~mask):
            resid_masked = (y_obs - model_total)[~mask]
            ylim_res = ax1.get_ylim()
            in_ylim = (resid_masked > ylim_res[0]) & (resid_masked < ylim_res[1])
            if np.any(in_ylim):
                ax1.plot(obs_pix[~mask][in_ylim], resid_masked[in_ylim],
                         '.', color="gray", alpha=0.15, markersize=4, zorder=1)
    else:
        ax0.set_xlabel(wavelength_str)

# ------------------------------------------------------------------------------
</file>

<file path="pre_solve_check.py">
#!/usr/bin/env python3
# Pre-solve sanity checks for CubeFit HDF5 files.

import sys, argparse, json, random
from typing import List
try:
    import h5py
    import numpy as np
except Exception:
    print("ERROR: This script requires h5py and numpy. Please install them.", file=sys.stderr)
    sys.exit(2)

def validate_schema(h5_path: str):
    issues: List[str] = []
    warnings: List[str] = []
    with h5py.File(h5_path, "r", swmr=True) as f:
        # Required datasets
        required = [
            "/DataCube", "/Templates", "/LOSVD", "/TemPix", "/ObsPix", "/VelPix",
            "/R_T", "/HyperCube/models", "/HyperCube/_done"
        ]
        missing = [p for p in required if p not in f]
        if missing:
            issues += [f"missing dataset: {p}" for p in missing]
            return False, issues, warnings

        # Shapes
        S, L = f["/DataCube"].shape
        P, T = f["/Templates"].shape
        S2, V, C = f["/LOSVD"].shape
        T2 = f["/TemPix"].shape[0]
        L2 = f["/ObsPix"].shape[0]
        V2 = f["/VelPix"].shape[0]

        if S2 != S: issues.append(f"/LOSVD first dim S={S2} ≠ /DataCube S={S}")
        if V2 != V: issues.append(f"/VelPix V={V2} ≠ /LOSVD V={V}")
        if T2 != T: issues.append(f"/TemPix T={T2} ≠ /Templates T={T}")
        if L2 != L: issues.append(f"/ObsPix L={L2} ≠ /DataCube L={L}")

        # Rebin operators
        if f["/R_T"].shape != (T, L):
            issues.append(f"/R_T shape {f['/R_T'].shape} != (T,L)=({T},{L})")
        if "/RebinMatrix" in f and f["/RebinMatrix"].shape != (L, T):
            issues.append(f"/RebinMatrix shape {f['/RebinMatrix'].shape} != (L,T)=({L},{T})")
        if f["/R_T"].dtype != np.float32:
            warnings.append(f"/R_T dtype {f['/R_T'].dtype} (expected float32)")

        # FFT caches (optional)
        if "/TemplatesFFT" in f:
            tfft = f["/TemplatesFFT"]
            if tfft.shape != (P, T//2 + 1):
                issues.append(f"/TemplatesFFT shape {tfft.shape} != (P, T//2+1)=({P},{T//2+1})")
            if tfft.dtype.kind != "c":
                warnings.append(f"/TemplatesFFT dtype {tfft.dtype} (expected complex)")
        if "/TemplatesFFT_R" in f:
            tfftr = f["/TemplatesFFT_R"]
            if tfftr.shape != (P, T//2 + 1):
                issues.append(f"/TemplatesFFT_R shape {tfftr.shape} != (P, T//2+1)")

        # Mask (optional)
        if "/Mask" in f and f["/Mask"].shape[0] != L:
            issues.append(f"/Mask length {f['/Mask'].shape[0]} != L={L}")

        # HyperCube dataset
        HC = f["/HyperCube/models"]
        if HC.shape != (S, C, P, L):
            issues.append(f"/HyperCube/models shape {HC.shape} != (S,C,P,L)=({S},{C},{P},{L})")
        if HC.dtype != np.float32:
            warnings.append(f"/HyperCube/models dtype {HC.dtype} (expected float32)")

        # Root attrs
        if "dims" in f["/"].attrs:
            try:
                d = f["/"].attrs["dims"]
                d = json.loads(d) if isinstance(d, (bytes, str)) else d
                check = dict(nSpat=S, nLSpec=L, nTSpec=T, nVel=V, nComp=C, nPop=P)
                for k, v in check.items():
                    key = str(k)
                    if key in d and int(d[key]) != v:
                        issues.append(f"dims[{k}]={d[key]} ≠ {v}")
            except Exception as e:
                warnings.append(f"failed to parse root attr 'dims': {e}")
        else:
            warnings.append("root attr 'dims' missing")

        # Grid monotonicity
        for name in ["/TemPix", "/ObsPix", "/VelPix"]:
            v = f[name][...]
            if not np.all(np.diff(v) > 0):
                issues.append(f"{name} is not strictly increasing")

    return (len(issues) == 0), issues, warnings

def transpose_cache_delta(f) -> float:
    if "/RebinMatrix" not in f:
        return float("nan")
    RM = f["/RebinMatrix"][...]
    RTt = f["/R_T"][...].T
    return float(np.max(np.abs(RM - RTt)))

def sample_neg_nan_stats(f, samples=3, seed=0):
    rng = random.Random(seed)
    M = f["/HyperCube/models"]; S,C,P,L = M.shape
    picks = [(0,0), (S-1, C-1)]
    for _ in range(max(0, samples - len(picks))):
        picks.append((rng.randrange(S), rng.randrange(C)))
    results = []
    for s,c in picks:
        slab = np.asarray(M[s,c,:,:], np.float32)  # (P,L)
        neg_frac = float((slab < 0).mean())
        has_nan = bool(np.isnan(slab).any())
        maxabs = float(np.max(np.abs(slab))) if slab.size else 0.0
        results.append(((s,c), neg_frac, has_nan, maxabs))
    return results

def main():
    ap = argparse.ArgumentParser(description="Pre-solve sanity checks for CubeFit HDF5 files.")
    ap.add_argument("h5_path", help="Path to HDF5 file")
    ap.add_argument("--samples", type=int, default=3, help="How many (s,c) slabs to probe")
    ap.add_argument("--seed", type=int, default=0, help="RNG seed for slab sampling")
    ap.add_argument("--max-transpose-delta", type=float, default=1e-6, help="Tolerance for /RebinMatrix vs /R_T.T")
    ap.add_argument("--fail-on-warn", action="store_true", help="Exit nonzero if warnings are present")
    args = ap.parse_args()

    ok, issues, warnings = validate_schema(args.h5_path)

    with h5py.File(args.h5_path, "r", swmr=True) as f:
        delta = transpose_cache_delta(f)
        if not np.isnan(delta):
            print(f"transpose cache max|Δ| = {delta:.3e}")
            if delta > args.max_transpose_delta:
                issues.append(f"/RebinMatrix differs from /R_T.T by {delta:.3e} > {args.max_transpose_delta}")
        else:
            print("transpose cache: /RebinMatrix not present (skip)")

        stats = sample_neg_nan_stats(f, samples=args.samples, seed=args.seed)
        for (s,c), neg_frac, has_nan, maxabs in stats:
            print(f\"(s={s}, c={c}) neg%={100*neg_frac:.4f}%  nan?={has_nan}  max|model|={maxabs:.3e}\")

        # Try optional edge diagnostics if available
        try:
            import hdf5_manager as H5M  # user's module
            if hasattr(H5M, "diagnose_rebin_edges"):
                safe = H5M.diagnose_rebin_edges(args.h5_path, quiet=True)
                print(f"diagnose_rebin_edges: Safe? {safe}")
                if not safe:
                    warnings.append("diagnose_rebin_edges reported Safe=False")
        except Exception as e:
            print(f"diagnose_rebin_edges: unavailable ({e.__class__.__name__})")

    # Print summary
    if issues:
        print("\\nSchema INVALID:")
        for i in issues:
            print(" -", i)
    else:
        print("\\nSchema OK")

    if warnings:
        print("\\nWarnings:")
        for w in warnings:
            print(" -", w)

    # Exit code
    if issues or (warnings and args.fail_on_warn):
        sys.exit(1)
    sys.exit(0)

if __name__ == "__main__":
    main()
</file>

<file path="docs/CubeFit.html">
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>CubeFit</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">CubeFit</h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a
href="#cubefit--losvd-convolved-stellar-population-fitting-for-ifu-cubes"
id="toc-cubefit--losvd-convolved-stellar-population-fitting-for-ifu-cubes">CubeFit
— LOSVD-convolved stellar population fitting for IFU cubes</a>
<ul>
<li><a href="#hdf5-layout-what-lives-where"
id="toc-hdf5-layout-what-lives-where">HDF5 layout (what lives
where)</a></li>
<li><a href="#manager-creating-and-populating-the-hdf5-backbone"
id="toc-manager-creating-and-populating-the-hdf5-backbone">Manager:
creating and populating the HDF5 backbone</a></li>
<li><a href="#hypercube-builder" id="toc-hypercube-builder">Hypercube
builder</a></li>
<li><a href="#normalization-conversion-post-hoc"
id="toc-normalization-conversion-post-hoc">Normalization conversion
(post-hoc)</a></li>
<li><a href="#λ-feature-emphasis-line-weights"
id="toc-λ-feature-emphasis-line-weights">λ-feature emphasis (line
weights)</a></li>
<li><a
href="#solver-multiprocess-batched-kaczmarz--weighted-nnls-polish"
id="toc-solver-multiprocess-batched-kaczmarz--weighted-nnls-polish">Solver:
multiprocess batched Kaczmarz (+ weighted NNLS polish)</a></li>
<li><a href="#reconstruction--plots-without-storing-a-full-modelcube"
id="toc-reconstruction--plots-without-storing-a-full-modelcube">Reconstruction
&amp; plots (without storing a full ModelCube)</a></li>
<li><a href="#fit-tracking--snapshots"
id="toc-fit-tracking--snapshots">Fit tracking &amp; snapshots</a></li>
<li><a href="#utilities" id="toc-utilities">Utilities</a></li>
<li><a href="#environment-variables-knobs"
id="toc-environment-variables-knobs">Environment variables (knobs)</a>
<ul>
<li><a href="#hdf5--dataset-caching" id="toc-hdf5--dataset-caching">HDF5
&amp; dataset caching</a></li>
<li><a href="#multiprocessing--blasopenmp"
id="toc-multiprocessing--blasopenmp">Multiprocessing &amp;
BLAS/OpenMP</a></li>
<li><a
href="#solver-worker-level-stability--steps-in-kaczmarz_solver_cchunk_mppy"
id="toc-solver-worker-level-stability--steps-in-kaczmarz_solver_cchunk_mppy">Solver:
worker-level stability &amp; steps (in
<code>kaczmarz_solver_cchunk_mp.py</code>)</a></li>
<li><a
href="#solver-aggregation--nnls-polish-in-kaczmarz_solver_cchunk_mppy"
id="toc-solver-aggregation--nnls-polish-in-kaczmarz_solver_cchunk_mppy">Solver:
aggregation &amp; NNLS polish (in
<code>kaczmarz_solver_cchunk_mp.py</code>)</a></li>
<li><a
href="#solver-λ-weights-in-kaczmarz_solver_cchunk_mppy--cube_utilspy"
id="toc-solver-λ-weights-in-kaczmarz_solver_cchunk_mppy--cube_utilspy">Solver:
λ-weights (in <code>kaczmarz_solver_cchunk_mp.py</code> /
<code>cube_utils.py</code>)</a></li>
<li><a href="#fit-tracking-in-fit_trackerpy"
id="toc-fit-tracking-in-fit_trackerpy">Fit tracking (in
<code>fit_tracker.py</code>)</a></li>
</ul></li>
<li><a href="#typical-end-to-end-workflow"
id="toc-typical-end-to-end-workflow">Typical end-to-end
workflow</a></li>
<li><a href="#notes-on-normalization--physics"
id="toc-notes-on-normalization--physics">Notes on normalization &amp;
physics</a></li>
<li><a href="#troubleshooting--sanity-checks"
id="toc-troubleshooting--sanity-checks">Troubleshooting &amp; sanity
checks</a></li>
<li><a href="#where-to-look-in-the-code"
id="toc-where-to-look-in-the-code">Where to look in the code</a></li>
<li><a href="#building-the-docs" id="toc-building-the-docs">Building the
docs</a></li>
</ul></li>
</ul>
</nav>
<h1
id="cubefit--losvd-convolved-stellar-population-fitting-for-ifu-cubes">CubeFit
— LOSVD-convolved stellar population fitting for IFU cubes</h1>
<p>CubeFit builds a <strong>hypercube of convolved template
spectra</strong> for every
<code>(spaxel S, component C, population P)</code> and solves for a
global, non-negative mixture that best explains the observed IFU data
cube. It is designed for large datasets (10^3–10^5 spaxels, 10^2–10^3
populations), is robust to restarts, and uses HDF5 end-to-end with
streaming, tile-aligned I/O.</p>
<ul>
<li><strong>Hypercube builder:</strong> FFT-based LOSVD convolution on
the template grid, followed by flux-conserving rebin to the observed
grid.</li>
<li><strong>Normalization:</strong> choose “model” (LOSVD amplitude) or
“data” (per-spaxel observed flux with LOSVD-proportional splits).
Convert between modes later without a rebuild.</li>
<li><strong>Line emphasis:</strong> optional λ-weights so absorption
features drive the fit.</li>
<li><strong>Solver:</strong> multiprocess Kaczmarz with diagonal
preconditioning, trust region, backtracking, global step caps, near-zero
column freezing, optional de-correlation nudge, and a tile-local
<strong>weighted NNLS polish</strong>.</li>
<li><strong>Tracking:</strong> streaming fit metrics and optional
snapshots to a SWMR-friendly sidecar.</li>
<li><strong>Utilities:</strong> chunk-aware normalization conversion;
global column energy; reconstruction and plotting helpers that never
touch the giant <code>/HyperCube/models</code> unless needed.</li>
</ul>
<hr />
<h2 id="hdf5-layout-what-lives-where">HDF5 layout (what lives
where)</h2>
<p>Core inputs:</p>
<ul>
<li><code>/Templates</code> — <code>(P, T)</code> template spectra on
the template grid (time domain).</li>
<li><code>/TemPix</code> — <code>(T,)</code> template grid in log-λ
(natural log).</li>
<li><code>/ObsPix</code> — <code>(L,)</code> observed wavelength grid
(log-λ).</li>
<li><code>/R_T</code> — <code>(T, L)</code> (or <code>(L, T)</code>)
<strong>flux-conserving</strong> linear rebin operator mapping the
template grid to the observed grid.</li>
<li><code>/LOSVD</code> — <code>(S, V, C)</code> LOSVD histograms (per
spaxel and component).</li>
<li><code>/VelPix</code> — <code>(V,)</code> velocity grid in km/s (for
LOSVD).</li>
</ul>
<p>Optional inputs:</p>
<ul>
<li><code>/Mask</code> — <code>(L,)</code> boolean wavelength mask; used
consistently by builder and solver.</li>
<li><code>/HyperCube/data_flux</code> — <code>(S,)</code> masked mean
data flux per spaxel (required for <code>norm="data"</code>).</li>
</ul>
<p>Built artifacts:</p>
<ul>
<li><code>/HyperCube/models</code> — <code>(S, C, P, L)</code> float32
convolved+rebinned spectra, chunked for streaming and resumable via a
<code>_done</code> bitmap.</li>
<li><code>/HyperCube/col_energy</code> — <code>(C, P)</code> float64
global column energy <code>E[c,p] = Σ_{s,λ in mask} A^2</code>, for
step-size control.</li>
<li><code>/HyperCube/norm/losvd_amp</code> — <code>(S, C)</code> LOSVD
amplitude (sum or trapz).</li>
<li><code>/HyperCube/norm/losvd_amp_sum</code> — <code>(S,)</code>
per-spaxel sum of amplitudes.</li>
<li><code>/HyperCube/lambda_weights</code> — <code>(L,)</code> optional
λ-weights in <code>[floor, 1]</code> (generated by a median-DoG
heuristic).</li>
</ul>
<p>Fit outputs:</p>
<ul>
<li><code>/X_global</code> — <code>(C*P,)</code> solution vector,
row-major <code>(C, P)</code> flattened.</li>
</ul>
<hr />
<h2 id="manager-creating-and-populating-the-hdf5-backbone">Manager:
creating and populating the HDF5 backbone</h2>
<p><code>hdf5_manager.py</code> provides:</p>
<ul>
<li><strong>Safe file open</strong> with retries, optional SWMR, and
lock-handling (<code>open_h5</code>).</li>
<li><strong>Dataset creation</strong> for the core arrays and dimensions
(<code>H5Manager</code> + <code>H5Dims</code>).</li>
<li><strong>Population</strong> from NumPy arrays
(<code>populate_from_arrays</code>), including grid checks.</li>
<li><strong>Flux-conserving rebin</strong> operator construction
(<code>ensure_rebin_and_resample</code>).</li>
</ul>
<p>Typical setup:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> CubeFit.hdf5_manager <span class="im">import</span> H5Manager, H5Dims</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>mgr <span class="op">=</span> H5Manager(<span class="st">&quot;galaxy.h5&quot;</span>, tem_pix<span class="op">=</span>tem_loglam, obs_pix<span class="op">=</span>obs_loglam)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>mgr.init_base(H5Dims(nSpat<span class="op">=</span>S, nLSpec<span class="op">=</span>L, nTSpec<span class="op">=</span>T, nVel<span class="op">=</span>V, nComp<span class="op">=</span>C, nPop<span class="op">=</span>P))</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>mgr.populate_from_arrays(losvd<span class="op">=</span>H_SVC, datacube<span class="op">=</span>Y_SL, templates<span class="op">=</span>T_PT)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>mgr.ensure_rebin_and_resample()  <span class="co"># builds /R_T (and validates shapes)</span></span></code></pre></div>
<p>Notes:</p>
<ul>
<li><code>/R_T</code> is built by exact bin overlap in log-λ, preserving
flux.</li>
<li><code>/Mask</code> (if present) is used later by both builder and
solver.</li>
<li>Manager helpers centralize chunk-cache setup so downstream is
fast.</li>
</ul>
<hr />
<h2 id="hypercube-builder">Hypercube builder</h2>
<p><code>hypercube_builder.py</code> constructs
<code>/HyperCube/models</code> in streaming, tile-aligned passes:</p>
<ol type="1">
<li><strong>Kernel from LOSVD:</strong> each <code>(s,c)</code> LOSVD
row → <strong>unit-area</strong> kernel on the template grid (linear
interpolation to integer pixel shifts). A separate scalar
<strong>amplitude</strong> is computed per <code>(s,c)</code> (either
sum or trapezoidal integral).</li>
<li><strong>Convolution (per p):</strong> FFT-multiply template by
kernel and crop the linear convolution back to the template length.</li>
<li><strong>Rebin to observed grid:</strong> multiply by
<code>/R_T</code> to get <code>(P, L)</code> for each
<code>(s,c)</code>.</li>
<li><strong>Normalization:</strong>
<ul>
<li><code>norm="model"</code>: multiply each <code>(s,c,p,:)</code> by
its LOSVD amplitude.</li>
<li><code>norm="data"</code>: compute per-spaxel mean observed flux
(from <code>/HyperCube/data_flux</code>), then split across components
in proportion to their LOSVD amplitudes, preserving the (c) ratios
determined by LOSVD at that spaxel.</li>
</ul></li>
<li><strong>Global column energy:</strong> simultaneously accumulate
<code>E[c,p]=Σ_{s,λ in mask} A²</code> into
<code>/HyperCube/col_energy</code> for solver preconditioning.</li>
</ol>
<p><strong>Resumability &amp; chunking:</strong> the builder creates
<code>/HyperCube/models</code> with chunks
<code>(S_chunk, C_chunk, P_chunk, L)</code> and maintains a
<code>_done</code> bitmap over the
<code>(S_chunk, C_chunk, P_chunk)</code> tile grid; this lets you resume
safely or inspect progress mid-build. Metadata is flushed regularly.</p>
<hr />
<h2 id="normalization-conversion-post-hoc">Normalization conversion
(post-hoc)</h2>
<p>Flip between <code>norm="data"</code> and <code>norm="model"</code>
after the build:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> CubeFit.hypercube_builder <span class="im">import</span> convert_hypercube_norm</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>convert_hypercube_norm(<span class="st">&quot;galaxy.h5&quot;</span>, to_mode<span class="op">=</span><span class="st">&quot;model&quot;</span>, recompute_energy<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>This reads <code>/HyperCube/norm/losvd_amp_sum</code> (and
<code>/HyperCube/data_flux</code> when needed), computes a
<strong>per-spaxel scalar</strong> <code>F[s]</code>, and scales
<code>/HyperCube/models[s,:,:,:] *= F[s]</code> in
<strong>S-tiles</strong> (chunk-aligned). It updates
<code>/HyperCube/col_energy</code> (optional) and flips the
<code>/HyperCube</code> attribute <code>norm.mode</code>. It also
respects <code>/Mask</code> when deriving or validating the spaxel flux
vector.</p>
<p>This is a safe, <strong>no-FFT</strong> operation. Use it to avoid
rebuilding the hypercube when only a normalization flip is required.</p>
<hr />
<h2 id="λ-feature-emphasis-line-weights">λ-feature emphasis (line
weights)</h2>
<p>Absorption features can be down-weighted in plain least squares. To
emphasize them, the solver supports <strong>λ-weights</strong>:</p>
<ul>
<li>Build weights once via a <strong>median spectrum</strong> and a
two-scale smoothing <strong>difference-of-Gaussians</strong>
(implemented with boxcars), then map to <code>[min_w, 1]</code>. Store
at <code>/HyperCube/lambda_weights</code>.</li>
<li>During solving, multiply both <strong>residuals</strong> and
<strong>design matrix columns</strong> by <code>sqrt(w)</code> —
equivalent to solving in a diagonal metric where line pixels matter
more.</li>
</ul>
<p>Helpers in <code>cube_utils.py</code>:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> CubeFit.cube_utils <span class="im">import</span> ensure_lambda_weights, read_lambda_weights</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> ensure_lambda_weights(<span class="st">&quot;galaxy.h5&quot;</span>)  <span class="co"># writes /HyperCube/lambda_weights</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>w2 <span class="op">=</span> read_lambda_weights(<span class="st">&quot;galaxy.h5&quot;</span>)   <span class="co"># reads, with floor &amp; mask handling</span></span></code></pre></div>
<p>At solver startup you’ll see a banner printing
<code>min/max/mean</code> of the λ-weights used (after masking).</p>
<hr />
<h2
id="solver-multiprocess-batched-kaczmarz--weighted-nnls-polish">Solver:
multiprocess batched Kaczmarz (+ weighted NNLS polish)</h2>
<p>Entry point:</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> CubeFit.pipeline_runner <span class="im">import</span> PipelineRunner</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>runner <span class="op">=</span> PipelineRunner(<span class="st">&quot;galaxy.h5&quot;</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>x_global, stats <span class="op">=</span> runner.solve_all_mp_batched(</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>E,</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    pixels_per_aperture<span class="op">=</span>...,            <span class="co"># row batching (if used)</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    lr<span class="op">=</span>...,                             <span class="co"># base learning-rate</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    project_nonneg<span class="op">=</span><span class="va">True</span>,                <span class="co"># x &gt;= 0</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    processes<span class="op">=</span>..., blas_threads<span class="op">=</span>...,    <span class="co"># MP and BLAS knobs</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    orbit_weights<span class="op">=</span>...,                  <span class="co"># optional (C,) ratio guidance</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># …other optional arguments…</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>High-level flow (<code>pipeline_runner.py</code> →
<code>kaczmarz_solver_cchunk_mp.py</code>):</p>
<ol type="1">
<li><strong>Tile scheduling:</strong> sort S-tiles by data norm (coarse
“hardest first” ordering).</li>
<li><strong>Workers:</strong> each process takes a contiguous
<strong>c-band</strong> for the active S-tile.</li>
<li><strong>Inside a worker (per band):</strong>
<ul>
<li><strong>Sanitize</strong> non-finite values in <code>A</code> and
<code>R</code> to zeros.</li>
<li><strong>Apply λ-weights</strong> by computing gradients and
denominators on <code>√w · A</code> and <code>√w · R</code> (but updates
and residuals are returned in the unweighted space).</li>
<li><strong>Diagonal preconditioning:</strong> per-population step
<code>dx_p ∝ g_p / Σ (A_w)^2</code>, blended with global column energy
<code>E[c,p]</code> for stability.</li>
<li><strong>Near-zero column freeze</strong> (relative &amp; absolute
thresholds).</li>
<li><strong>Trust region</strong> on ΔR in weighted space
(<code>||ΔR_w|| ≤ τ ||R_w||</code>) + <strong>backtracking</strong> for
monotone weighted RMSE drop.</li>
<li><strong>Optional de-correlation nudge</strong> to avoid chasing flat
null-space directions across <code>P</code>.</li>
<li><strong>Candidate set</strong> for <strong>tile-local weighted NNLS
polish</strong> (top-K by score).</li>
</ul></li>
<li><strong>Parent process:</strong>
<ul>
<li>Aggregates ΔR over bands and runs <strong>tile-level
backtracking</strong>.</li>
<li>Applies a <strong>global step cap</strong> using
<code>Σ dx² · E[c,p]</code> vs. <code>||Y||</code>.</li>
<li>Optionally runs <strong>NNLS polish</strong> on the proposed (c,p)
set using a λ-subsample (rows) and accepts only if it reduces
<strong>weighted RMSE</strong> enough.</li>
<li>Commits the single global α for the tile: update <code>R</code> and
<code>x</code>, enforce <code>x ≥ 0</code>.</li>
<li>Optional <strong>ratio penalty</strong> step nudges the component
totals toward <code>orbit_weights</code> (if provided).</li>
<li>Streams metrics to <code>FitTracker</code> and snapshots
(optional).</li>
</ul></li>
</ol>
<p>The result is written to <code>/X_global</code> (flattened
<code>(C*P,)</code>), along with run stats.</p>
<hr />
<h2
id="reconstruction--plots-without-storing-a-full-modelcube">Reconstruction
&amp; plots (without storing a full ModelCube)</h2>
<p>You can reconstruct <code>Ŷ = A·x</code> for diagnostics without
materializing a full <code>/ModelCube</code>:</p>
<ul>
<li><strong>Reconstruction utilities</strong> read <code>A</code> in the
same chunk order and contract against <code>x</code> in λ-bands. They
write a small target array you name (e.g., <code>/ReconTile</code>) or
return the result.</li>
<li><strong>Parallel spectral plots</strong> pull just the selected rows
from <code>/DataCube</code> and the reconstructed array and produce
side-by-side data/model spectral plots for worst/best spaxels.</li>
</ul>
<p>Both are tile-aligned and cache-aware; they never read
<code>/HyperCube/models</code> beyond the needed slices.</p>
<hr />
<h2 id="fit-tracking--snapshots">Fit tracking &amp; snapshots</h2>
<p><code>fit_tracker.py</code> provides a <strong>non-blocking</strong>
sidecar writer process:</p>
<ul>
<li>Destination: a sidecar H5 next to the main file
(<code>*.fit.&lt;pid&gt;.&lt;ts&gt;.h5</code>).</li>
<li>Bounded <code>mp.Queue</code> so the solver never blocks on
I/O.</li>
<li>SWMR enabled for dashboards.</li>
<li>Streams <strong>RMSE history</strong>, <strong>EWMA</strong>,
<strong>progress</strong>, and optional <strong>x snapshots</strong>
(best/last/history).</li>
</ul>
<p>Use <code>tracker=FitTracker(...)</code> in the solver; the runner
wires this up for you.</p>
<hr />
<h2 id="utilities">Utilities</h2>
<ul>
<li><strong>Normalization conversion:</strong>
<code>convert_hypercube_norm(h5, to_mode="model"|"data", recompute_energy=True)</code>
— chunk-aligned, mask-aware.</li>
<li><strong>Global column energy:</strong> created during build; can be
recomputed later if you changed normalization.</li>
<li><strong>Done-bitmap control:</strong> functions to inspect and
invalidate <code>_done</code> to resume or redo specific tiles.</li>
<li><strong>Recompression:</strong> rewrite
<code>/HyperCube/models</code> with different compression/filter options
(tile-wise copy).</li>
</ul>
<hr />
<h2 id="environment-variables-knobs">Environment variables (knobs)</h2>
<p>These can be exported to tune performance, stability, or behavior.
Defaults shown are what the code uses if you don’t set them.</p>
<h3 id="hdf5--dataset-caching">HDF5 &amp; dataset caching</h3>
<table>
<thead>
<tr class="header">
<th>Variable</th>
<th style="text-align: right;">Default</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>HDF5_USE_FILE_LOCKING</code></td>
<td style="text-align: right;"><code>FALSE</code></td>
<td>Avoid HDF5 file locking issues on shared filesystems.</td>
</tr>
<tr class="even">
<td><code>CUBEFIT_RDCC_NSLOTS</code></td>
<td style="text-align: right;"><code>400003</code></td>
<td>File-level raw chunk cache: number of slots.</td>
</tr>
<tr class="odd">
<td><code>CUBEFIT_RDCC_NBYTES</code></td>
<td style="text-align: right;"><code>4294967296</code> (4 GiB)</td>
<td>File-level raw chunk cache size in bytes.</td>
</tr>
<tr class="even">
<td><code>CUBEFIT_RDCC_W0</code></td>
<td style="text-align: right;"><code>0.9</code></td>
<td>File-level raw chunk cache preemption policy.</td>
</tr>
<tr class="odd">
<td><code>CUBEFIT_RDCC_SLOTS</code></td>
<td style="text-align: right;"><code>1000003</code></td>
<td>Per-dataset cache slots used by some readers.</td>
</tr>
<tr class="even">
<td><code>CUBEFIT_RDCC_BYTES</code></td>
<td style="text-align: right;"><code>268435456</code> (256 MiB)</td>
<td>Per-dataset cache size used by some readers.</td>
</tr>
</tbody>
</table>
<h3 id="multiprocessing--blasopenmp">Multiprocessing &amp;
BLAS/OpenMP</h3>
<table>
<thead>
<tr class="header">
<th>Variable</th>
<th style="text-align: right;">Default</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>CUBEFIT_MP_CTX</code></td>
<td style="text-align: right;"><code>forkserver</code> (fallback
<code>spawn</code>)</td>
<td>MP start method for the solver pool.</td>
</tr>
<tr class="even">
<td><code>OMP_NUM_THREADS</code></td>
<td style="text-align: right;">set by code</td>
<td>BLAS/OpenMP threads per worker (also
<code>OPENBLAS_NUM_THREADS</code>, <code>MKL_NUM_THREADS</code>).</td>
</tr>
<tr class="odd">
<td><code>KMP_INIT_AT_FORK</code></td>
<td style="text-align: right;"><code>FALSE</code></td>
<td>Avoid OpenMP deadlocks with <code>fork</code>.</td>
</tr>
<tr class="even">
<td><code>SLURM_CPUS_PER_TASK</code></td>
<td style="text-align: right;">(if present)</td>
<td>Used by reconstruction helpers to set BLAS threads.</td>
</tr>
<tr class="odd">
<td><code>PYTHONUNBUFFERED</code></td>
<td style="text-align: right;"><code>1</code> (recommended)</td>
<td>Unbuffered stdout for real-time logs on HPC.</td>
</tr>
</tbody>
</table>
<h3
id="solver-worker-level-stability--steps-in-kaczmarz_solver_cchunk_mppy">Solver:
worker-level stability &amp; steps (in
<code>kaczmarz_solver_cchunk_mp.py</code>)</h3>
<table>
<thead>
<tr class="header">
<th>Variable</th>
<th style="text-align: right;">Default</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>CUBEFIT_BT_STEPS</code></td>
<td style="text-align: right;"><code>3</code></td>
<td>Backtracking steps per band (worker) for monotone weighted
RMSE.</td>
</tr>
<tr class="even">
<td><code>CUBEFIT_BT_FACTOR</code></td>
<td style="text-align: right;"><code>0.5</code></td>
<td>Multiplicative α shrink in backtracking.</td>
</tr>
<tr class="odd">
<td><code>CUBEFIT_TRUST_TAU</code></td>
<td style="text-align: right;"><code>0.7</code></td>
<td>Trust-region cap: `</td>
</tr>
<tr class="even">
<td><code>CUBEFIT_EPS</code></td>
<td style="text-align: right;"><code>1e-12</code></td>
<td>Numerical floor in denominators and divisions.</td>
</tr>
<tr class="odd">
<td><code>CUBEFIT_ZERO_COL_REL</code></td>
<td style="text-align: right;"><code>1e-12</code></td>
<td>Relative threshold (vs median energy) to <strong>freeze</strong>
near-zero columns.</td>
</tr>
<tr class="even">
<td><code>CUBEFIT_ZERO_COL_ABS</code></td>
<td style="text-align: right;"><code>1e-24</code></td>
<td>Absolute threshold to <strong>freeze</strong> columns.</td>
</tr>
<tr class="odd">
<td><code>CUBEFIT_DEBUG_SAFE</code></td>
<td style="text-align: right;"><code>0</code></td>
<td>Print sanitization/freeze stats per tile if <code>1</code>.</td>
</tr>
<tr class="even">
<td><code>CUBEFIT_NNLS_PROP_PER_BAND</code></td>
<td style="text-align: right;"><code>6</code></td>
<td>How many (c,p) candidates to propose for NNLS polish per band.</td>
</tr>
</tbody>
</table>
<h3
id="solver-aggregation--nnls-polish-in-kaczmarz_solver_cchunk_mppy">Solver:
aggregation &amp; NNLS polish (in
<code>kaczmarz_solver_cchunk_mp.py</code>)</h3>
<table>
<thead>
<tr class="header">
<th>Variable</th>
<th style="text-align: right;">Default</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>CUBEFIT_TILE_BT_STEPS</code></td>
<td style="text-align: right;"><code>6</code></td>
<td>Backtracking steps for the <strong>aggregated</strong> ΔR across
bands.</td>
</tr>
<tr class="even">
<td><code>CUBEFIT_TILE_BT_FACTOR</code></td>
<td style="text-align: right;"><code>0.5</code></td>
<td>Aggregated α shrink factor.</td>
</tr>
<tr class="odd">
<td><code>CUBEFIT_GLOBAL_TAU</code></td>
<td style="text-align: right;"><code>0.5</code></td>
<td>Global step cap using <code>Σ dx²·E[c,p]</code> vs `</td>
</tr>
<tr class="even">
<td><code>CUBEFIT_GLOBAL_ENERGY_BLEND</code></td>
<td style="text-align: right;"><code>1e-3</code></td>
<td>Blend local <code>Σ(A_w)^2</code> with global column energy
<code>E[c,p]</code> in denominators.</td>
</tr>
<tr class="odd">
<td><code>CUBEFIT_NNLS_ENABLE</code></td>
<td style="text-align: right;"><code>1</code></td>
<td>Enable tile-local <strong>weighted</strong> NNLS polish.</td>
</tr>
<tr class="even">
<td><code>CUBEFIT_NNLS_EVERY</code></td>
<td style="text-align: right;"><code>1</code></td>
<td>Run NNLS polish every N tiles.</td>
</tr>
<tr class="odd">
<td><code>CUBEFIT_NNLS_MAX_COLS</code></td>
<td style="text-align: right;"><code>128</code></td>
<td>Max columns K in the NNLS system.</td>
</tr>
<tr class="even">
<td><code>CUBEFIT_NNLS_MAX_BYTES</code></td>
<td style="text-align: right;"><code>1000000000</code></td>
<td>Approx memory cap for NNLS system (rows×8 per column).</td>
</tr>
<tr class="odd">
<td><code>CUBEFIT_NNLS_SUB_L</code></td>
<td style="text-align: right;"><code>0</code></td>
<td>λ-subsample size for NNLS (0 → use all).</td>
</tr>
<tr class="even">
<td><code>CUBEFIT_NNLS_SOLVER</code></td>
<td style="text-align: right;"><code>nnls</code></td>
<td>One of <code>nnls</code>, <code>lsq</code>, <code>mu</code>,
<code>fista</code> (FISTA recommended for speed/accuracy).</td>
</tr>
<tr class="odd">
<td><code>CUBEFIT_NNLS_MAX_ITER</code></td>
<td style="text-align: right;"><code>50</code></td>
<td>Iteration cap for iterative NNLS (<code>mu</code>,
<code>fista</code>, <code>lsq</code>’s TRF).</td>
</tr>
<tr class="even">
<td><code>CUBEFIT_NNLS_MIN_IMPROVE</code></td>
<td style="text-align: right;"><code>0.999</code></td>
<td>Required weighted-RMSE ratio to accept polish (&lt; this
accepts).</td>
</tr>
</tbody>
</table>
<h3
id="solver-λ-weights-in-kaczmarz_solver_cchunk_mppy--cube_utilspy">Solver:
λ-weights (in <code>kaczmarz_solver_cchunk_mp.py</code> /
<code>cube_utils.py</code>)</h3>
<table>
<thead>
<tr class="header">
<th>Variable</th>
<th style="text-align: right;">Default</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>CUBEFIT_LAMBDA_WEIGHTS_ENABLE</code></td>
<td style="text-align: right;"><code>1</code></td>
<td>Enable λ-weights for solver (<code>√w</code> weighting).</td>
</tr>
<tr class="even">
<td><code>CUBEFIT_LAMBDA_WEIGHTS_DSET</code></td>
<td
style="text-align: right;"><code>/HyperCube/lambda_weights</code></td>
<td>Dataset to read.</td>
</tr>
<tr class="odd">
<td><code>CUBEFIT_LAMBDA_MIN_W</code></td>
<td style="text-align: right;"><code>1e-6</code></td>
<td>Floor on λ-weights; protects √w.</td>
</tr>
<tr class="even">
<td><code>CUBEFIT_LAMBDA_WEIGHTS_AUTO</code></td>
<td style="text-align: right;"><code>1</code></td>
<td>Auto-generate weights if missing.</td>
</tr>
</tbody>
</table>
<h3 id="fit-tracking-in-fit_trackerpy">Fit tracking (in
<code>fit_tracker.py</code>)</h3>
<table>
<thead>
<tr class="header">
<th>Variable</th>
<th style="text-align: right;">Default</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>FITTRACKER_START</code></td>
<td style="text-align: right;"><code>spawn</code> (or best
available)</td>
<td>Start method for the tracker process
(<code>spawn</code>/<code>forkserver</code>/<code>fork</code>).</td>
</tr>
<tr class="even">
<td><code>CUBEFIT_TRACKER_QSIZE</code></td>
<td style="text-align: right;"><code>8192</code></td>
<td>Max queue size; drops messages when full if non-blocking.</td>
</tr>
<tr class="odd">
<td><code>CUBEFIT_TRACKER_FLUSH_EVERY</code></td>
<td style="text-align: right;"><code>128</code></td>
<td>Flush to disk after this many messages.</td>
</tr>
<tr class="even">
<td><code>CUBEFIT_TRACKER_FLUSH_SEC</code></td>
<td style="text-align: right;"><code>5.0</code></td>
<td>Or flush if this many seconds have passed.</td>
</tr>
<tr class="odd">
<td><code>CUBEFIT_RMSE_STRIDE</code></td>
<td style="text-align: right;"><code>16</code></td>
<td>Only enqueue every Nth batch RMSE sample (sub-sampling).</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="typical-end-to-end-workflow">Typical end-to-end workflow</h2>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) Build HDF5 backbone &amp; rebin operator</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>mgr <span class="op">=</span> H5Manager(<span class="st">&quot;galaxy.h5&quot;</span>, tem_pix<span class="op">=</span>tem_loglam, obs_pix<span class="op">=</span>obs_loglam)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>mgr.init_base(H5Dims(nSpat<span class="op">=</span>S, nLSpec<span class="op">=</span>L, nTSpec<span class="op">=</span>T, nVel<span class="op">=</span>V, nComp<span class="op">=</span>C, nPop<span class="op">=</span>P))</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>mgr.populate_from_arrays(losvd<span class="op">=</span>H_SVC, datacube<span class="op">=</span>Y_SL, templates<span class="op">=</span>T_PT)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>mgr.ensure_rebin_and_resample()</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) Build hypercube</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> CubeFit.hypercube_builder <span class="im">import</span> build_hypercube</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>build_hypercube(<span class="st">&quot;galaxy.h5&quot;</span>, norm_mode<span class="op">=</span><span class="st">&quot;data&quot;</span>)  <span class="co"># or &quot;model&quot;</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># (Optional) Flip normalization later without rebuild</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> CubeFit.hypercube_builder <span class="im">import</span> convert_hypercube_norm</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>convert_hypercube_norm(<span class="st">&quot;galaxy.h5&quot;</span>, to_mode<span class="op">=</span><span class="st">&quot;model&quot;</span>, recompute_energy<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 3) Create λ-weights</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> CubeFit.cube_utils <span class="im">import</span> ensure_lambda_weights</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>ensure_lambda_weights(<span class="st">&quot;galaxy.h5&quot;</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 4) Solve</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> CubeFit.pipeline_runner <span class="im">import</span> PipelineRunner</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>runner <span class="op">=</span> PipelineRunner(<span class="st">&quot;galaxy.h5&quot;</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>x, stats <span class="op">=</span> runner.solve_all_mp_batched(</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">6</span>, lr<span class="op">=</span><span class="fl">0.6</span>, project_nonneg<span class="op">=</span><span class="va">True</span>, processes<span class="op">=</span><span class="dv">8</span>, blas_threads<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    orbit_weights<span class="op">=</span>orbit_w  <span class="co"># optional (C,)</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<hr />
<h2 id="notes-on-normalization--physics">Notes on normalization &amp;
physics</h2>
<ul>
<li><code>norm="model"</code> keeps each <code>(s,c)</code> model’s
scale proportional to the LOSVD amplitude — closest to “physical
mass/flux” carried by LOSVD at that spaxel.</li>
<li><code>norm="data"</code> ties the per-spaxel model sum to the
observed mean flux (masked), splitting across components in proportion
to LOSVD amplitudes. This is convenient for direct data-scale
comparisons but can blur absolute differences across datasets. You can
switch modes post-hoc with <code>convert_hypercube_norm</code>.</li>
</ul>
<p>The solver respects <code>/Mask</code> and λ-weights uniformly, so
<strong>absorption features</strong> can dominate the fit when λ-weights
are present (verify the printed min/max/mean at startup to ensure they
are not all ones).</p>
<hr />
<h2 id="troubleshooting--sanity-checks">Troubleshooting &amp; sanity
checks</h2>
<ul>
<li><strong>Hypercube completeness:</strong> use the <code>_done</code>
bitmap helpers to inspect or reset tiles if needed.</li>
<li><strong>Normalization flips:</strong> use
<code>convert_hypercube_norm</code> (streams in S-tiles and handles
<code>/Mask</code>); do <strong>not</strong> read the entire dataset
into RAM.</li>
<li><strong>Line weights:</strong> ensure
<code>/HyperCube/lambda_weights</code> exists and spans a range (e.g.,
min ≪ 1, max ≈ 1); the solver prints min/max/mean.</li>
<li><strong>Population mixture looks uniform across components:</strong>
<ul>
<li>Confirm λ-weights are applied (banner shows they’re not all 1).</li>
<li>Increase epochs modestly; keep <code>GLOBAL_ENERGY_BLEND</code>
small but nonzero.</li>
<li>Enable NNLS polish with a moderate
<code>CUBEFIT_NNLS_MAX_COLS</code>, a λ-subsample
(<code>CUBEFIT_NNLS_SUB_L</code>), and
<code>CUBEFIT_NNLS_SOLVER=fista</code> for sharper per-tile
updates.</li>
</ul></li>
</ul>
<hr />
<h2 id="where-to-look-in-the-code">Where to look in the code</h2>
<ul>
<li><strong>HDF5 backbone:</strong> <code>hdf5_manager.py</code> (safe
open, grids, rebin operator).</li>
<li><strong>Builder:</strong> <code>hypercube_builder.py</code> (FFT
conv, normalization, col_energy, chunking + resume).</li>
<li><strong>Solver:</strong> <code>kaczmarz_solver_cchunk_mp.py</code>
(MP loop, λ-weights, trust/backtracking, NNLS polish).</li>
<li><strong>Runner:</strong> <code>pipeline_runner.py</code>
(orchestration, tracking, <code>/X_global</code>).</li>
<li><strong>Tracking:</strong> <code>fit_tracker.py</code> (sidecar
writer, RMSE history, progress, snapshots).</li>
<li><strong>Recon/plots:</strong> utilities in
<code>kz_fitSpec.py</code> (tile-aligned recon) and safe plotting
helpers.</li>
</ul>
<hr />
<h2 id="building-the-docs">Building the docs</h2>
<p>Place this file at <code>docs/CubeFit.md</code> and run:</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">make</span>            <span class="co"># builds docs/CubeFit.html and docs/CubeFit.pdf via pandoc</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="ex">mkdocs</span> serve    <span class="co"># live preview with Material theme (served from docs/)</span></span></code></pre></div>
<p>Pandoc targets (from your Makefile):</p>
<ul>
<li><code>docs/CubeFit.html</code> via:<br />
<code>pandoc -s -f gfm -t html5 docs/CubeFit.md -o docs/CubeFit.html --metadata title="CubeFit" --toc</code></li>
<li><code>docs/CubeFit.pdf</code> via XeLaTeX:<br />
<code>pandoc -s -f gfm docs/CubeFit.md -o docs/CubeFit.pdf --pdf-engine=xelatex -V geometry:margin=1in -V mainfont="Latin Modern Roman" -V monofont="Latin Modern Mono" --toc</code></li>
</ul>
<p>MkDocs (from your <code>mkdocs.yml</code>):</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">site_name</span><span class="kw">:</span><span class="at"> CubeFit</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">theme</span><span class="kw">:</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> material</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="fu">nav</span><span class="kw">:</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">Home</span><span class="kw">:</span><span class="at"> index.md</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">CubeFit</span><span class="kw">:</span><span class="at"> CubeFit.md</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="fu">docs_dir</span><span class="kw">:</span><span class="at"> docs</span></span></code></pre></div>
</body>
</html>
</file>

<file path="docs/CubeFit.md">
# CubeFit — LOSVD-convolved stellar population fitting for IFU cubes

CubeFit builds a **hypercube of convolved template spectra** for every `(spaxel S, component C, population P)` and solves for a global, non-negative mixture that best explains the observed IFU data cube. It is designed for large datasets (10^3–10^5 spaxels, 10^2–10^3 populations), is robust to restarts, and uses HDF5 end-to-end with streaming, tile-aligned I/O.

- **Hypercube builder:** FFT-based LOSVD convolution on the template grid, followed by flux-conserving rebin to the observed grid.
- **Normalization:** choose “model” (LOSVD amplitude) or “data” (per-spaxel observed flux with LOSVD-proportional splits). Convert between modes later without a rebuild.
- **Line emphasis:** optional λ-weights so absorption features drive the fit.
- **Solver:** multiprocess Kaczmarz with diagonal preconditioning, trust region, backtracking, global step caps, near-zero column freezing, optional de-correlation nudge, and a tile-local **weighted NNLS polish**.
- **Tracking:** streaming fit metrics and optional snapshots to a SWMR-friendly sidecar.
- **Utilities:** chunk-aware normalization conversion; global column energy; reconstruction and plotting helpers that never touch the giant `/HyperCube/models` unless needed.

---

## HDF5 layout (what lives where)

Core inputs:

- `/Templates` — `(P, T)` template spectra on the template grid (time domain).
- `/TemPix` — `(T,)` template grid in log-λ (natural log).
- `/ObsPix` — `(L,)` observed wavelength grid (log-λ).
- `/R_T` — `(T, L)` (or `(L, T)`) **flux-conserving** linear rebin operator mapping the template grid to the observed grid.
- `/LOSVD` — `(S, V, C)` LOSVD histograms (per spaxel and component).
- `/VelPix` — `(V,)` velocity grid in km/s (for LOSVD).

Optional inputs:

- `/Mask` — `(L,)` boolean wavelength mask; used consistently by builder and solver.
- `/HyperCube/data_flux` — `(S,)` masked mean data flux per spaxel (required for `norm="data"`).

Built artifacts:

- `/HyperCube/models` — `(S, C, P, L)` float32 convolved+rebinned spectra, chunked for streaming and resumable via a `_done` bitmap.
- `/HyperCube/col_energy` — `(C, P)` float64 global column energy `E[c,p] = Σ_{s,λ in mask} A^2`, for step-size control.
- `/HyperCube/norm/losvd_amp` — `(S, C)` LOSVD amplitude (sum or trapz).
- `/HyperCube/norm/losvd_amp_sum` — `(S,)` per-spaxel sum of amplitudes.
- `/HyperCube/lambda_weights` — `(L,)` optional λ-weights in `[floor, 1]` (generated by a median-DoG heuristic).

Fit outputs:

- `/X_global` — `(C*P,)` solution vector, row-major `(C, P)` flattened.

---

## Manager: creating and populating the HDF5 backbone

`hdf5_manager.py` provides:

- **Safe file open** with retries, optional SWMR, and lock-handling (`open_h5`).
- **Dataset creation** for the core arrays and dimensions (`H5Manager` + `H5Dims`).
- **Population** from NumPy arrays (`populate_from_arrays`), including grid checks.
- **Flux-conserving rebin** operator construction (`ensure_rebin_and_resample`).

Typical setup:

```python
from CubeFit.hdf5_manager import H5Manager, H5Dims

mgr = H5Manager("galaxy.h5", tem_pix=tem_loglam, obs_pix=obs_loglam)
mgr.init_base(H5Dims(nSpat=S, nLSpec=L, nTSpec=T, nVel=V, nComp=C, nPop=P))
mgr.populate_from_arrays(losvd=H_SVC, datacube=Y_SL, templates=T_PT)
mgr.ensure_rebin_and_resample()  # builds /R_T (and validates shapes)
```

Notes:

- `/R_T` is built by exact bin overlap in log-λ, preserving flux.
- `/Mask` (if present) is used later by both builder and solver.
- Manager helpers centralize chunk-cache setup so downstream is fast.

---

## Hypercube builder

`hypercube_builder.py` constructs `/HyperCube/models` in streaming, tile-aligned passes:

1. **Kernel from LOSVD:** each `(s,c)` LOSVD row → **unit-area** kernel on the template grid (linear interpolation to integer pixel shifts). A separate scalar **amplitude** is computed per `(s,c)` (either sum or trapezoidal integral).
2. **Convolution (per p):** FFT-multiply template by kernel and crop the linear convolution back to the template length.
3. **Rebin to observed grid:** multiply by `/R_T` to get `(P, L)` for each `(s,c)`.
4. **Normalization:**  
   - `norm="model"`: multiply each `(s,c,p,:)` by its LOSVD amplitude.  
   - `norm="data"`: compute per-spaxel mean observed flux (from `/HyperCube/data_flux`), then split across components in proportion to their LOSVD amplitudes, preserving the (c) ratios determined by LOSVD at that spaxel.
5. **Global column energy:** simultaneously accumulate `E[c,p]=Σ_{s,λ in mask} A²` into `/HyperCube/col_energy` for solver preconditioning.

**Resumability & chunking:** the builder creates `/HyperCube/models` with chunks `(S_chunk, C_chunk, P_chunk, L)` and maintains a `_done` bitmap over the `(S_chunk, C_chunk, P_chunk)` tile grid; this lets you resume safely or inspect progress mid-build. Metadata is flushed regularly.

---

## Normalization conversion (post-hoc)

Flip between `norm="data"` and `norm="model"` after the build:

```python
from CubeFit.hypercube_builder import convert_hypercube_norm
convert_hypercube_norm("galaxy.h5", to_mode="model", recompute_energy=True)
```

This reads `/HyperCube/norm/losvd_amp_sum` (and `/HyperCube/data_flux` when needed), computes a **per-spaxel scalar** `F[s]`, and scales `/HyperCube/models[s,:,:,:] *= F[s]` in **S-tiles** (chunk-aligned). It updates `/HyperCube/col_energy` (optional) and flips the `/HyperCube` attribute `norm.mode`. It also respects `/Mask` when deriving or validating the spaxel flux vector.

This is a safe, **no-FFT** operation. Use it to avoid rebuilding the hypercube when only a normalization flip is required.

---

## λ-feature emphasis (line weights)

Absorption features can be down-weighted in plain least squares. To emphasize them, the solver supports **λ-weights**:

- Build weights once via a **median spectrum** and a two-scale smoothing **difference-of-Gaussians** (implemented with boxcars), then map to `[min_w, 1]`. Store at `/HyperCube/lambda_weights`.
- During solving, multiply both **residuals** and **design matrix columns** by `sqrt(w)` — equivalent to solving in a diagonal metric where line pixels matter more.

Helpers in `cube_utils.py`:

```python
from CubeFit.cube_utils import ensure_lambda_weights, read_lambda_weights

w = ensure_lambda_weights("galaxy.h5")  # writes /HyperCube/lambda_weights
w2 = read_lambda_weights("galaxy.h5")   # reads, with floor & mask handling
```

At solver startup you’ll see a banner printing `min/max/mean` of the λ-weights used (after masking).

---

## Solver: multiprocess batched Kaczmarz (+ weighted NNLS polish)

Entry point:

```python
from CubeFit.pipeline_runner import PipelineRunner

runner = PipelineRunner("galaxy.h5")
x_global, stats = runner.solve_all_mp_batched(
    epochs=E,
    pixels_per_aperture=...,            # row batching (if used)
    lr=...,                             # base learning-rate
    project_nonneg=True,                # x >= 0
    processes=..., blas_threads=...,    # MP and BLAS knobs
    orbit_weights=...,                  # optional (C,) ratio guidance
    # …other optional arguments…
)
```

High-level flow (`pipeline_runner.py` → `kaczmarz_solver_cchunk_mp.py`):

1. **Tile scheduling:** sort S-tiles by data norm (coarse “hardest first” ordering).
2. **Workers:** each process takes a contiguous **c-band** for the active S-tile.
3. **Inside a worker (per band):**
   - **Sanitize** non-finite values in `A` and `R` to zeros.
   - **Apply λ-weights** by computing gradients and denominators on `√w · A` and `√w · R` (but updates and residuals are returned in the unweighted space).
   - **Diagonal preconditioning:** per-population step `dx_p ∝ g_p / Σ (A_w)^2`, blended with global column energy `E[c,p]` for stability.
   - **Near-zero column freeze** (relative & absolute thresholds).
   - **Trust region** on ΔR in weighted space (`||ΔR_w|| ≤ τ ||R_w||`) + **backtracking** for monotone weighted RMSE drop.
   - **Optional de-correlation nudge** to avoid chasing flat null-space directions across `P`.
   - **Candidate set** for **tile-local weighted NNLS polish** (top-K by score).
4. **Parent process:**
   - Aggregates ΔR over bands and runs **tile-level backtracking**.
   - Applies a **global step cap** using `Σ dx² · E[c,p]` vs. `||Y||`.
   - Optionally runs **NNLS polish** on the proposed (c,p) set using a λ-subsample (rows) and accepts only if it reduces **weighted RMSE** enough.
   - Commits the single global α for the tile: update `R` and `x`, enforce `x ≥ 0`.
   - Optional **ratio penalty** step nudges the component totals toward `orbit_weights` (if provided).
   - Streams metrics to `FitTracker` and snapshots (optional).

The result is written to `/X_global` (flattened `(C*P,)`), along with run stats.

---

## Reconstruction & plots (without storing a full ModelCube)

You can reconstruct `Ŷ = A·x` for diagnostics without materializing a full `/ModelCube`:

- **Reconstruction utilities** read `A` in the same chunk order and contract against `x` in λ-bands. They write a small target array you name (e.g., `/ReconTile`) or return the result.
- **Parallel spectral plots** pull just the selected rows from `/DataCube` and the reconstructed array and produce side-by-side data/model spectral plots for worst/best spaxels.

Both are tile-aligned and cache-aware; they never read `/HyperCube/models` beyond the needed slices.

---

## Fit tracking & snapshots

`fit_tracker.py` provides a **non-blocking** sidecar writer process:

- Destination: a sidecar H5 next to the main file (`*.fit.<pid>.<ts>.h5`).
- Bounded `mp.Queue` so the solver never blocks on I/O.
- SWMR enabled for dashboards.
- Streams **RMSE history**, **EWMA**, **progress**, and optional **x snapshots** (best/last/history).

Use `tracker=FitTracker(...)` in the solver; the runner wires this up for you.

---

## Utilities

- **Normalization conversion:** `convert_hypercube_norm(h5, to_mode="model"|"data", recompute_energy=True)` — chunk-aligned, mask-aware.
- **Global column energy:** created during build; can be recomputed later if you changed normalization.
- **Done-bitmap control:** functions to inspect and invalidate `_done` to resume or redo specific tiles.
- **Recompression:** rewrite `/HyperCube/models` with different compression/filter options (tile-wise copy).

---

## Environment variables (knobs)

These can be exported to tune performance, stability, or behavior. Defaults shown are what the code uses if you don’t set them.

### HDF5 & dataset caching

| Variable | Default | Purpose |
|---|---:|---|
| `HDF5_USE_FILE_LOCKING` | `FALSE` | Avoid HDF5 file locking issues on shared filesystems. |
| `CUBEFIT_RDCC_NSLOTS` | `400003` | File-level raw chunk cache: number of slots. |
| `CUBEFIT_RDCC_NBYTES` | `4294967296` (4 GiB) | File-level raw chunk cache size in bytes. |
| `CUBEFIT_RDCC_W0` | `0.9` | File-level raw chunk cache preemption policy. |
| `CUBEFIT_RDCC_SLOTS` | `1000003` | Per-dataset cache slots used by some readers. |
| `CUBEFIT_RDCC_BYTES` | `268435456` (256 MiB) | Per-dataset cache size used by some readers. |

### Multiprocessing & BLAS/OpenMP

| Variable | Default | Purpose |
|---|---:|---|
| `CUBEFIT_MP_CTX` | `forkserver` (fallback `spawn`) | MP start method for the solver pool. |
| `OMP_NUM_THREADS` | set by code | BLAS/OpenMP threads per worker (also `OPENBLAS_NUM_THREADS`, `MKL_NUM_THREADS`). |
| `KMP_INIT_AT_FORK` | `FALSE` | Avoid OpenMP deadlocks with `fork`. |
| `SLURM_CPUS_PER_TASK` | (if present) | Used by reconstruction helpers to set BLAS threads. |
| `PYTHONUNBUFFERED` | `1` (recommended) | Unbuffered stdout for real-time logs on HPC. |

### Solver: worker-level stability & steps (in `kaczmarz_solver_cchunk_mp.py`)

| Variable | Default | Purpose |
|---|---:|---|
| `CUBEFIT_BT_STEPS` | `3` | Backtracking steps per band (worker) for monotone weighted RMSE. |
| `CUBEFIT_BT_FACTOR` | `0.5` | Multiplicative α shrink in backtracking. |
| `CUBEFIT_TRUST_TAU` | `0.7` | Trust-region cap: `||ΔR_w|| ≤ τ ||R_w||`. |
| `CUBEFIT_EPS` | `1e-12` | Numerical floor in denominators and divisions. |
| `CUBEFIT_ZERO_COL_REL` | `1e-12` | Relative threshold (vs median energy) to **freeze** near-zero columns. |
| `CUBEFIT_ZERO_COL_ABS` | `1e-24` | Absolute threshold to **freeze** columns. |
| `CUBEFIT_DEBUG_SAFE` | `0` | Print sanitization/freeze stats per tile if `1`. |
| `CUBEFIT_NNLS_PROP_PER_BAND` | `6` | How many (c,p) candidates to propose for NNLS polish per band. |

### Solver: aggregation & NNLS polish (in `kaczmarz_solver_cchunk_mp.py`)

| Variable | Default | Purpose |
|---|---:|---|
| `CUBEFIT_TILE_BT_STEPS` | `6` | Backtracking steps for the **aggregated** ΔR across bands. |
| `CUBEFIT_TILE_BT_FACTOR` | `0.5` | Aggregated α shrink factor. |
| `CUBEFIT_GLOBAL_TAU` | `0.5` | Global step cap using `Σ dx²·E[c,p]` vs `||Y||`. |
| `CUBEFIT_GLOBAL_ENERGY_BLEND` | `1e-3` | Blend local `Σ(A_w)^2` with global column energy `E[c,p]` in denominators. |
| `CUBEFIT_NNLS_ENABLE` | `1` | Enable tile-local **weighted** NNLS polish. |
| `CUBEFIT_NNLS_EVERY` | `1` | Run NNLS polish every N tiles. |
| `CUBEFIT_NNLS_MAX_COLS` | `128` | Max columns K in the NNLS system. |
| `CUBEFIT_NNLS_MAX_BYTES` | `1000000000` | Approx memory cap for NNLS system (rows×8 per column). |
| `CUBEFIT_NNLS_SUB_L` | `0` | λ-subsample size for NNLS (0 → use all). |
| `CUBEFIT_NNLS_SOLVER` | `nnls` | One of `nnls`, `lsq`, `mu`, `fista` (FISTA recommended for speed/accuracy). |
| `CUBEFIT_NNLS_MAX_ITER` | `50` | Iteration cap for iterative NNLS (`mu`, `fista`, `lsq`’s TRF). |
| `CUBEFIT_NNLS_MIN_IMPROVE` | `0.999` | Required weighted-RMSE ratio to accept polish (< this accepts). |

### Solver: λ-weights (in `kaczmarz_solver_cchunk_mp.py` / `cube_utils.py`)

| Variable | Default | Purpose |
|---|---:|---|
| `CUBEFIT_LAMBDA_WEIGHTS_ENABLE` | `1` | Enable λ-weights for solver (`√w` weighting). |
| `CUBEFIT_LAMBDA_WEIGHTS_DSET` | `/HyperCube/lambda_weights` | Dataset to read. |
| `CUBEFIT_LAMBDA_MIN_W` | `1e-6` | Floor on λ-weights; protects √w. |
| `CUBEFIT_LAMBDA_WEIGHTS_AUTO` | `1` | Auto-generate weights if missing. |

### Fit tracking (in `fit_tracker.py`)

| Variable | Default | Purpose |
|---|---:|---|
| `FITTRACKER_START` | `spawn` (or best available) | Start method for the tracker process (`spawn`/`forkserver`/`fork`). |
| `CUBEFIT_TRACKER_QSIZE` | `8192` | Max queue size; drops messages when full if non-blocking. |
| `CUBEFIT_TRACKER_FLUSH_EVERY` | `128` | Flush to disk after this many messages. |
| `CUBEFIT_TRACKER_FLUSH_SEC` | `5.0` | Or flush if this many seconds have passed. |
| `CUBEFIT_RMSE_STRIDE` | `16` | Only enqueue every Nth batch RMSE sample (sub-sampling). |

---

## Typical end-to-end workflow

```python
# 1) Build HDF5 backbone & rebin operator
mgr = H5Manager("galaxy.h5", tem_pix=tem_loglam, obs_pix=obs_loglam)
mgr.init_base(H5Dims(nSpat=S, nLSpec=L, nTSpec=T, nVel=V, nComp=C, nPop=P))
mgr.populate_from_arrays(losvd=H_SVC, datacube=Y_SL, templates=T_PT)
mgr.ensure_rebin_and_resample()

# 2) Build hypercube
from CubeFit.hypercube_builder import build_hypercube
build_hypercube("galaxy.h5", norm_mode="data")  # or "model"

# (Optional) Flip normalization later without rebuild
from CubeFit.hypercube_builder import convert_hypercube_norm
convert_hypercube_norm("galaxy.h5", to_mode="model", recompute_energy=True)

# 3) Create λ-weights
from CubeFit.cube_utils import ensure_lambda_weights
ensure_lambda_weights("galaxy.h5")

# 4) Solve
from CubeFit.pipeline_runner import PipelineRunner
runner = PipelineRunner("galaxy.h5")
x, stats = runner.solve_all_mp_batched(
    epochs=6, lr=0.6, project_nonneg=True, processes=8, blas_threads=2,
    orbit_weights=orbit_w  # optional (C,)
)
```

---

## Notes on normalization & physics

- `norm="model"` keeps each `(s,c)` model’s scale proportional to the LOSVD amplitude — closest to “physical mass/flux” carried by LOSVD at that spaxel.
- `norm="data"` ties the per-spaxel model sum to the observed mean flux (masked), splitting across components in proportion to LOSVD amplitudes. This is convenient for direct data-scale comparisons but can blur absolute differences across datasets. You can switch modes post-hoc with `convert_hypercube_norm`.

The solver respects `/Mask` and λ-weights uniformly, so **absorption features** can dominate the fit when λ-weights are present (verify the printed min/max/mean at startup to ensure they are not all ones).

---

## Troubleshooting & sanity checks

- **Hypercube completeness:** use the `_done` bitmap helpers to inspect or reset tiles if needed.  
- **Normalization flips:** use `convert_hypercube_norm` (streams in S-tiles and handles `/Mask`); do **not** read the entire dataset into RAM.  
- **Line weights:** ensure `/HyperCube/lambda_weights` exists and spans a range (e.g., min ≪ 1, max ≈ 1); the solver prints min/max/mean.  
- **Population mixture looks uniform across components:**  
  - Confirm λ-weights are applied (banner shows they’re not all 1).  
  - Increase epochs modestly; keep `GLOBAL_ENERGY_BLEND` small but nonzero.  
  - Enable NNLS polish with a moderate `CUBEFIT_NNLS_MAX_COLS`, a λ-subsample (`CUBEFIT_NNLS_SUB_L`), and `CUBEFIT_NNLS_SOLVER=fista` for sharper per-tile updates.

---

## Where to look in the code

- **HDF5 backbone:** `hdf5_manager.py` (safe open, grids, rebin operator).
- **Builder:** `hypercube_builder.py` (FFT conv, normalization, col_energy, chunking + resume).
- **Solver:** `kaczmarz_solver_cchunk_mp.py` (MP loop, λ-weights, trust/backtracking, NNLS polish).
- **Runner:** `pipeline_runner.py` (orchestration, tracking, `/X_global`).
- **Tracking:** `fit_tracker.py` (sidecar writer, RMSE history, progress, snapshots).
- **Recon/plots:** utilities in `kz_fitSpec.py` (tile-aligned recon) and safe plotting helpers.

---

## Building the docs

Place this file at `docs/CubeFit.md` and run:

```bash
make            # builds docs/CubeFit.html and docs/CubeFit.pdf via pandoc
mkdocs serve    # live preview with Material theme (served from docs/)
```

Pandoc targets (from your Makefile):

- `docs/CubeFit.html` via:  
  `pandoc -s -f gfm -t html5 docs/CubeFit.md -o docs/CubeFit.html --metadata title="CubeFit" --toc`
- `docs/CubeFit.pdf` via XeLaTeX:  
  `pandoc -s -f gfm docs/CubeFit.md -o docs/CubeFit.pdf --pdf-engine=xelatex -V geometry:margin=1in -V mainfont="Latin Modern Roman" -V monofont="Latin Modern Mono" --toc`

MkDocs (from your `mkdocs.yml`):

```yaml
site_name: CubeFit
theme:
  name: material
nav:
  - Home: index.md
  - CubeFit: CubeFit.md
docs_dir: docs
```
</file>

<file path="hypercube_reader.py">
# -*- coding: utf-8 -*-
r"""
    hypercube_reader.py
    Adriano Poci
    University of Oxford
    2025

    Platforms
    ---------
    Unix, Windows

    Synopsis
    --------
    Manages Zarr data storage for CubeFit pipeline, including creation, loading,
    and validation of large, chunked arrays (templates, data cube, LOSVD, weights).
    Supports buffered template grids for safe convolution.

    Authors
    -------
    Adriano Poci <adriano.poci@physics.ox.ac.uk>

History
-------
v1.0:   Initial design and validation. 14 August 2025
v1.1:   Complete re-write to use HDF5. 7 September 2025

Hypercube reader that streams planes for Kaczmarz solving.

Contract with the solver:
  - read_spaxel_plane(s) -> (A, y)
      A: (N, L_eff) float32   with N = C*P
      y: (L_eff,)   float64   observed spectrum
  - Mask (if present at /Mask) is applied to BOTH A and y (True = keep)
"""
from __future__ import annotations

import os
from dataclasses import dataclass
from typing import Optional, Iterator, Tuple

import numpy as np

from CubeFit.hdf5_manager import open_h5

@dataclass
class ReaderCfg:
    s_tile: Optional[int] = None
    c_tile: Optional[int] = None
    p_tile: Optional[int] = None
    dtype_models: str = "float32"   # how to read models; math stays float64
    apply_mask: bool = True

class HyperCubeReader:
    """
    Minimal, robust reader for /HyperCube/models.
    Provides tiles and (s,c) planes as float32 (cast to float64 in solver).
    """

    def __init__(self, h5_path: str, cfg: ReaderCfg | None = None):
        self._ctx = open_h5(h5_path, "reader")   # keep the context manager
        self._f = self._ctx.__enter__()          # get the h5py.File
        self._closed = False

        f = self._f
        if "/HyperCube/models" not in f:
            roots = list(f.keys())
            try:
                hc = list(f["/"].keys())
            except Exception:
                hc = roots
            path = getattr(f, "filename", h5_path)
            raise RuntimeError(
                f"Missing /HyperCube/models in '{path}'. "
                f"Root groups present: {hc}. Did you build the same file?"
            )

        self._models = f["/HyperCube/models"]             # (S,C,P,L)
        self._done   = f["/HyperCube/_done"]              # (Sgrid,Cgrid,Pgrid)
        self._Y      = f["/DataCube"]                     # (S,L)
        self._Mask   = f["/Mask"] if "/Mask" in f else None

        self.nSpat, self.nComp, self.nPop, self.nLSpec = map(int, self._models.shape)
        self._chunks = self._models.chunks or (self.nSpat, 1, self.nPop, self.nLSpec)

        # config
        cfg = cfg or ReaderCfg()
        self._model_dtype = np.dtype(cfg.dtype_models).type
        self._apply_mask = bool(cfg.apply_mask)

        # default tile sizes fall back to storage chunks
        self.s_tile = int(cfg.s_tile or self._chunks[0])
        self.c_tile = int(cfg.c_tile or self._chunks[1])
        self.p_tile = int(cfg.p_tile or self._chunks[2])

        # cache mask as boolean (if present)
        self._mask = None
        if self._Mask is not None and self._apply_mask:
            self._mask = np.asarray(self._Mask[...], dtype=bool)

    # --------------------------------------------------------------------------

    @property
    def has_mask(self) -> bool:
        """True if /Mask exists in the file."""
        return self._Mask is not None

    @property
    def has_models(self) -> bool:
        """True if /HyperCube/models exists (reader requires it at init)."""
        try:
            return "/HyperCube/models" in self._f
        except Exception:
            return False

    @property
    def models_complete(self) -> bool:
        """True if /HyperCube.attrs['complete'] is set."""
        try:
            return bool(self._f["/HyperCube"].attrs.get("complete", False))
        except Exception:
            return False

    @property
    def models_path(self) -> str:
        """Path of the HDF5 file that contains the models (for logging)."""
        return str(getattr(self._f, "filename", ""))

    # Make the reader usable as a context manager and clean up on GC.
    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc, tb):
        self.close()

    def __del__(self):
        try:
            self.close()
        except Exception:
            pass

    # ----------------------------- lifecycle ---------------------------------

    def close(self):
        if not self._closed:
            try:
                self._ctx.__exit__(None, None, None)
            finally:
                self._closed = True

    # ----------------------------- helpers -----------------------------------

    def spaxel_tiles(self) -> Iterator[Tuple[int, int]]:
        """Yield (s0, s1) ranges across S using s_tile."""
        S = self.nSpat
        for s0 in range(0, S, self.s_tile):
            s1 = min(s0 + self.s_tile, S)
            yield s0, s1

    def _read_obs_row(self, s: int) -> np.ndarray:
        return np.asarray(self._Y[s, :], dtype=np.float64)

    # ----------------------------- public API --------------------------------

    def read_spaxel_plane(self, s: int) -> Tuple[np.ndarray, np.ndarray]:
        """
        Return (A, y) for a single spaxel:
            A: (N=LHS rows = C*P,   L columns) in dtype_models
            y: (L,) float64
        Mask (if present and apply_mask=True) is applied at the end.
        """
        if not (0 <= s < self.nSpat):
            raise IndexError(f"spaxel {s} out of bounds [0,{self.nSpat})")

        C, P, L = self.nComp, self.nPop, self.nLSpec
        slab = np.asarray(self._models[s, :, :, :], dtype=self._model_dtype, order="C")  # (C,P,L)
        A = slab.reshape(C * P, L)
        y = self._read_obs_row(s)

        if self._mask is not None:
            A = A[:, self._mask]
            y = y[self._mask]
        return A, y

    def read_spaxel_range(self, s0: int, s1: int, *, flatten: bool = True) -> Tuple[np.ndarray, np.ndarray]:
        """
        Return (A_slab, Y_slab) for s in [s0, s1):
            if flatten=True:
                A_slab: (ΔS, N=C*P, L) in dtype_models
                Y_slab: (ΔS, L)       in float64
            else:
                A_slab: (ΔS, C, P, L), Y_slab: (ΔS, L)
        """
        if not (0 <= s0 < s1 <= self.nSpat):
            raise IndexError(f"spaxel range [{s0},{s1}) invalid for S={self.nSpat}")

        C, P, L = self.nComp, self.nPop, self.nLSpec
        dS = s1 - s0

        # models slab (ΔS, C, P, L)
        A = np.empty((dS, C, P, L), dtype=self._model_dtype)
        for k, s in enumerate(range(s0, s1)):
            A[k, :, :, :] = np.asarray(self._models[s, :, :, :], dtype=self._model_dtype, order="C")

        # observed slab (ΔS, L)
        Y = np.empty((dS, L), dtype=np.float64)
        for k, s in enumerate(range(s0, s1)):
            Y[k, :] = self._read_obs_row(s)

        if self._mask is not None:
            A = A[..., self._mask]
            Y = Y[..., self._mask]

        if flatten:
            return A.reshape(dS, C * P, A.shape[-1]), Y
        else:
            return A, Y
</file>

<file path="kz_0All.sh">
#!/bin/bash -l

module purge
module load foss/2023a
module load Python/3.11.3-GCCcore-12.3.0
# glibc / allocator hygiene
export MALLOC_ARENA_MAX=2

# Threading (OpenBLAS-backed NumPy)
export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-48}
export OPENBLAS_NUM_THREADS=$OMP_NUM_THREADS
export OMP_DYNAMIC=FALSE
export OMP_PROC_BIND=false
unset GOMP_CPU_AFFINITY
export KMP_AFFINITY=disabled
export MKL_NUM_THREADS=1            # harmless, prevents surprise MKL use elsewhere
export NUMEXPR_NUM_THREADS=1        # avoid hidden extra threads

# File descriptors
ulimit -n 8192

# HDF5 raw chunk cache (tune if memory pressure)
export CUBEFIT_RDCC_NBYTES=$((16*1024*1024*1024))  # 8–16 GiB are sane
export CUBEFIT_RDCC_NSLOTS=400003
export CUBEFIT_RDCC_W0=0.9

cd /data/phys-gal-dynamics/phys2603/CubeFit
# declare fSGA=$(sbatch --parsable "kz_1Gen.sh")
# declare fSF=$(sbatch --parsable --dependency=afterok:"${fSGA}" "kz_2Fit.sh")
declare fSF=$(sbatch --parsable "kz_2Fit.sh")
sbatch --dependency=afterok:"${fSF}" "kz_3Rio.sh"
</file>

<file path="kz_1Gen.sh">
#!/bin/bash -l
# #SBATCH -A durham
# #SBATCH -D "/cosma5/data/durham/dc-poci1/muse"
# #SBATCH --output="/cosma5/data/durham/dc-poci1/muse/slurm_tri_losvdDecomp.log" --open-mode=append
# #SBATCH --error="/cosma5/data/durham/dc-poci1/muse/slurm_tri_losvdDecomp.log" --open-mode=append

# #SBATCH -A oz059
# #SBATCH -D "/fred/oz059/poci/muse"
# #SBATCH --output="/fred/oz059/poci/muse/slurm_tri_losvdDecomp.log" --open-mode=append
# #SBATCH --error="/fred/oz059/poci/muse/slurm_tri_losvdDecomp.log" --open-mode=append

#SBATCH -D "/data/phys-gal-dynamics/phys2603/muse"
#SBATCH --output="/data/phys-gal-dynamics/phys2603/CubeFit/log_1Gen.log" --open-mode=append
#SBATCH --error="/data/phys-gal-dynamics/phys2603/CubeFit/log_1Gen.log" --open-mode=append
#SBATCH -p short

#SBATCH --job-name="CubeFit_1Gen"
#SBATCH --time=0-12:00
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=12
#SBATCH --mem=150G
#SBATCH --mail-type=ALL
#SBATCH --mail-user=adriano.poci@physics.ox.ac.uk
#SBATCH --hint=nomultithread
#SBATCH --exclusive

module purge
module load foss/2023a
module load Python/3.11.3-GCCcore-12.3.0
# glibc / allocator hygiene
export MALLOC_ARENA_MAX=2

# Threading (OpenBLAS-backed NumPy)
export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-48}
export OPENBLAS_NUM_THREADS=$OMP_NUM_THREADS
export OMP_DYNAMIC=FALSE
export OMP_PROC_BIND=false
unset GOMP_CPU_AFFINITY
export KMP_AFFINITY=disabled
export MKL_NUM_THREADS=1            # harmless, prevents surprise MKL use elsewhere
export NUMEXPR_NUM_THREADS=1        # avoid hidden extra threads

# File descriptors
ulimit -n 8192

# HDF5 raw chunk cache (tune if memory pressure)
export CUBEFIT_RDCC_NBYTES=$((16*1024*1024*1024))  # 8–16 GiB are sane
export CUBEFIT_RDCC_NSLOTS=400003
export CUBEFIT_RDCC_W0=0.9

cd /data/phys-gal-dynamics/phys2603/CubeFit
# run your job as a Slurm step (gives you the full cpuset)
srun -n1 -c${SLURM_CPUS_PER_TASK} --cpu-bind=cores \
  python -m IPython kz_run.py -- --run-switch 'gen' --redraw
</file>

<file path="kz_3Rio.sh">
#!/bin/bash -l
# #SBATCH -A durham
# #SBATCH -D "/cosma5/data/durham/dc-poci1/muse"
# #SBATCH --output="/cosma5/data/durham/dc-poci1/muse/slurm_tri_losvdDecomp.log" --open-mode=append
# #SBATCH --error="/cosma5/data/durham/dc-poci1/muse/slurm_tri_losvdDecomp.log" --open-mode=append

# #SBATCH -A oz059
# #SBATCH -D "/fred/oz059/poci/muse"
# #SBATCH --output="/fred/oz059/poci/muse/slurm_tri_losvdDecomp.log" --open-mode=append
# #SBATCH --error="/fred/oz059/poci/muse/slurm_tri_losvdDecomp.log" --open-mode=append

#SBATCH -D "/data/phys-gal-dynamics/phys2603/muse"
#SBATCH --output="/data/phys-gal-dynamics/phys2603/CubeFit/log_3Rio.log" --open-mode=append
#SBATCH --error="/data/phys-gal-dynamics/phys2603/CubeFit/log_3Rio.log" --open-mode=append
#SBATCH -p short

#SBATCH --job-name="CubeFit_3Rio"
#SBATCH --time=0-12:00
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=12
#SBATCH --mem=150G
#SBATCH --hint=nomultithread
#SBATCH --exclusive
#SBATCH --mail-type=ALL
#SBATCH --mail-user=adriano.poci@physics.ox.ac.uk

module purge
module load foss/2023a
module load Python/3.11.3-GCCcore-12.3.0

# glibc / allocator hygiene
export MALLOC_ARENA_MAX=2

# Threading (OpenBLAS-backed NumPy)
export OPENBLAS_NUM_THREADS=${SLURM_CPUS_PER_TASK}
export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}      # harmless with OpenBLAS(pthreads)
export OMP_DYNAMIC=FALSE
export OMP_PROC_BIND=false
unset GOMP_CPU_AFFINITY
export KMP_AFFINITY=disabled
export MKL_NUM_THREADS=1
export NUMEXPR_NUM_THREADS=1

# HDF5 raw chunk cache (4 GiB is plenty; bump if you like)
export CUBEFIT_RDCC_NBYTES=$((4*1024*1024*1024))  # 4 GiB
export CUBEFIT_RDCC_NSLOTS=400003
export CUBEFIT_RDCC_W0=0.9

# FitTracker tweaks (reduce sidecar churn; enforce spawn)
export CUBEFIT_RMSE_STRIDE=16
export CUBEFIT_TRACKER_QSIZE=8192
export FITTRACKER_START=spawn

# File descriptors
ulimit -n 8192

cd /data/phys-gal-dynamics/phys2603/CubeFit

# sanity print (once) to confirm cpuset and BLAS threads
srun -n1 -c${SLURM_CPUS_PER_TASK} --cpu-bind=cores \
  python - <<'PY'
import os, json
print(f"[sanity] cpuset cores: {len(os.sched_getaffinity(0))}")
try:
    from threadpoolctl import threadpool_info
    print("[sanity] BLAS pools:", json.dumps(threadpool_info(), indent=2)[:600], "...")
except Exception as e:
    print("[sanity] threadpoolctl not available:", e)
PY

# run your job as a Slurm step (gives you the full cpuset)
srun -n1 -c${SLURM_CPUS_PER_TASK} --cpu-bind=cores \
  python -m IPython kz_rio.py -- --redraw
</file>

<file path="logger.py">
# -*- coding: utf-8 -*-
r"""
    logger.py
    Adriano Poci
    University of Oxford
    2025

    Synopsis
    --------
    Provides a harmonized, central logging interface for all CubeFit modules.
    Can be used as a context manager to capture all output in a logfile.

    Authors
    -------
    Adriano Poci <adriano.poci@physics.ox.ac.uk>
"""

import sys
import threading
import datetime
from pathlib import Path
import traceback
from contextlib import contextmanager, redirect_stdout, redirect_stderr

class CubeFitLogger:
    _instance = None
    _lock = threading.Lock()

    def __new__(cls, logfile=None, mode="a"):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance.init(logfile, mode)
            else:
                if logfile is not None and cls._instance.logfile != Path(logfile):
                    print(f"[CubeFitLogger WARNING] Attempt to reinitialize "
                          f"logger with {logfile}, but already initialized "
                          f"with {cls._instance.logfile}. Ignoring new logfile!", file=sys.__stdout__)
            return cls._instance

    def init(self, logfile=None, mode="a"):
        if logfile is not None:
            logfile = Path(logfile)
            logfile.parent.mkdir(parents=True, exist_ok=True)
            if mode == "w":
                logfile.write_text("")  # clear the file
        self.logfile = Path(logfile) if logfile is not None else None
        self.mode = mode

    def log(self, msg="", flush=True, ts=True):
        # accept any object (including Exceptions) and stringify safely
        try:
            text = str(msg)
        except Exception:
            text = repr(msg)
        now = datetime.datetime.now().strftime("[%Y-%m-%d %H:%M:%S]") if ts else ""
        outmsg = (now + " " + text) if ts else text
        with threading.Lock():
            print(outmsg, file=sys.__stdout__, flush=flush)
            if self.logfile is not None:
                with self.logfile.open("a") as f:
                    f.write(outmsg + "\n")
            if flush:
                sys.__stdout__.flush()

    def log_exc(self, exc: BaseException, prefix="[EXC]"):
        tb = "".join(traceback.format_exception(type(exc), exc, exc.__traceback__))
        self.log(f"{prefix} {tb}", flush=True)

    def flush(self):
        sys.__stdout__.flush()
        if self.logfile is not None:
            with self.logfile.open("a"):
                pass

    @contextmanager
    def capture_all_output(self):
        original_excepthook = sys.excepthook

        def log_excepthook(exc_type, exc_value, exc_traceback):
            exception_msg = ''.join(
                traceback.format_exception(exc_type, exc_value, exc_traceback)
            )
            self.log("[Exception Captured]\n" + exception_msg, flush=True)
            # Optionally call original excepthook too
            original_excepthook(exc_type, exc_value, exc_traceback)

        sys.excepthook = log_excepthook

        class LoggerStream:
            def __init__(self, logger):
                self.logger = logger

            def write(self, message):
                message = message.rstrip()
                if message:
                    self.logger.log(message, flush=True, ts=True)

            def flush(self):
                pass  # Implement if needed

        try:
            with redirect_stdout(LoggerStream(self)), redirect_stderr(LoggerStream(self)):
                yield
        finally:
            sys.excepthook = original_excepthook

# Singleton access
_logger_instance = None
_logger_lock = threading.Lock()

def get_logger(logfile=None, mode="a"):
    global _logger_instance
    with _logger_lock:
        if _logger_instance is None:
            _logger_instance = CubeFitLogger(logfile, mode)
        return _logger_instance
</file>

<file path="Makefile">
DOC=docs/CubeFit.md
HTML=docs/CubeFit.html
PDF=docs/CubeFit.pdf

all: html pdf

html:
	pandoc -s -f gfm -t html5 $(DOC) -o $(HTML) --metadata title="CubeFit" --toc

pdf:
	pandoc -s -f gfm $(DOC) -o $(PDF) --pdf-engine=xelatex \
	  -V geometry:margin=1in -V mainfont="Open Sans" \
	  -V monofont="IntoneMono Nerd Font Mono" --toc
</file>

<file path="README.md">
# CubeFit

CubeFit is a high-throughput pipeline for orbital spectral–fitting on IFU data cubes.  
It builds a **HyperCube** of convolved template populations, then solves for global
population weights with a parallel **Kaczmarz** solver, including sidecar tracking,
a live dashboard, and memory-safe reconstruction/plotting utilities.

- Fast, chunk-aligned HDF5 I/O (SWMR-friendly)
- On-the-fly or cached λ-weights and global column energy
- Robust multi-process Kaczmarz with trust-region, backtracking, optional NNLS polish
- Fit sidecar writer for live progress & EWMA metrics
- Reconstruction of `ModelCube` without materializing the full basis
- Lightweight plotting (best/worst residual spectra) and diagnostics

> **Full documentation** → [`docs/CubeFit.md`](docs/CubeFit.md)  
> **Environment variables (all knobs)** → [`docs/CubeFit.md#environment-variables`](docs/CubeFit.md#environment-variables)

---

## Installation

Requirements: Python ≥ 3.10, NumPy, SciPy, h5py, matplotlib, tqdm, threadpoolctl, (optional) Cython.

```bash
# (optional) create an environment
python -m venv .venv && source .venv/bin/activate

# install package & deps
pip install -r requirements.txt  # if present
pip install -e .
```

If your build includes Cython extensions (e.g., continuum detrending), ensure a C compiler
is available; `pip install -e .` will compile them in place.

---

## Quickstart

Below is a minimal end-to-end sketch. See the manual for full options and data prep.

```python
from CubeFit.hypercube_builder import build_hypercube
from CubeFit.pipeline_runner import PipelineRunner

H5 = "CubeFit/NGC4365/NGC4365_04.h5"

# 1) Build the HyperCube (choose normalization you’ll use in fitting)
build_hypercube(
    H5,
    norm_mode="model",   # or "data" (see docs on normalization)
    amp_mode="sum"       # or "trapz" (requires /VelPix)
)

# 2) Solve (multi-process Kaczmarz); see docs for all knobs/args
pr = PipelineRunner(H5)
x_global, stats = pr.solve_all_mp_batched(
    epochs=3,
    pixels_per_aperture=1247,
    lr=0.9,
    project_nonneg=True,
    processes=8,
    blas_threads=8,
)

# 3) Reconstruct the model cube for diagnostics/plots
from CubeFit.kz_fitSpec import reconstruct_model_cube_single
reconstruct_model_cube_single(h5_path=H5, x_global=x_global, array_name="ModelCube", blas_threads=8)
```

**Tips**
- If your HyperCube was built in the other normalization, you can convert in-place
  (streaming) without a full rebuild. See **Normalization utilities** in the manual.
- λ-weights (feature emphasis) and global column energy are read/cached automatically if present.

---

## Building the docs

This repo ships a full manual at `docs/CubeFit.md`. You can build HTML/PDF with your Makefile:

```bash
make html
make pdf
```

MkDocs config (`mkdocs.yml`) already points navigation to `docs/CubeFit.md`.

---

## Where to next?

- **Full reference & walkthrough**: [`docs/CubeFit.md`](docs/CubeFit.md)  
- **All environment variables** (solver, NNLS, tracking, mp, etc.):
  [`docs/CubeFit.md#environment-variables`](docs/CubeFit.md#environment-variables)

---

## License

See `LICENSE` (if present). If you plan to publish results, please cite the repository and any underlying data/model libraries you use (e.g., MILES/eMILES).
</file>

<file path="cube_debug.py">
# -*- coding: utf-8 -*-
r"""
    cube_debug.py
    Adriano Poci
    University of Oxford
    2025

    Platforms
    ---------
    Unix, Windows

    Synopsis
    --------
    A collection of helpful functionality to assist in debugging aspects of the CubeFit pipeline.

    Authors
    -------
    Adriano Poci <adriano.poci@physics.ox.ac.uk>

History
-------
v1.0:   Added `nnls_seed_diagnostics` to investigate how the NNLS seed behaves
            as a full solution. 13 December 2025
"""
import time, os
import numpy as np
from pathlib import Path
from typing import Optional, Sequence

from CubeFit.hdf5_manager import open_h5
from CubeFit.hypercube_builder import read_global_column_energy
from CubeFit.cube_utils import (
    project_to_component_weights,
    # project_to_component_weights_strict,
)
from CubeFit.live_fit_dashboard import (
    render_aperture_fits_with_x,
    render_sfh_from_x,
)

# ------------------------------------------------------------------------------

def _read_C_P(f) -> tuple[int, int]:
    M = f["/HyperCube/models"]
    _, C, P, _ = map(int, M.shape)
    return C, P

def _row_or_vec_to_CP(arr: np.ndarray, C: int, P: int) -> np.ndarray:
    """Map various X layouts to a (C,P) array."""
    X = np.asarray(arr, np.float64)
    if X.ndim == 2 and X.shape == (C, P):
        return X.copy()
    if X.ndim == 2 and X.shape == (P, C):
        return X.T.copy()
    v = X.ravel(order="C")
    if v.size != C * P:
        raise ValueError(
            f"Cannot reshape X of size {v.size} to (C,P)=({C},{P})."
        )
    return v.reshape(C, P, order="C").copy()

def _read_orbit_weights(f) -> np.ndarray:
    # Same preference order you use elsewhere
    if "/Fit/orbit_weights" in f:
        w = np.asarray(f["/Fit/orbit_weights"][...], np.float64)
    elif "/CompWeights" in f:
        w = np.asarray(f["/CompWeights"][...], np.float64)
    else:
        raise RuntimeError("No orbital weights found (/Fit/orbit_weights or /CompWeights).")
    return np.nan_to_num(w, nan=0.0, posinf=0.0, neginf=0.0)

def _read_X(h5_path: str,
            x_dset: str | None,
            C: int,
            P: int) -> np.ndarray:
    """
    Read a real solution / seed vector and return it as (C,P).

    x_dset=None → use your usual preference order.
    """
    with open_h5(h5_path, role="reader", swmr=True) as f:
        if x_dset is not None:
            if x_dset not in f:
                raise RuntimeError(f"Requested x_dset '{x_dset}' not found.")
            X_raw = np.asarray(f[x_dset][...], np.float64)
        else:
            # same search order as compare_usage_to_orbit_weights, but main-file only
            for name in ("/X_global",
                         "/Fit/x_latest",
                         "/Seeds/x0_nnls_patch",
                         "/Fit/x_best",
                         "/Fit/x_last",
                         "/Fit/x_epoch_last"):
                if name in f:
                    X_raw = np.asarray(f[name][...], np.float64)
                    break
            else:
                raise RuntimeError("No X dataset found in main HDF5.")
    return _row_or_vec_to_CP(X_raw, C, P)

def _usage(X: np.ndarray,
           E: np.ndarray | None = None) -> tuple[np.ndarray, np.ndarray]:
    """
    Return (raw_usage, normalized_usage) per component.
    If E is provided, usage is energy-weighted: s_c = sum_p X[c,p]*E[c,p].
    """
    X64 = np.asarray(X, np.float64)
    if E is None:
        s = X64.sum(axis=1)
    else:
        E64 = np.asarray(E, np.float64)
        s = (X64 * E64).sum(axis=1)
    s = np.nan_to_num(s, nan=0.0, posinf=0.0, neginf=0.0)
    s = np.maximum(s, 0.0)
    S = float(s.sum() or 1.0)
    return s, s / S

def debug_test_projectors_on_h5(
    h5_path: str,
    x_dset: str | None = None,
    *,
    use_energy_metric: bool = True,
) -> None:
    """
    Test project_to_component_weights and project_to_component_weights_strict
    on *real* data from an HDF5, as close to runtime as possible.
    """
    with open_h5(h5_path, role="reader", swmr=True) as f:
        C, P = _read_C_P(f)
        w_raw = _read_orbit_weights(f)

    # Reduce orbit_weights to (C,) if needed (C*P -> sum over P)
    w_vec = np.asarray(w_raw, np.float64).ravel(order="C")
    if w_vec.size == C:
        w_c = w_vec.copy()
    elif w_vec.size == C * P:
        w_c = w_vec.reshape(C, P, order="C").sum(axis=1)
    else:
        raise ValueError(
            f"orbit_weights length {w_vec.size} incompatible with C={C}, C*P={C*P}."
        )
    w_c = np.maximum(np.nan_to_num(w_c, nan=0.0, posinf=0.0, neginf=0.0), 0.0)
    Wsum = float(w_c.sum() or 1.0)
    w_fracs = w_c / Wsum

    # Real X_cp from file
    X0 = _read_X(h5_path, x_dset=x_dset, C=C, P=P)
    # Real global column energy (same as softbox/usage)
    E_cp = read_global_column_energy(h5_path)

    print(f"[debug] h5={h5_path}")
    print(f"[debug] shapes: X0={X0.shape}, E_cp={E_cp.shape}, w_c={w_c.shape}")

    # Baseline usage
    s_plain0, u_plain0 = _usage(X0, E=None)
    s_energy0, u_energy0 = _usage(X0, E_cp if use_energy_metric else None)

    def _report(label: str, u: np.ndarray) -> None:
        diff = u - w_fracs
        l1 = float(np.sum(np.abs(diff)))
        linf = float(np.max(np.abs(diff)))
        print(f"[{label}] L1={l1:.3e}  L∞={linf:.3e}")

    print("\n[baseline] before any projection:")
    _report("plain-usage", u_plain0)
    _report("energy-usage", u_energy0)
    print("  mass_plain =", float(s_plain0.sum()))
    print("  mass_energy =", float(s_energy0.sum()))

    # ---------- inter-epoch projector (gentle) ----------
    X1 = X0.copy()
    t0 = time.perf_counter()
    project_to_component_weights(
        X1,
        t_vec=w_c,                # same target as runtime
        E_cp=(E_cp if use_energy_metric else None),
        minw=1e-12,
    )
    dt1 = time.perf_counter() - t0

    s_plain1, u_plain1 = _usage(X1, E=None)
    s_energy1, u_energy1 = _usage(X1, E_cp if use_energy_metric else None)

    print("\n[proj] after project_to_component_weights:")
    print(f"  runtime = {dt1:.4e} s")
    print("  finite X1? ", np.isfinite(X1).all(),
          "  min(X1) =", float(np.nanmin(X1)))
    print("  mass_plain  (before/after) =",
          float(s_plain0.sum()), "→", float(s_plain1.sum()))
    print("  mass_energy (before/after) =",
          float(s_energy0.sum()), "→", float(s_energy1.sum()))
    _report("plain-usage", u_plain1)
    _report("energy-usage", u_energy1)

    # # ---------- strict projector (hard constraint, post-epoch) ----------
    # X2 = X0.copy()
    # t0 = time.perf_counter()
    # project_to_component_weights_strict(
    #     X2,
    #     orbit_weights=w_c,        # or full (C*P,) if you want; fn should handle both
    #     E_cp=(E_cp if use_energy_metric else None),
    #     min_target=1e-12,
    # )
    # dt2 = time.perf_counter() - t0

    # s_plain2, u_plain2 = _usage(X2, E=None)
    # s_energy2, u_energy2 = _usage(X2, E_cp if use_energy_metric else None)

    # print("\n[strict] after project_to_component_weights_strict:")
    # print(f"  runtime = {dt2:.4e} s")
    # print("  finite X2? ", np.isfinite(X2).all(),
    #       "  min(X2) =", float(np.nanmin(X2)))
    # print("  mass_plain  (before/after) =",
    #       float(s_plain0.sum()), "→", float(s_plain2.sum()))
    # print("  mass_energy (before/after) =",
    #       float(s_energy0.sum()), "→", float(s_energy2.sum()))
    # _report("plain-usage", u_plain2)
    # _report("energy-usage", u_energy2)

    print("\n[debug] done.\n")

# ------------------------------------------------------------------------------

def nnls_seed_diagnostics(
    h5_path: str,
    *,
    seed_path: Optional[str] = None,
    n_apertures: int = 6,
    apertures: Optional[Sequence[int]] = None,
    show_residual: bool = True,
) -> dict:
    """
    Treat the NNLS seed as the final solution and render standard diagnostics.

    This is a convenience wrapper around `render_aperture_fits_with_x` and
    `render_sfh_from_x` that:
      - reads the seed vector X from the main HDF5 file,
      - flattens it to (C*P,) in the runtime layout,
      - renders a small set of representative aperture fits, and
      - renders the orbital SFH panel.

    Parameters
    ----------
    h5_path : str
        Path to the main CubeFit HDF5 file.
    seed_path : str, optional
        Dataset to use for the seed X. Defaults to the environment
        variable CUBEFIT_SEED_PATH if set, otherwise "/Seeds/x0_nnls_patch".
    n_apertures : int, optional
        Number of apertures to sample uniformly across the field if
        `apertures` is not provided. Default is 6.
    apertures : sequence of int, optional
        Explicit list of aperture indices to plot. If provided, this
        overrides `n_apertures`.
    show_residual : bool, optional
        Whether to overlay residuals in the aperture-fit panel.

    Returns
    -------
    info : dict
        Dictionary with keys:
          - "seed_path" : str
          - "apertures" : list[int]
          - "fits_png" : str
          - "sfh_png" : str
    """

    if seed_path is None:
        seed_path = os.environ.get(
            "CUBEFIT_SEED_PATH", "/Seeds/x0_nnls_patch"
        )

    # Read geometry and seed vector as (C,P)
    with open_h5(h5_path, role="reader", swmr=True) as f:
        C, P = _read_C_P(f)
        S = int(f["/DataCube"].shape[0])

    X_cp = _read_X(h5_path, x_dset=seed_path, C=C, P=P)
    x_flat = np.asarray(X_cp, dtype=np.float64).ravel(order="C")
    x_flat = np.nan_to_num(
        x_flat, copy=False, nan=0.0, posinf=0.0, neginf=0.0
    )

    # Choose apertures to plot
    if apertures is not None:
        ap_idx = [int(i) for i in apertures if 0 <= int(i) < S]
    else:
        n_plot = int(min(max(1, n_apertures), S))
        ap_idx = np.linspace(0, S - 1, n_plot, dtype=int).tolist()

    base = Path(h5_path).parent / "figures"
    base.mkdir(parents=True, exist_ok=True)

    fits_png = base / "nnls_seed_fits.png"
    sfh_png = base / "nnls_seed_sfh.png"

    render_sfh_from_x(
        h5_path,
        x_flat,
        sfh_png,
    )
    
    if ap_idx:
        render_aperture_fits_with_x(
            h5_path,
            x_flat,
            fits_png,
            apertures=ap_idx,
            show_residual=show_residual,
            title="NNLS seed treated as final solution",
        )


    return {
        "seed_path": str(seed_path),
        "apertures": ap_idx,
        "fits_png": str(fits_png),
        "sfh_png": str(sfh_png),
    }

# ------------------------------------------------------------------------------
</file>

<file path="kz_initNGC4365.py">
"""
#!/apps/skylake/software/core/anaconda3/5.1.0/bin/python3
#SBATCH -A oz059
#SBATCH --job-name="slurmSpecNGC4365"
#SBATCH --time=2-00:00
#SBATCH -D "/fred/oz059/poci/muse"
#SBATCH --output="/fred/oz059/poci/muse/slurmSpecNGC4365.log"
#SBATCH --error="/fred/oz059/poci/muse/slurmSpecNGC4365.log"
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32
#SBATCH --mem=200GB
#SBATCH --mail-type=ALL
#SBATCH --mail-user=adriano.poci@students.mq.edu.au

    slurmSpecNGC4365.py
    Adriano Poci
    Durham University
    2021

    Platforms
    ---------
    Unix, Windows

    Synopsis
    --------
    This module executes some function in the `SLURM` queueing environment

    Authors
    -------
    Adriano Poci <adriano.poci@durham.ac.uk>

History
-------
v1.0:	12 November 2021
"""

# from site import addsitedir as sas
# import pathlib as plp
# sas(str(plp.Path(plp.os.sep, 'fred', 'oz059', 'poci')))
# sas(str(plp.Path(plp.os.sep, 'fred', 'oz059', 'poci', 'dynamics')))
# sas(str(plp.Path(plp.os.sep, 'fred', 'oz059', 'poci', 'pxf')))
# sas(str(plp.Path(plp.os.sep, 'fred', 'oz059', 'poci', 'muse')))
# do not need to add to paths, if run with
#   mpiexec -usize <nProcs+1> -n 1 ipython slurmSpecFCC170.py

# props = dict(galaxy='NGC4365', mPath='hdhdc4365', SN=100, nCuts=393, lOrder=7,
#     specRange=[5100, 5950], full=True, lsf=True, iso='BaSTI', nProcs=1,
#     band='F814W', smask=[[5550, 5560]], genSwitch=None)
# props = dict(galaxy='NGC4365', mPath='hdhdc4365', SN=100, nCuts=207, lOrder=12,
props = dict(galaxy='NGC4365', mPath='hdhdc4365', SN=100, nCuts=3, lOrder=12,
    specRange=[5100, 6650], full=True, lsf=True, iso='BaSTI', nProcs=1,
    band='F814W', genSwitch=None, kind='SMILES', cont=True,
    smask=[[5530, 5555], [6255, 6270], [6320, 6335], [7580, 7700], [8775, 9000]],
    lam=1e-12)
</file>

<file path="kz_rio.py">
"""
#!/apps/skylake/software/core/anaconda3/5.1.0/bin/python3
#SBATCH -A oz059
#SBATCH --job-name="slurmSpecNGC4365"
#SBATCH --time=2-00:00
#SBATCH -D "/fred/oz059/poci/muse"
#SBATCH --output="/fred/oz059/poci/muse/slurmSpecNGC4365.log"
#SBATCH --error="/fred/oz059/poci/muse/slurmSpecNGC4365.log"
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32
#SBATCH --mem=200GB
#SBATCH --mail-type=ALL
#SBATCH --mail-user=adriano.poci@students.mq.edu.au

    slurmSpecNGC4365.py
    Adriano Poci
    Durham University
    2021

    Platforms
    ---------
    Unix, Windows

    Synopsis
    --------
    This module executes some function in the `SLURM` queueing environment

    Authors
    -------
    Adriano Poci <adriano.poci@durham.ac.uk>

History
-------
v1.0:	12 November 2021
v1.1:   Capture exceptions around `loadCubeFit` call. 4 December 2025
"""

# from site import addsitedir as sas
# import pathlib as plp
# sas(str(plp.Path(plp.os.sep, 'fred', 'oz059', 'poci')))
# sas(str(plp.Path(plp.os.sep, 'fred', 'oz059', 'poci', 'dynamics')))
# sas(str(plp.Path(plp.os.sep, 'fred', 'oz059', 'poci', 'pxf')))
# sas(str(plp.Path(plp.os.sep, 'fred', 'oz059', 'poci', 'muse')))
# do not need to add to paths, if run with
#   mpiexec -usize <nProcs+1> -n 1 ipython slurmSpecFCC170.py

import os
t = os.environ.get("SLURM_CPUS_PER_TASK", "12")
os.environ["OMP_NUM_THREADS"]      = t
os.environ["MKL_NUM_THREADS"]      = t
os.environ["OPENBLAS_NUM_THREADS"] = t
os.environ["OMP_DYNAMIC"]          = "FALSE"
os.environ["MKL_DYNAMIC"]          = "FALSE"

import numpy as np
import re, sys
import pathlib as plp
import argparse

# Custom modules
from CubeFit.kz_fitSpec import loadCubeFit
from CubeFit.kz_initNGC4365 import props

def main():
    ap = argparse.ArgumentParser(description="Thin wrapper around genCubeFit")
    # boolean redraw with explicit on/off flags
    group = ap.add_mutually_exclusive_group()
    group.add_argument("--redraw", dest="redraw", action="store_true",
                       help="Enable redraw mode")
    group.add_argument("--no-redraw", dest="redraw", action="store_false",
                       help="Disable redraw mode")
    ap.set_defaults(redraw=False)

    args = ap.parse_args()

    # Detect CPUs
    slurm_cpu = os.environ.get('SLURM_CPUS_PER_TASK')
    if slurm_cpu is not None:
        nCPU = int(slurm_cpu)
    else:
        curdir = plp.Path(__file__).parent
        try:
            with open(curdir/'kz_addqueue.sh') as f:
                content = f.read()
            m = re.search(r'^\s*nCPU\s*=\s*(\d+)', content, re.MULTILINE)
            nCPU = int(m.group(1)) if m else 20
        except FileNotFoundError:
            nCPU = 20

    print(f"Setting nCPU to {nCPU} from SLURM_CPUS_PER_TASK or kz_addqueue.sh")
    props['nProcs'] = nCPU

    # Pass-through args
    props['redraw'] = bool(args.redraw)
    print(f"redraw = {props['redraw']}")

    try:
        loadCubeFit(**props, pplots=['spec', 'mw'])
    except SystemExit:
        # Let explicit sys.exit()s behave normally
        raise
    except BaseException as e:
        # Log + print the traceback explicitly
        import traceback
        print("[kz_rio] FATAL: unhandled exception in genCubeFit", file=sys.__stderr__, flush=True)
        traceback.print_exc()
        # This *forces* the interpreter to exit, even under IPython
        sys.exit(1)

if __name__ == "__main__":
    main()
</file>

<file path="fit_tracker.py">
# -*- coding: utf-8 -*-
"""
fit_tracker.py

Sidecar-based, non-blocking fit tracker for live Kaczmarz runs.

- Writes ONLY to <main>.fit.<pid>.<ts>.h5 (a sidecar), never the main HDF5.
- Uses a bounded mp.Queue so the solver never blocks on I/O.
- No SWMR; no file locking on the main file.
"""

from __future__ import annotations
import queue as _queue
import os, time, math, json, multiprocessing as mp
from multiprocessing.queues import Queue as MPQueue  # put near other imports
from dataclasses import dataclass
from typing import Optional
import numpy as np
import h5py

# for lock-clear + detection
from CubeFit.hdf5_manager import _h5clear, _looks_like_lock_error
import CubeFit.cube_utils as cu
from CubeFit.logger import get_logger

logger = get_logger()

# ---------------------------- configuration ----------------------------------

@dataclass
class TrackerConfig:
    ring_size: int = 96
    metrics_interval_sec: float = 300.0
    diag_seed: int = 12345

# ------------------------------ writer proc -----------------------------------

def _writer_main(h5_path: str, cfg: TrackerConfig, rx: MPQueue) -> None:
    # Resolve/construct a sidecar path...
    sidecar = cu._find_latest_sidecar(h5_path)
    if not sidecar:
        sidecar = cu._default_sidecar_path(h5_path)
    if not sidecar:
        base = str(h5_path) if h5_path else "cube"
        sidecar = f"{base}.fit.{os.getpid()}.{int(time.time())}.h5"

    if os.environ.get("HDF5_USE_FILE_LOCKING") is None:
        os.environ["HDF5_USE_FILE_LOCKING"] = "FALSE"

    # lock-aware open
    retries, backoff, last_exc = 3, 0.4, None
    f = None
    for attempt in range(retries + 1):
        try:
            f = h5py.File(sidecar, "a", libver="latest")
            break
        except OSError as e:
            last_exc = e
            try:
                looks_lock = _looks_like_lock_error(e)
            except Exception:
                looks_lock = ("Unable to synchronously open file" in str(e)
                              or "open for write" in str(e)
                              or "open for read-only" in str(e))
            if (attempt == retries) or (not looks_lock):
                raise
            try:
                _h5clear(sidecar)
            except Exception:
                pass
            time.sleep(backoff * (attempt + 1))

    with f:
        gfit = f.require_group("/Fit")
        gfit.attrs["source_main_h5"] = str(h5_path)

        try:
            f.flush()
            f.swmr_mode = True
            logger.log(
                f"[FitTracker] SWMR writer enabled on sidecar: {sidecar}")
        except Exception as e:
            logger.log("[FitTracker] SWMR enable failed:")
            logger.log_exc(e)

        # history datasets (small)
        if "rmse_hist" not in gfit:
            gfit.create_dataset("rmse_hist", shape=(0,), maxshape=(None,), chunks=(8192,), dtype="f4")
        if "rmse_ewma" not in gfit:
            gfit.create_dataset("rmse_ewma", shape=(0,), maxshape=(None,), chunks=(8192,), dtype="f4")
        if "progress" not in gfit:
            gfit.create_dataset("progress", shape=(0, 4), maxshape=(None, 4), chunks=(2048, 4), dtype="f4")


        # lazy x datasets are created only on first save_x
        def _save_x(vec: np.ndarray, epoch: float, rmse: float) -> None:
            x = np.asarray(vec, dtype=np.float64).ravel(order="C")
            if "x_last" not in gfit:
                gfit.create_dataset("x_last", data=x, dtype="f8", maxshape=(x.size,), chunks=(x.size,))
            else:
                ds = gfit["x_last"]
                if ds.shape != (x.size,):
                    del gfit["x_last"]
                    gfit.create_dataset("x_last", data=x, dtype="f8", maxshape=(x.size,), chunks=(x.size,))
                else:
                    ds[...] = x
            gfit["x_last"].attrs["epoch"] = float(epoch)
            gfit["x_last"].attrs["rmse"]  = float(rmse)

            # optional x_best (keep best-by-RMSE)
            try:
                keep = False
                if "x_best" not in gfit:
                    keep = True
                else:
                    cur = float(gfit["x_best"].attrs.get("rmse", np.inf))
                    keep = (rmse < cur)
                if keep:
                    if "x_best" not in gfit:
                        gfit.create_dataset("x_best", data=x, dtype="f8", maxshape=(x.size,), chunks=(x.size,))
                    else:
                        dsb = gfit["x_best"]
                        if dsb.shape != (x.size,):
                            del gfit["x_best"]
                            gfit.create_dataset("x_best", data=x, dtype="f8", maxshape=(x.size,), chunks=(x.size,))
                        else:
                            dsb[...] = x
                    gfit["x_best"].attrs["epoch"] = float(epoch)
                    gfit["x_best"].attrs["rmse"]  = float(rmse)
            except Exception as e:
                logger.log("[FitTracker] x_best update failed:")
                logger.log_exc(e)
        # ---------- helpers for x snapshots ----------
        def _ensure_x_ds(N: int) -> None:
            """Create ring + last datasets if absent, sized by N."""
            if N <= 0:
                return
            if "x_last" not in gfit:
                gfit.create_dataset("x_last", shape=(N,), dtype="f4", chunks=(N,))
            if "x_ring" not in gfit:
                ring = gfit.create_dataset(
                    "x_ring",
                    shape=(cfg.ring_size, N),
                    maxshape=(cfg.ring_size, N),
                    chunks=(1, N),
                    dtype="f4",
                )
                gfit.create_dataset("x_epoch", shape=(cfg.ring_size,), dtype="i4")
                gfit.create_dataset("x_ts",    shape=(cfg.ring_size,), dtype="f8")
                gfit.create_dataset("x_rmse",  shape=(cfg.ring_size,), dtype="f4")
                gfit.attrs["x_head"] = np.int64(0)

        def _append_x(x32: np.ndarray, epoch: int, rmse: float | None) -> None:
            """Append into the ring and update x_last."""
            N = int(x32.size)
            _ensure_x_ds(N)
            head = int(gfit.attrs.get("x_head", 0))
            idx  = head % int(cfg.ring_size)
            gfit["x_ring"][idx, :] = x32
            gfit["x_epoch"][idx]   = int(epoch) if epoch is not None else -1
            gfit["x_ts"][idx]      = float(time.time())
            gfit["x_rmse"][idx]    = float(rmse) if (rmse is not None and np.isfinite(rmse)) else np.nan
            gfit["x_last"][:]      = x32
            gfit.attrs["x_head"]   = np.int64(head + 1)

        # batching knobs
        FLUSH_EVERY = int(os.environ.get("CUBEFIT_TRACKER_FLUSH_EVERY", "128"))
        FLUSH_INTERVAL = float(os.environ.get("CUBEFIT_TRACKER_FLUSH_SEC", "5.0"))
        pending = 0
        t_last = time.monotonic()

        # main loop with timeout, so we can flush even with no messages
        while True:
            try:
                msg = rx.get(timeout=1.0)
            except _queue.Empty:
                # idle tick: flush if needed
                now = time.monotonic()
                if (pending > 0) and (now - t_last >= FLUSH_INTERVAL):
                    try:
                        f.flush()
                    except Exception as e:
                        logger.log("[FitTracker] periodic flush failed:")
                        logger.log_exc(e)
                    pending = 0
                    t_last = now
                continue

            if msg is None or msg.get("op") == "stop":
                break

            try:
                op = msg.get("op")
                if op == "rmse_batch":
                    r = float(msg["value"]); e = float(msg.get("ewma", r))
                    d1 = gfit["rmse_hist"]; n1 = d1.shape[0]; d1.resize((n1+1,)); d1[n1] = r
                    d2 = gfit["rmse_ewma"]; n2 = d2.shape[0]; d2.resize((n2+1,)); d2[n2] = e
                    pending += 1

                elif op == "progress":
                    epoch = float(msg.get("epoch", 0))
                    done  = float(msg.get("done", 0))
                    total = float(msg.get("total", 0))
                    ewma  = float(msg.get("ewma") or np.nan)
                    dp = gfit["progress"]; n = dp.shape[0]
                    dp.resize((n+1, 4)); dp[n, :] = (epoch, done, total, ewma)
                    pending += 1

                elif op == "epoch_end":
                    epoch = float(msg.get("epoch", 0))
                    dp = gfit["progress"]; n = dp.shape[0]
                    dp.resize((n+1, 4)); dp[n, :] = (epoch, np.nan, np.nan, np.nan)
                    pending += 1

                elif op == "save_x":
                    _save_x(
                        np.asarray(msg["x"], np.float64),
                        float(msg.get("epoch", -1)),
                        float(msg.get("rmse", np.nan)),
                    )
                    pending += 1

                elif op == "x_snapshot":
                    _append_x(
                        np.asarray(msg["x"], np.float32),
                        msg.get("epoch"),
                        msg.get("rmse"),
                    )
                    pending += 1

                # batch/interval flush
                now = time.monotonic()
                if pending >= FLUSH_EVERY or (now - t_last) >= FLUSH_INTERVAL:
                    try:
                        f.flush()
                        pending = 0
                        t_last = now
                    except Exception as e:
                        logger.log("[FitTracker] flush failed:")
                        logger.log_exc(e)

            except Exception as e:
                logger.log("[FitTracker] message handling error:")
                logger.log_exc(e)

        # final flush
        try:
            f.flush()
        except Exception:
            pass

# --------------------------------- public API --------------------------------

class FitTracker:
    """
    Non-blocking tracker façade. Sends tiny messages to a sidecar writer proc.
    """
    def __init__(self, h5_path: str, cfg: Optional[TrackerConfig] = None):
        self.h5_path = str(h5_path)
        self.cfg = cfg or TrackerConfig()
        self._ewma = None
        self._alpha = 0.02

        # Sampling & queue knobs (env-overridable)
        self._rmse_stride = int(os.environ.get("CUBEFIT_RMSE_STRIDE", "16"))
        self._rmse_ctr = 0

        prefer = (os.environ.get("FITTRACKER_START", "spawn")).lower()
        avail = mp.get_all_start_methods()
        order = [m for m in (prefer, "spawn", "forkserver", "fork") if m in avail]

        last_err = None
        self._q = None
        self._proc = None
        self._start_method = None

        self._snap_last_t = 0.0
        self._snap_min_sec = float(os.environ.get("CUBEFIT_X_SNAPSHOT_SEC", "3600"))  # seconds between snapshots


        for m in order:
            try:
                ctx = mp.get_context(m)
                self._q = ctx.Queue(maxsize=int(os.environ.get("CUBEFIT_TRACKER_QSIZE", "8192")))
                self._proc = ctx.Process(target=_writer_main, args=(self.h5_path, self.cfg, self._q))
                self._proc.daemon = False
                self._proc.start()
                self._start_method = m
                break
            except Exception as e:
                last_err = e
                self._q = None
                self._proc = None
                continue

        if self._proc is None or self._q is None:
            raise RuntimeError("FitTracker: could not start writer") from last_err

    # ------------ public methods used by PipelineRunner / solver ---------------

    def set_meta(self, N: int) -> None:
        self._try_put({"op": "set_meta", "N": int(N)})

    def set_orbit_weights(self, w: np.ndarray) -> None:
        self._try_put({"op": "set_orbit_weights",
                       "w": np.asarray(w, np.float64).ravel()})

    def on_batch_rmse(self, rmse: float, *, block: bool = False) -> None:
        """
        Non-blocking RMSE push with optional subsampling.
        - Updates local EWMA always.
        - Enqueues at most every `self._rmse_stride` batches.
        - Queue put is non-blocking by default; drops if full.
        """
        try:
            r = float(rmse)
        except Exception:
            return
        if not np.isfinite(r):
            return

        
        # ---- drop pathological outliers to avoid huge spikes ----
        if self._ewma is not None:
            # Use the current EWMA as a scale; protect against 0.
            scale = max(self._ewma, 1.0)
            # Factor 1e3 is deliberately generous; tune if needed.
            if r > 1e3 * scale:
                # Ignore this sample completely; don't update EWMA or write to disk.
                return
        # ----------------------------------------------------------------

        # standard EWMA update
        if self._ewma is None:
            self._ewma = r
        else:
            self._ewma = (1.0 - self._alpha) * self._ewma + self._alpha * r

        self._rmse_ctr += 1
        if (self._rmse_ctr % max(1, self._rmse_stride)) != 0:
            return

        self._try_put({"op": "rmse_batch", "value": r, "ewma": float(self._ewma)}, block=block)

    def on_progress(self, epoch: int, spax_done: int, spax_total: int, *, rmse_ewma: Optional[float] = None) -> None:
        self._try_put({"op": "progress", "epoch": int(epoch),
                    "done": int(spax_done), "total": int(spax_total),
                    "ewma": float(rmse_ewma) if rmse_ewma is not None else None}, block=False)

    def on_epoch_end(self, epoch: int, stats: dict) -> None:
        self._try_put({"op": "epoch_end", "epoch": int(epoch), "stats": dict(stats)}, block=False)

    def maybe_save(self, x_final: np.ndarray, stats: dict) -> None:
        self._try_put({"op": "save_x",
                    "x": np.asarray(x_final, np.float32).ravel(order="C"),
                    "epoch": int(stats.get("epochs", -1)),
                    "rmse": float(stats.get("rmse_epoch_last",
                                            stats.get("rmse_final", math.nan)))}, block=False)

    def maybe_snapshot_x(self, x: np.ndarray, *, epoch: int | None = None,
                        rmse: float | None = None, force: bool = False) -> bool:
        """
        Non-blocking, time-gated snapshot of the current global solution vector.
        - Downcasts to float32 before enqueue to cut payload in half.
        - Throttled to at most one snapshot every _snap_min_sec unless force=True.
        Returns True if enqueued, False if skipped/dropped.
        """
        now = time.monotonic()
        if (not force) and ((now - self._snap_last_t) < max(1.0, self._snap_min_sec)):
            return False
        self._snap_last_t = now

        try:
            x32 = np.asarray(x, np.float32).ravel(order="C")
        except Exception:
            return False

        return self._try_put({"op": "x_snapshot",
                            "x": x32,
                            "epoch": int(epoch) if epoch is not None else None,
                            "rmse": float(rmse) if (rmse is not None and np.isfinite(rmse)) else None},
                            block=False)


    def close(self, timeout: float = 2.0) -> None:
        # send a real sentinel the writer understands
        q = getattr(self, "_q", None)
        if q is not None:
            try:
                q.put_nowait(None)  # sentinel
            except Exception:
                pass

        if self._proc is not None:
            self._proc.join(timeout=timeout)
            if self._proc.is_alive():
                try:
                    self._proc.terminate()
                except Exception:
                    pass

    # ----------------------------- helpers ------------------------------------

    def _try_put(self, msg, block: bool = False) -> bool:
        """
        Put `msg` into the tracker queue; non-blocking by default.
        Returns True if enqueued, False if dropped or no queue.
        """
        q = getattr(self, "_q", None)
        if q is None:
            return False
        try:
            if block:
                q.put(msg)
            else:
                q.put_nowait(msg)
            return True
        except _queue.Full:
            return False

class NullTracker:
    def set_meta(self, *a, **k): pass
    def set_orbit_weights(self, *a, **k): pass
    def on_progress(self, *a, **k): pass
    def on_batch_rmse(self, *a, **k): pass
    def on_epoch_end(self, *a, **k): pass
    def maybe_save(self, *a, **k): pass
    def close(self, *a, **k): pass
</file>

<file path="hdf5_manager.py">
# -*- coding: utf-8 -*-
r"""
    hdf5_manager.py
    Adriano Poci
    University of Oxford
    2025

    Platforms
    ---------
    Unix, Windows

    Synopsis
    --------
    Manages HDF5 data storage for CubeFit pipeline, including creation, loading,
    and validation of large, chunked arrays (templates, data cube, LOSVD, weights).
    Supports buffered template grids for safe convolution.

    Authors
    -------
    Adriano Poci <adriano.poci@physics.ox.ac.uk>

History
-------
v1.0:   Initial HDF5 design and validation. 7 September 2025


HDF5 manager for CubeFit.

This module provides a single, store-centric API around the HDF5 base file:
    - Persist core datasets (/LOSVD, /DataCube, /Templates, /Mask, /X_global)
    - Compute and persist spectral rebin operator (/RebinMatrix, /R_T)
      using an exact, flux-conserving bin-overlap scheme equivalent to
      the original `linRebin(lamRange, spectrum, M)`.
    - Compute and persist template FFTs (/TemplatesFFT) and a re-sized
      version consistent with the rebin rows (/TemplatesFFT_R).
    - Keep a small dimension manifest in root attributes.

Everything downstream (builder, solver, reader) reads from the file; nothing
needs optional overrides in memory.
"""

from __future__ import annotations
import numpy as np
import json
from dataclasses import dataclass
from pathlib import Path
from types import NoneType
from typing import Optional, Tuple, Sequence
import os, time, errno, subprocess, tempfile, shutil, fcntl
from contextlib import contextmanager
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D  # for BL legend proxies
import h5py

from dynamics.IFU.Constants import Constants

CTS = Constants()
C_KMS = CTS.c

# --------------------------------------------------------------------------- #
# Logger (safe fallback)
# --------------------------------------------------------------------------- #

try:
    # If your project already exposes a global `logger` module attribute,
    # import it so we can call `logger.log(...)`. Otherwise, use a fallback.
    from CubeFit.logger import get_logger

    logger = get_logger()
except Exception:  # pragma: no cover
    class _Logger:
        def log(self, *parts) -> None:
            try:
                msg = " ".join(str(p) for p in parts)
            except Exception:
                msg = "<log message rendering failed>"
            print(msg)
    logger = _Logger()  # type: ignore

# --------------------------------------------------------------------------- #
# Data structures
# --------------------------------------------------------------------------- #

# -------- Robust HDF5 open helpers (single entry-point) ---------------------
# Ensure HDF5 locking is disabled on shared filesystems unless the user overrides.
os.environ.setdefault("HDF5_USE_FILE_LOCKING", "FALSE")

# ------------------------------------------------------------------------------

def invalidate_done(h5_path: str):
    with open_h5(h5_path, role="writer") as f:
        g = f.require_group("/HyperCube")
        g.attrs["complete"] = False
        if "_done" in g:
            del g["_done"]

# ------------------------------------------------------------------------------

def print_hypercube_done_status(h5_path: str) -> None:
    """
    Print a compact resume of /HyperCube/_done and basic dataset geometry.
    """

    with open_h5(h5_path, "reader") as f:
        assert "/HyperCube/models" in f, "No /HyperCube/models in file."
        M = f["/HyperCube/models"]
        S, C, P, L = map(int, M.shape)
        chunks = M.chunks or (S, 1, P, L)
        print(f"models shape={M.shape} chunks={chunks}")

        assert "/HyperCube/_done" in f, "No /HyperCube/_done bitmap."
        D = np.asarray(f["/HyperCube/_done"][...])
        total = int(D.size)
        done  = int(D.sum())
        print(f"_done: {done}/{total} tiles complete "
              f"({100.0*done/max(1,total):.1f}%)")

        # Show a few finished tile indices (grid space)
        idx = np.argwhere(D != 0)
        if idx.size:
            print("first finished tiles (is,ic,ip):", idx[:5].tolist())
        else:
            print("no finished tiles yet.")

# ------------------------------------------------------------------------------

def _vel_to_pixel_shift(v_kms: np.ndarray | float, dlog: float) -> np.ndarray | float:
    """Convert velocity (km/s) → pixel shift on a log-λ grid with step dlog."""
    return np.log1p(np.asarray(v_kms, dtype=np.float64) / C_KMS) / float(dlog)

def choose_minimal_template_bounds(
    *,
    TemPix_log: np.ndarray,      # (T,)   template log-λ grid
    ObsPix_log: np.ndarray,      # (L,)   observed log-λ grid
    VelPix_kms: np.ndarray,      # (V,)   LOSVD velocity grid (km/s)
    LOSVD: np.ndarray | None = None,  # optional (S,V,C) to auto-trim tails
    mode: str = "robust",        # "robust" or "conservative"
    eps: float = 1e-3,           # central (1-eps) mass if mode="robust"
    safety_pixels: int = 8       # extra guard on each side (template pixels)
) -> dict:
    """
    Compute minimal safe template bounds (on the template grid) so that:
      - linear convolution by the LOSVD kernel never needs signal outside the band
      - rebinning to ObsPix is clean (no edge artefacts)

    Returns:
      {
        'i_min': int, 'i_max': int,     # slice TemPix_log[i_min : i_max+1]
        'tmin': float, 'tmax': float,   # log-λ bounds actually chosen
        'k_left': int, 'k_right': int,  # required guard in template pixels
        'dlog': float
      }
    """
    TemPix_log = np.asarray(TemPix_log, dtype=np.float64).ravel()
    ObsPix_log = np.asarray(ObsPix_log, dtype=np.float64).ravel()
    VelPix_kms = np.asarray(VelPix_kms, dtype=np.float64).ravel()

    if TemPix_log.size < 3 or ObsPix_log.size < 3 or VelPix_kms.size < 3:
        raise ValueError("TemPix_log, ObsPix_log, VelPix_kms must be 1-D arrays of length ≥ 3.")

    dlog = float(np.median(np.diff(TemPix_log)))

    # 1) effective velocity span to cover
    if mode.lower() == "robust" and LOSVD is not None:
        H = np.asarray(LOSVD, dtype=np.float64)
        if H.ndim != 3 or H.shape[1] != VelPix_kms.size:
            raise ValueError("LOSVD must have shape (S,V,C) matching len(VelPix_kms).")
        H_sum = H.sum(axis=(0, 2))  # (V,)
        total = H_sum.sum()
        if not np.isfinite(total) or total <= 0:
            v_left, v_right = float(VelPix_kms.min()), float(VelPix_kms.max())
        else:
            pdf = H_sum / total
            cdf = np.cumsum(pdf)
            v_left  = float(np.interp(eps/2,       cdf, VelPix_kms))
            v_right = float(np.interp(1.0 - eps/2, cdf, VelPix_kms))
    else:
        v_left, v_right = float(VelPix_kms.min()), float(VelPix_kms.max())

    # 2) guard in template pixels (allow asymmetric)
    k_left  = int(np.ceil(abs(_vel_to_pixel_shift(min(0.0, v_left),  dlog))))
    k_right = int(np.ceil(abs(_vel_to_pixel_shift(max(0.0, v_right), dlog))))
    k_left  += int(safety_pixels)
    k_right += int(safety_pixels)

    # 3) target template bounds around observed range
    t_target_min = ObsPix_log[0]  - k_left  * dlog
    t_target_max = ObsPix_log[-1] + k_right * dlog

    i_min = int(np.searchsorted(TemPix_log, t_target_min, side="right") - 1)
    i_max = int(np.searchsorted(TemPix_log, t_target_max, side="left"))
    i_min = max(0, i_min)
    i_max = min(TemPix_log.size - 1, i_max)

    tmin = float(TemPix_log[i_min])
    tmax = float(TemPix_log[i_max])

    # sanity: ensure margins are sufficient in pixel units
    left_margin_px  = int(np.floor((ObsPix_log[0]  - tmin) / dlog))
    right_margin_px = int(np.floor((tmax - ObsPix_log[-1]) / dlog))
    if left_margin_px < k_left or right_margin_px < k_right:
        i_min = max(0, i_min - (k_left  - left_margin_px  + 1))
        i_max = min(TemPix_log.size - 1, i_max + (k_right - right_margin_px + 1))
        tmin, tmax = float(TemPix_log[i_min]), float(TemPix_log[i_max])

    return dict(i_min=i_min, i_max=i_max, tmin=tmin, tmax=tmax,
                k_left=k_left, k_right=k_right, dlog=dlog)
            
# ------------------------------------------------------------------------------

def _run(cmd: list[str]) -> tuple[bool, str]:
    try:
        out = subprocess.run(cmd, check=True, capture_output=True, text=True)
        return True, out.stdout.strip()
    except Exception as e:
        return False, str(e)

def _h5repack_inplace(path: Path) -> bool:
    tmp = path.with_suffix(path.suffix + ".tmp-repack")
    ok, out = _run(["h5repack", str(path), str(tmp)])
    if ok:
        try:
            os.replace(tmp, path)
            logger.log(f"[HDF5] h5repack succeeded; replaced {path}")
        except Exception as e:
            try:
                os.remove(tmp)
            except Exception:
                pass
            return False
        return True
    else:
        try:
            logger.log(f"[HDF5] h5repack failed: {out}")
        except Exception:
            pass
        return False

def _open_once(path: str, mode: str):
    """One attempt to open an HDF5 file with libver='latest'."""
    # NB: SWMR read requires explicit swmr=True; we don’t enable it by default
    # because most pipelines don’t publish SWMR metadata consistently.
    return h5py.File(path, mode, libver="latest")

# ---------- GLOBAL FILE LOCK (one writer at a time across processes) ----------
@contextmanager
def _writer_lock(base_path: str | Path, timeout: float = 300.0, poll: float = 0.2):
    """
    Cross-process OS lock using fcntl on a sidecar .lock file.
    Ensures exactly one writer for a given .h5 at a time.
    """
    lock_path = Path(str(base_path) + ".lock")
    lock_fd = os.open(lock_path, os.O_CREAT | os.O_RDWR, 0o644)
    start = time.time()
    try:
        while True:
            try:
                fcntl.flock(lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
                break
            except BlockingIOError:
                if (time.time() - start) > timeout:
                    raise TimeoutError(f"Timeout acquiring writer lock: {lock_path}")
                time.sleep(poll)
        yield
    finally:
        try:
            fcntl.flock(lock_fd, fcntl.LOCK_UN)
        finally:
            os.close(lock_fd)

# Treat these as "fatal" only if truly unavoidable.
_LOCK_STRS = ("Unable to synchronously open file (unable to lock file",
              "file is already open for write",
              "file is already open for read-only")

def _looks_like_lock_error(exc: Exception) -> bool:
    msg = str(exc)
    return any(s in msg for s in _LOCK_STRS)

def _h5clear(path: str | Path) -> bool:
    """Try to clear SWMR/consistency flags in-place. Returns True on success."""
    p = str(path)
    try:
        # Avoid getting stuck on filesystems with broken locking.
        env = dict(os.environ)
        env.setdefault("HDF5_USE_FILE_LOCKING", "FALSE")
        r = subprocess.run(["h5clear", "-s", p], env=env,
                           stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                           text=True)
        if r.returncode == 0:
            logger.log(f"[HDF5] h5clear -s succeeded on {p}")
            return True
        else:
            logger.log(f"[HDF5] h5clear failed rc={r.returncode}: {r.stderr.strip()}")
            return False
    except FileNotFoundError:
        logger.log("[HDF5] h5clear not found on PATH.")
        return False
    except Exception as e:
        logger.log(f"[HDF5] h5clear unexpected error: {e}")
        return False

@contextmanager
def open_h5(path: str | Path,
            role: str = "reader",
            retries: int = 3,
            backoff: float = 0.4,
            *,
            swmr: bool | None = None,
            locking: bool | None = None):
    """
    Robust HDF5 open with lock handling and modest raw-chunk cache.
    role='reader' -> read-only; role='writer' -> append/update.
    If swmr is True and role='reader', open in SWMR-read mode.
    """
    p = Path(path)
    if role not in ("reader", "writer"):
        raise ValueError("role must be 'reader' or 'writer'")

    # Allow caller to force HDF5 file locking on/off for this process
    if locking is not None:
        os.environ["HDF5_USE_FILE_LOCKING"] = "TRUE" if locking else "FALSE"
    elif os.environ.get("HDF5_USE_FILE_LOCKING") is None:
        os.environ["HDF5_USE_FILE_LOCKING"] = "FALSE"

    mode = "r" if role == "reader" else "a"
    last_exc = None
    for attempt in range(retries + 1):
        try:
            rdcc_nbytes = int(os.environ.get("CUBEFIT_RDCC_NBYTES", str(4 * 1024**3)))
            rdcc_nslots = int(os.environ.get("CUBEFIT_RDCC_NSLOTS", "400_003"))
            rdcc_w0     = float(os.environ.get("CUBEFIT_RDCC_W0", "0.9"))

            kwargs = dict(libver="latest",
                          rdcc_nbytes=rdcc_nbytes,
                          rdcc_nslots=rdcc_nslots,
                          rdcc_w0=rdcc_w0)

            # h5py allows swmr=... only for read-only opens
            if role == "reader" and (swmr is True):
                kwargs["swmr"] = True

            f = h5py.File(p, mode, **kwargs)
            try:
                yield f
            finally:
                try: f.flush()
                except Exception: pass
                f.close()
            return
        except OSError as e:
            last_exc = e
            # Only retry on likely lock/consistency errors
            if not _looks_like_lock_error(e) or attempt == retries:
                raise
            logger.log(f"[HDF5] open({mode}) failed with lock: {e} (attempt {attempt+1}/{retries})")
            try: _h5clear(p)
            except Exception: pass
            time.sleep(backoff * (attempt + 1))
    raise last_exc

# ---------------------------------------------------------------------------

@dataclass(frozen=True)
class H5Dims:
    """Dimensions recorded in the base HDF5 file."""
    nSpat: int   # spatial apertures
    nLSpec: int  # observed spectrum length
    nTSpec: int  # template spectrum length (time-domain)
    nVel: int    # LOSVD length
    nComp: int   # number of LOSVD components
    nPop: int    # number of stellar population templates

# --------------------------------------------------------------------------- #
# Flux-conserving rebin helpers (exact linRebin equivalent)
# --------------------------------------------------------------------------- #

def _bin_edges_from_centers(centers: np.ndarray) -> np.ndarray:
    """
    Compute bin edges from approximately uniform centers. Interior edges are
    midpoints; the first/last edges are extrapolated by half a step.
    """
    c = np.asarray(centers, dtype=np.float64).ravel()
    if c.ndim != 1 or c.size < 2:
        raise ValueError("centers must be 1-D with length >= 2.")
    step0 = c[1] - c[0]
    stepN = c[-1] - c[-2]
    mid = 0.5 * (c[:-1] + c[1:])
    edges = np.empty(c.size + 1, dtype=np.float64)
    edges[1:-1] = mid
    edges[0] = c[0] - 0.5 * step0
    edges[-1] = c[-1] + 0.5 * stepN
    return edges

def _build_linrebin_matrix_from_edges(old_edges: np.ndarray,
                                      new_edges: np.ndarray) -> np.ndarray:
    """
    Build a dense, flux-conserving rebin matrix R such that

        new[j] = sum_i old[i] * overlap(old_i, new_j) / width(new_j),

    where old_i = [old_edges[i], old_edges[i+1]] and new_j is defined
    similarly. This matches the semantics of the reference linRebin.
    """
    old_edges = np.asarray(old_edges, dtype=np.float64).ravel()
    new_edges = np.asarray(new_edges, dtype=np.float64).ravel()
    if old_edges.ndim != 1 or new_edges.ndim != 1:
        raise ValueError("edges must be 1-D.")
    if np.any(np.diff(old_edges) <= 0) or np.any(np.diff(new_edges) <= 0):
        raise ValueError("edges must be strictly increasing.")

    n_src = old_edges.size - 1
    n_obs = new_edges.size - 1
    R = np.zeros((n_obs, n_src), dtype=np.float64)

    i = 0
    for j in range(n_obs):
        a, b = new_edges[j], new_edges[j + 1]
        width_new = b - a
        # advance i to first old bin that may overlap
        while i < n_src and old_edges[i + 1] <= a:
            i += 1
        ii = i
        while ii < n_src and old_edges[ii] < b:
            left = max(a, old_edges[ii])
            right = min(b, old_edges[ii + 1])
            overlap = right - left
            if overlap > 0:
                R[j, ii] = overlap / width_new
            ii += 1
    return R

def _build_linrebin_matrix_from_range(lam_range: Tuple[float, float],
                                      n_src: int, n_obs: int) -> np.ndarray:
    """
    Convenience wrapper to build R from a common wavelength range and counts.
    """
    lam_lo, lam_hi = float(lam_range[0]), float(lam_range[1])
    old_edges = np.linspace(lam_lo, lam_hi, num=n_src + 1, dtype=np.float64)
    new_edges = np.linspace(lam_lo, lam_hi, num=n_obs + 1, dtype=np.float64)
    return _build_linrebin_matrix_from_edges(old_edges, new_edges)

# --------------------------------------------------------------------------- #
# Manager
# --------------------------------------------------------------------------- #

class H5Manager:
    """
    HDF5 manager that owns the base file layout and derived artifacts.

    Typical usage:
        mgr = H5Manager(base_h5, tem_pix=tem, obs_pix=obs)
        mgr.populate_from_arrays(losvd=..., datacube=..., templates=...)
        mgr.ensure_rebin_and_resample()  # /RebinMatrix, /R_T, /TemplatesFFT_R

    After this, the builder and solver read everything they need from disk.
    """

    def __init__(self,
                 base_path: str | Path,
                 *,
                 compression: str = "gzip",
                 clevel: int = 4,
                 shuffle: bool = True,
                 tem_pix: Optional[np.ndarray] = None,
                 obs_pix: Optional[np.ndarray] = None) -> None:
        self.base_path = Path(base_path)
        self.compression = compression
        self.clevel = int(clevel)
        self.shuffle = bool(shuffle)
        self.base_path.parent.mkdir(parents=True, exist_ok=True)

        # If grids are provided, persist them and build rebin artifacts.
        if tem_pix is not None and obs_pix is not None:
            self.set_spectral_grids(tem_pix, obs_pix)

    # ------------------------------- Base IO -------------------------------- #

    def _open_ro(self):
        """Robust read-only open via global wrapper."""
        return open_h5(self.base_path, role="reader")

    def _open_rw(self):
        """Robust read/write open via global wrapper."""
        return open_h5(self.base_path, role="writer")

    def init_base(self, dims: H5Dims) -> None:
        """
        Ensure core datasets exist with correct shape/dtype. Recreate any that
        mismatch. Record dims in file attributes.
        """
        nS, nL, nT = int(dims.nSpat), int(dims.nLSpec), int(dims.nTSpec)
        nV, nC, nP = int(dims.nVel), int(dims.nComp), int(dims.nPop)

        def req(name: str, shape, dtype, chunks):
            with self._open_rw() as f:
                if name in f:
                    ds = f[name]
                    if tuple(ds.shape) == tuple(shape) and str(ds.dtype) == dtype:
                        return
                    del f[name]
                f.create_dataset(
                    name, shape=shape, dtype=dtype, chunks=chunks,
                    compression=self.compression, compression_opts=self.clevel,
                    shuffle=self.shuffle, fletcher32=False
                )

        # LOSVD kernels: (nSpat, nVel, nComp)
        req("/LOSVD", (nS, nV, nC), "f8",
            (min(32, nS), nV, min(16, nC)))

        # Observed spectra: (nSpat, nLSpec)
        req("/DataCube", (nS, nL), "f8", (min(32, nS), nL))

        # Templates: time-domain (nPop, nTSpec)
        req("/Templates", (nP, nT), "f8", (min(32, nP), nT))

        # Templates FFT (same width as time-domain length)
        req("/TemplatesFFT", (nP, nT), "complex128",
            (min(32, nP), nT))

        with self._open_rw() as f:
            # Store scalar attrs (portable in HDF5)
            f.attrs["nSpat"] = int(nS)
            f.attrs["nLSpec"] = int(nL)
            f.attrs["nTSpec"] = int(nT)
            f.attrs["nVel"]   = int(nV)
            f.attrs["nComp"]  = int(nC)
            f.attrs["nPop"]   = int(nP)
            f.attrs["dims_schema"] = "scalar_v1"  # optional hint
        logger.log("[H5Manager] Base initialized.")

    @property
    def models_path(self) -> str:
        """The HDF5 file that contains /HyperCube/models (same as base_path)."""
        return str(self.base_path)

    def ensure_models(self, *, shape: tuple, chunks: tuple, dtype: str = "f4") -> None:
        """
        Ensure /HyperCube/models exists with the given shape/chunks/dtype.
        Recreate if incompatible. Stores basic attrs on the /HyperCube group.
        """
        S, C, P, L = map(int, shape)
        cS, cC, cP, cL = map(int, chunks)
        with self._open_rw() as f:
            hg = f.require_group("/HyperCube")
            if "models" in hg:
                ds = hg["models"]
                ok_shape = tuple(ds.shape) == (S, C, P, L)
                ok_dtype = str(ds.dtype) == dtype or (dtype == "f4" and str(ds.dtype) == "float32")
                ok_chunks = (getattr(ds, "chunks", None) == (cS, cC, cP, cL))
                if not (ok_shape and ok_dtype and ok_chunks):
                    del hg["models"]
                    ds = None
            else:
                ds = None

            if ds is None:
                ds = hg.create_dataset(
                    "models",
                    shape=(S, C, P, L),
                    chunks=(cS, cC, cP, cL),
                    dtype=dtype,
                    compression=self.compression,
                    compression_opts=self.clevel,
                    shuffle=self.shuffle,
                )
                logger.log(f"[HDF5] Created /HyperCube/models: shape={(S,C,P,L)}, "
                        f"chunks={(cS,cC,cP,cL)}, dtype={dtype}")
            else:
                logger.log(f"[HDF5] Reusing /HyperCube/models: shape={ds.shape}, "
                        f"chunks={ds.chunks}, dtype={ds.dtype}")

            # stash basic info on the group for quick inspection
            hg.attrs["chunks_S"] = cS
            hg.attrs["chunks_C"] = cC
            hg.attrs["chunks_P"] = cP
            hg.attrs["chunks_L"] = cL
            # clear completion flag (builder will set it true on success)
            hg.attrs["complete"] = False

    def get_manifest(self) -> dict | None:
        """Return manifest dict from /HyperCube attrs, or None if absent."""
        with h5py.File(self.base_path, "r", libver="latest") as f:
            if "/HyperCube" not in f:
                return None
            g = f["/HyperCube"]
            # preferred: JSON string
            if "manifest_json" in g.attrs:
                try:
                    return json.loads(g.attrs["manifest_json"])
                except Exception:
                    return None
            # fallback: older scalar attrs (if you had them)
            keys = ("shape_S", "shape_C", "shape_P", "shape_L", "dtype_models")
            if all(k in g.attrs for k in keys):
                return {
                    "shape": (int(g.attrs["shape_S"]), int(g.attrs["shape_C"]),
                            int(g.attrs["shape_P"]), int(g.attrs["shape_L"])),
                    "chunks": (int(g.attrs.get("chunks_S", 0)),
                            int(g.attrs.get("chunks_C", 0)),
                            int(g.attrs.get("chunks_P", 0)),
                            int(g.attrs.get("chunks_L", 0))),
                    "dtype_models": str(g.attrs["dtype_models"]),
                }
        return None

    def set_manifest(self, *, shape: tuple, chunks: tuple, dtype_models: str = "f4",
                    extra: dict | None = None) -> None:
        """
        Write manifest to /HyperCube attrs (as JSON). Safe to call repeatedly.
        """
        man = {
            "shape": tuple(map(int, shape)),
            "chunks": tuple(map(int, chunks)),
            "dtype_models": dtype_models,
            "models_path": str(self.base_path),   # << required by HyperCubeReader
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        }
        if extra:
            try:
                json.dumps(extra)
                man["extra"] = extra
            except Exception:
                man["extra"] = str(extra)

        with self._open_rw() as f:
            g = f.require_group("/HyperCube")
            g.attrs["manifest_json"] = json.dumps(man)
            # also store a few scalars for quick grep/debug
            S, C, P, L = man["shape"]
            g.attrs["shape_S"] = S; g.attrs["shape_C"] = C
            g.attrs["shape_P"] = P; g.attrs["shape_L"] = L
            cS, cC, cP, cL = man["chunks"]
            g.attrs["chunks_S"] = cS; g.attrs["chunks_C"] = cC
            g.attrs["chunks_P"] = cP; g.attrs["chunks_L"] = cL
            g.attrs["dtype_models"] = dtype_models

    def models_complete(self) -> bool:
        """Return True if /HyperCube.attrs['complete'] is True."""
        with self._open_ro() as f:
            if "/HyperCube" not in f:
                return False
            return bool(f["/HyperCube"].attrs.get("complete", False))

    def mark_models_complete(self, flag: bool = True) -> None:
        """Set /HyperCube.attrs['complete'] flag."""
        with self._open_rw() as f:
            g = f.require_group("/HyperCube")
            g.attrs["complete"] = bool(flag)
        logger.log(f"[HDF5] /HyperCube complete={bool(flag)}")

    def _compute_template_guard_pixels(self, tem_pix: np.ndarray, vel_pix: np.ndarray, *, safety_pad_px: int = 64) -> int:
        tem_pix = np.asarray(tem_pix, dtype=float).ravel()
        vel_pix = np.asarray(vel_pix, dtype=float).ravel()
        if tem_pix.size < 3: raise ValueError("tem_pix must have ≥3 points.")
        if vel_pix.size < 2: raise ValueError("vel_pix must have ≥2 points.")
        dlog_tem = float(np.median(np.diff(tem_pix)))
        vmax = float(np.max(np.abs(vel_pix)))
        kmax = int(np.ceil(np.log1p(vmax / C_KMS) / dlog_tem))
        return int(kmax + int(safety_pad_px))

    def _build_R_T_dense(self, tem_pix: np.ndarray, obs_pix: np.ndarray) -> np.ndarray:
        tem_pix = np.asarray(tem_pix, dtype=float).ravel()
        obs_pix = np.asarray(obs_pix, dtype=float).ravel()
        if not (np.all(np.diff(tem_pix) > 0) and np.all(np.diff(obs_pix) > 0)):
            raise ValueError("tem_pix and obs_pix must be strictly increasing (log-λ).")
        T, L = tem_pix.size, obs_pix.size
        R_T = np.zeros((T, L), dtype=np.float32)
        j  = np.searchsorted(tem_pix, obs_pix, side="right") - 1
        j  = np.clip(j, 0, T - 2)
        j1 = j + 1
        t0, t1 = tem_pix[j], tem_pix[j1]
        denom = (t1 - t0); denom[denom == 0.0] = 1.0
        w1 = (obs_pix - t0) / denom; w0 = 1.0 - w1
        cols = np.arange(L)
        R_T[j,  cols] += w0.astype(np.float32, copy=False)
        R_T[j1, cols] += w1.astype(np.float32, copy=False)
        return R_T

    def get_spectral_grids(self) -> tuple[np.ndarray, np.ndarray]:
        with self._open_ro() as f:
            tem = np.asarray(f["/TemPix"][...], dtype=np.float64)
            obs = np.asarray(f["/ObsPix"][...], dtype=np.float64)
        return tem, obs

    def get_velocity_grid(self) -> np.ndarray | None:
        with self._open_ro() as f:
            if "/VelPix" not in f: return None
            return np.asarray(f["/VelPix"][...], dtype=np.float64)

    def get_template_pop_shape(self) -> tuple[int, ...]:
        with self._open_ro() as f:
            Tds = f["/Templates"]
            a = Tds.attrs.get("pop_shape", None)
            if a is None:
                P = int(Tds.shape[0])
                return (P,)
            return tuple(int(x) for x in a)

    def read_templates_unflattened(self) -> np.ndarray:
        with self._open_ro() as f:
            T2 = np.asarray(f["/Templates"][...], dtype=np.float64)  # (P,T)
            pop_shape = self.get_template_pop_shape()
            P_expected = int(np.prod(pop_shape, dtype=np.int64))
            if T2.shape[0] != P_expected:
                return T2
            return T2.reshape(*pop_shape, T2.shape[1])

    def _write_dims_attrs(self, f, dims: dict) -> None:
        """
        Persist dims in multiple robust forms:
          - / attrs: dims_json (JSON string)
          - / attrs: dims.<key>=value (individual ints for quick access)
          - file attrs: dims_json (duplicate for convenience)
        """
        # normalize to ints
        d = {k: int(v) for k, v in dims.items()}
        s = json.dumps(d)

        # Root group scalars for quick lookups
        for k, v in d.items():
            f["/"].attrs[f"dims.{k}"] = int(v)

        # JSON copies (root and file)
        f["/"].attrs["dims_json"] = s
        f.attrs["dims_json"] = s

    def _read_dims_attrs(self, f) -> dict:
        """
        Read dims from file/group attrs, handling both JSON and scalar forms.
        Falls back to dataset shapes if needed.
        """
        # Prefer JSON on root
        if "dims_json" in f["/"].attrs:
            val = f["/"].attrs["dims_json"]
            if isinstance(val, bytes):
                val = val.decode("utf-8")
            try:
                return {k: int(v) for k, v in json.loads(val).items()}
            except Exception:
                pass

        # Reconstruct from scalars
        out = {}
        for k in ("nSpat", "nLSpec", "nTSpec", "nVel", "nComp", "nPop"):
            key = f"dims.{k}"
            if key in f["/"].attrs:
                out[k] = int(f["/"].attrs[key])

        # Fallback to dataset shapes if needed
        if not out:
            if "/DataCube" in f:
                S, L = map(int, f["/DataCube"].shape)
                out["nSpat"], out["nLSpec"] = S, L
            if "/LOSVD" in f:
                S2, V, C = map(int, f["/LOSVD"].shape)
                out.setdefault("nVel", V)
                out.setdefault("nComp", C)
            if "/Templates" in f:
                P, T = map(int, f["/Templates"].shape)
                out.setdefault("nPop", P)
                out.setdefault("nTSpec", T)
        return out

    def _write_guard_attrs(self, f, guard_info: dict) -> None:
        """
        Persist guard_info in robust forms:
          - / attrs: guard.Kguard_px, guard.dlog_tem, ...
          - / attrs: guard_info_json (JSON string)
        """
        # coerce to plain Python ints/floats
        d = {
            "Kguard_px": int(guard_info["Kguard_px"]),
            "dlog_tem": float(guard_info["dlog_tem"]),
            "obs_lo": float(guard_info["obs_lo"]),
            "obs_hi": float(guard_info["obs_hi"]),
            "tem_lo": float(guard_info["tem_lo"]),
            "tem_hi": float(guard_info["tem_hi"]),
            "safety_pad_px": int(guard_info["safety_pad_px"]),
        }
        # scalar attrs on root for quick lookups
        for k, v in d.items():
            f["/"].attrs[f"guard.{k}"] = v
        # JSON copy
        f["/"].attrs["guard_info_json"] = json.dumps(d)

    def _read_guard_attrs(self) -> dict:
        """
        Read guard_info from the file; returns a dict with keys matching _write_guard_attrs.
        """
        with self._open_ro() as f:
            root = f["/"].attrs
            if "guard_info_json" in root:
                val = root["guard_info_json"]
                if isinstance(val, bytes):
                    val = val.decode("utf-8")
                try:
                    return {k: (int(v) if k.endswith("_px") or k == "Kguard_px" else float(v))
                            for k, v in json.loads(val).items()}
                except Exception:
                    pass
            # fall back to scalar attrs if JSON missing
            out = {}
            for k in ("Kguard_px", "dlog_tem", "obs_lo", "obs_hi", "tem_lo", "tem_hi", "safety_pad_px"):
                key = f"guard.{k}"
                if key in root:
                    out[k] = root[key]
            return out

    def _write_1d(self, f, name: str, arr, dtype, *,
                units: str | None = None,
                semantic: str | None = None,
                chunk_elems: int = 4096):
        """Robust writer for 1-D arrays with overwrite semantics & attrs."""
        a = np.asarray(arr, dtype=dtype).ravel()
        if name in f:
            del f[name]                      # safe here; populate_* runs pre-SWMR
        ds = f.create_dataset(
            name,
            data=a,
            dtype=dtype,
            chunks=(min(int(a.size), int(chunk_elems)),),
            compression=self.compression,
            compression_opts=self.clevel,
            shuffle=self.shuffle,
        )
        if units is not None:
            ds.attrs["units"] = units
        if semantic is not None:
            ds.attrs["semantic"] = semantic
        return ds
    
    def populate_from_arrays(
        self,
        *,
        losvd: np.ndarray,
        datacube: np.ndarray,
        templates: np.ndarray,
        mask: np.ndarray | None = None,
        x_init: np.ndarray | None = None,
        tem_pix: np.ndarray | None = None,
        obs_pix: np.ndarray | None = None,
        vel_pix: np.ndarray | None = None,
        safety_pad_px: int = 64,
        xpix: np.ndarray | None = None,     # length nPix, float
        ypix: np.ndarray | None = None,     # length nPix, float
        binnum: np.ndarray | None = None,   # length nPix, int in [0, S)
        orbit_weights: np.ndarray | None = None, 
    ) -> dict:
        """
        Expected input shapes (strict):
        datacube : (L_obs, S)   # wavelength-major
        losvd    : (S, V, C)
        templates: (T_tem, nMetals, nAges, nAlphas) # spectral axis is first
        mask     : (L_obs,) optional

        Stores:
        /DataCube   -> (S, L_obs) float64
        /LOSVD      -> (S, V, C)  float64
        /Templates  -> (P, T_c)   float64 (flattened populations, cropped spectral)
        /TemPix     -> (T_c,)     float64
        /ObsPix     -> (L_obs,)   float64
        /VelPix     -> (V,)       float64
        /R_T        -> (T_c, L_obs) float32
        attrs on /Templates to reconstruct N-D population axes and crop indices.
        /XPix       -> (nPix,) float64
        /YPix       -> (nPix,) float64
        /BinNum     -> (nPix,) int32   (pixel → spatial-bin index in [0..S-1])
        /HyperCube/data_flux -> (S,) float64
            Per-spaxel mean of observed flux over *unmasked* wavelengths (λ),
            computed only over unmasked wavelengths. Masked wavelengths
            are written as 0.0. This vector is the solver/builder’s
            canonical "L_vec".
        """

        # ---------- normalize arrays ----------
        losvd_in     = np.asarray(losvd)
        datacube_in  = np.asarray(datacube)
        templates_in = np.asarray(templates)

        # ---------- source grids from file if not passed ----------
        base_exists = Path(self.base_path).exists()

        if base_exists:
            # Only read if the file already exists
            with self._open_ro() as f_ro:
                if tem_pix is None:
                    if "/TemPix" not in f_ro:
                        raise RuntimeError("Missing /TemPix; pass tem_pix or call "
                                        "set_spectral_grids(...).")
                    tem_pix = np.asarray(f_ro["/TemPix"][...], dtype=np.float64)
                else:
                    tem_pix = np.asarray(tem_pix, dtype=np.float64)

                if obs_pix is None:
                    if "/ObsPix" not in f_ro:
                        raise RuntimeError("Missing /ObsPix; pass obs_pix or call "
                                        "set_spectral_grids(...).")
                    obs_pix = np.asarray(f_ro["/ObsPix"][...], dtype=np.float64)
                else:
                    obs_pix = np.asarray(obs_pix, dtype=np.float64)

                if vel_pix is not None:
                    vel_pix = np.asarray(vel_pix, dtype=np.float64)
        else:
            # Fresh run: no base file yet. If you supplied the grids, persist them.
            if tem_pix is None or obs_pix is None:
                raise RuntimeError(
                    "Base HDF5 does not exist yet and spectral grids were not "
                    "provided. Pass tem_pix and obs_pix (and optionally vel_pix) "
                    "on the first run."
                )
            tem_pix = np.asarray(tem_pix, dtype=np.float64)
            obs_pix = np.asarray(obs_pix, dtype=np.float64)
            if vel_pix is not None:
                vel_pix = np.asarray(vel_pix, dtype=np.float64)

            # Create the file and write the grids + rebin artifacts so downstream
            # code can read them
            self.set_spectral_grids(tem_pix, obs_pix)
            # (No read here; we just wrote them.)

        # ---------- strict shape checks & normalization ----------
        # datacube comes as (L, S) → store as (S, L)
        if datacube_in.ndim != 2:
            raise ValueError(f"datacube must be 2-D (L,S). Got "
                            f"{datacube_in.ndim}D with shape "
                            f"{datacube_in.shape}.")
        L_obs = int(obs_pix.size)
        if datacube_in.shape[0] != L_obs:
            raise ValueError(f"datacube first dim must equal len(ObsPix)={L_obs}, "
                            f"got {datacube_in.shape}.")
        datacube_SL = datacube_in.T  # (S, L)

        # losvd is (S, V, C)
        if losvd_in.ndim != 3:
            raise ValueError(f"losvd must be 3-D (S,V,C). Got "
                            f"{losvd_in.ndim}D with shape {losvd_in.shape}.")
        S, L = map(int, datacube_SL.shape)
        S2, V, C = map(int, losvd_in.shape)
        if S2 != S:
            raise ValueError(f"LOSVD S mismatch: datacube S={S}, losvd S={S2}.")
        if int(vel_pix.size) != V:
            raise ValueError(f"VelPix length {vel_pix.size} != LOSVD V {V}.")

        # templates have spectral axis FIRST with length len(tem_pix)
        T_len = int(tem_pix.size)
        if templates_in.ndim < 1:
            raise ValueError("templates must be at least 1-D.")
        if templates_in.shape[0] != T_len:
            raise ValueError(
                f"templates spectral axis (axis 0) length "
                f"{templates_in.shape[0]} != len(tem_pix) {T_len}."
            )

        templates_in_shape = tuple(templates_in.shape)          # (T, *pop_axes)
        orig_t_axis = 0
        pop_shape = tuple(int(s) for s in templates_in_shape[1:]) or (1,)

        # move spectral axis to last, then flatten populations -> (P, T_len)
        tmpl = np.moveaxis(templates_in, 0, -1)                 # (*pop_axes, T)
        templates_PT = tmpl.reshape(-1, T_len)                  # (P, T)
        P, T = map(int, templates_PT.shape)

        # ---------- compute guard (template pixels) & crop spectral axis ----------
        Kguard = self._compute_template_guard_pixels(
            tem_pix, vel_pix, safety_pad_px=safety_pad_px
        )
        dlog_tem = float(np.median(np.diff(tem_pix)))
        lam_lo   = float(obs_pix.min()) - Kguard * dlog_tem
        lam_hi   = float(obs_pix.max()) + Kguard * dlog_tem

        i_lo = int(np.searchsorted(tem_pix, lam_lo, side="left"))
        i_hi = int(np.searchsorted(tem_pix, lam_hi, side="right"))
        i_lo = max(0, i_lo)
        i_hi = min(T, i_hi)
        if i_hi - i_lo < 2:
            raise ValueError(
                "Templates do not span the required guard. "
                f"Need [{lam_lo:.6f}, {lam_hi:.6f}] in log-λ, but tem_pix runs "
                f"{tem_pix[0]:.6f}..{tem_pix[-1]:.6f}. Provide a wider template "
                "grid."
            )

        clipped_left  = (i_lo == 0) and (tem_pix[0] > lam_lo)
        clipped_right = (i_hi == T) and (tem_pix[-1] < lam_hi)
        if clipped_left or clipped_right:
            eff_lo_px = int(round(max(0.0,
                            (obs_pix.min() - tem_pix[0]) / dlog_tem)))
            eff_hi_px = int(round(max(0.0,
                            (tem_pix[-1] - obs_pix.max()) / dlog_tem)))
            raise ValueError(
                "Template guard insufficient after cropping: "
                f"effective guard (px) left={eff_lo_px}, right={eff_hi_px}, "
                f"required={Kguard}."
            )

        tem_pix_c   = tem_pix[i_lo:i_hi].copy()
        templates_c = templates_PT[:, i_lo:i_hi].copy()
        T_c = int(tem_pix_c.size)
        if T_c < 2:
            raise ValueError("Cropped template grid too small.")

        # ---------- build rebin operator for cropped grid ----------
        R_T = self._build_R_T_dense(tem_pix_c, obs_pix)  # (T_c, L), float32

        # ---------- per-pixel metadata validation ----------
        nPix = None
        if xpix is not None:
            xpix = np.asarray(xpix).ravel()
            nPix = int(xpix.size)
        if ypix is not None:
            ypix = np.asarray(ypix).ravel()
            nPix_y = int(ypix.size)
            if nPix is None:
                nPix = nPix_y
            elif nPix_y != nPix:
                raise RuntimeError(f"/XPix length ({nPix}) and /YPix length "
                                f"({nPix_y}) mismatch.")
        if binnum is not None:
            bn = np.asarray(binnum).ravel()
            nPix_b = int(bn.size)
            if nPix is None:
                nPix = nPix_b
            elif nPix_b != nPix:
                raise RuntimeError(f"/BinNum length ({nPix_b}) and /XPix length "
                                f"({nPix}) mismatch.")
            if not np.issubdtype(bn.dtype, np.integer):
                raise ValueError("binnum must be integer indices mapping pixels "
                                "to spatial bins [0..S-1].")
            if bn.size == 0:
                raise ValueError("binnum must be non-empty if provided.")
            if (bn.min() < 0) or (bn.max() >= S):
                raise ValueError("binnum contains out-of-range indices; expected "
                                f"in [0, {S-1}].")

        # ---------- write everything ----------
        with self._open_rw() as f:
            def _write(name, data, **create_kw):
                if name in f:
                    del f[name]
                return f.create_dataset(name, data=data, **create_kw)

            # grids
            _write("/TemPix", tem_pix_c, dtype=np.float64)
            _write("/ObsPix", obs_pix,   dtype=np.float64)
            _write("/VelPix", vel_pix,   dtype=np.float64)

            # core arrays
            _write("/DataCube",
                datacube_SL.astype(np.float64, copy=False),
                dtype=np.float64)  # (S,L)
            _write("/LOSVD",
                losvd_in.astype(np.float64, copy=False),
                dtype=np.float64)  # (S,V,C)

            Tds = _write("/Templates",
                        templates_c.astype(np.float64, copy=False),
                        dtype=np.float64)  # (P, T_c)

            if mask is not None:
                mask = np.asarray(mask, dtype=bool).ravel()
                if mask.size != L_obs:
                    raise ValueError(f"mask length {mask.size} != L {L_obs}.")
                frac_true = float(np.mean(mask))
                logger.log(
                    f"[Mask] True==keep; fraction kept = {frac_true:.3f}")
                _write("/Mask", mask, dtype=np.bool_)
            if x_init is not None:
                x_init = np.asarray(x_init, dtype=np.float64).ravel()
                if x_init.size != C * P:
                    raise ValueError(f"x_init length {x_init.size} != C*P={C*P}.")
                _write("/X_global", x_init, dtype=np.float64)

            # operators
            _write("/R_T", R_T, dtype=np.float32)

            # dims (post-crop)
            dims = dict(nSpat=S, nLSpec=L_obs, nTSpec=T_c, nVel=V, nComp=C,
                        nPop=P)
            self._write_dims_attrs(f, dims)

            # template metadata (reconstruction + crop)
            Tds.attrs["orig_shape"]  = np.asarray(templates_in_shape,
                                                dtype=np.int64)
            Tds.attrs["orig_t_axis"] = np.int64(orig_t_axis)
            Tds.attrs["pop_shape"]   = np.asarray(pop_shape, dtype=np.int64)
            Tds.attrs["crop_i_lo"]   = np.int64(i_lo)
            Tds.attrs["crop_i_hi"]   = np.int64(i_hi)
            Tds.attrs["T_len_in"]    = np.int64(T)
            Tds.attrs["T_len_out"]   = np.int64(T_c)

            # guard metadata
            guard_info = dict(
                Kguard_px=int(Kguard),
                dlog_tem=float(dlog_tem),
                obs_lo=float(obs_pix.min()),
                obs_hi=float(obs_pix.max()),
                tem_lo=float(tem_pix_c.min()),
                tem_hi=float(tem_pix_c.max()),
                safety_pad_px=int(safety_pad_px),
            )
            self._write_guard_attrs(f, guard_info)

            # ---------- NEW: write per-pixel metadata (only if provided) ----------
            if xpix is not None:
                self._write_1d(
                    f, "/XPix", xpix, np.float64,
                    units="pixel or arcsec (user-supplied)",
                    semantic="image-plane X coordinate per detector pixel",
                )
            if ypix is not None:
                self._write_1d(
                    f, "/YPix", ypix, np.float64,
                    units="pixel or arcsec (user-supplied)",
                    semantic="image-plane Y coordinate per detector pixel",
                )
            if binnum is not None:
                self._write_1d(
                    f, "/BinNum", binnum, np.int32,
                    units="index",
                    semantic="pixel→spatial-bin mapping [0..S-1]",
                )

            # ---------- compute & store /HyperCube/data_flux (S,) -----------
            # Per-spaxel mean over unmasked wavelengths (λ). Masked λ are ignored.
            if "/HyperCube" not in f:
                f.create_group("/HyperCube")

            if mask is None:
                with np.errstate(invalid="ignore"):
                    L_vec = np.nanmean(datacube_SL, axis=1).astype(np.float64)  # (S,)
            else:
                use = np.asarray(mask, dtype=bool).ravel()
                if use.size != L_obs:
                    raise ValueError(f"mask length {use.size} != L {L_obs}.")
                if np.any(use):
                    with np.errstate(invalid="ignore"):
                        L_vec = np.nanmean(datacube_SL[:, use], axis=1).astype(np.float64)
                else:
                    L_vec = np.zeros(S, dtype=np.float64)

            L_vec[~np.isfinite(L_vec)] = 0.0

            if "/HyperCube/data_flux" in f:
                del f["/HyperCube/data_flux"]
            ds = f.create_dataset("/HyperCube/data_flux", data=L_vec, dtype=np.float64)
            ds.attrs["semantic"] = "per-spaxel mean flux over unmasked λ"
            ds.attrs["source"] = "/DataCube + /Mask"
            ds.attrs["stat"] = "mean"

            # ---------- write /CompWeights if provided ----------
            if orbit_weights is not None:
                w = np.asarray(orbit_weights, dtype=np.float64).ravel(order="C")
                if w.size not in (C, C * P):
                    raise ValueError(
                        f"orbit_weights length must be C ({C}) or C*P ({C*P}); got {w.size}."
                    )
                if w.size == C * P:
                    w = w.reshape(C, P, order="C").sum(axis=1)  # reduce to component level

                # unit-sum normalize for canonical storage
                s = float(np.sum(w))
                w = (w / np.maximum(s, 1.0e-30)) if s > 0.0 else np.zeros_like(w)

                if "/CompWeights" in f:
                    del f["/CompWeights"]
                ds_w = f.create_dataset("/CompWeights", data=w, dtype=np.float64)
                ds_w.attrs["semantic"] = "prior component weights w_c (unit-sum)"
                ds_w.attrs["normalized"] = bool(np.isclose(np.sum(w), 1.0))

        return dict(nSpat=S, nLSpec=L_obs, nTSpec=T_c, nVel=V, nComp=C, nPop=P)

    # -------------------------- Spectral grids & rebin ------------------------

    def set_spectral_grids(self, tem_pix: np.ndarray,
                           obs_pix: np.ndarray) -> Tuple[int, int]:
        """
        Persist spectral grids (/TemPix, /ObsPix) and a canonical /LamRange
        inferred from tem_pix. Then compute and persist /RebinMatrix and /R_T.

        Returns (N_src, nL) where:
          N_src = len(tem_pix)  (template time-domain length)
          nL    = len(obs_pix)  (observed spectrum length)
        """
        self._store_pix(tem_pix, obs_pix)
        N_src, nL = self.ensure_rebin()
        return N_src, nL

    def _store_pix(self, tem_pix: np.ndarray, obs_pix: np.ndarray) -> None:
        """
        Store spectral grids and canonical wavelength range derived from
        template centers. Idempotent.
        """
        tem_pix = np.asarray(tem_pix, dtype=np.float64).ravel()
        obs_pix = np.asarray(obs_pix, dtype=np.float64).ravel()
        if tem_pix.ndim != 1 or tem_pix.size < 2:
            raise ValueError("tem_pix must be 1-D with length >= 2.")
        if obs_pix.ndim != 1 or obs_pix.size < 2:
            raise ValueError("obs_pix must be 1-D with length >= 2.")
        if not (np.all(np.diff(tem_pix) > 0) and np.all(np.diff(obs_pix) > 0)):
            raise ValueError("tem_pix and obs_pix must be strictly increasing.")

        tem_edges = _bin_edges_from_centers(tem_pix)
        lam_lo, lam_hi = float(tem_edges[0]), float(tem_edges[-1])

        with self._open_rw() as f:
            def put1d(name, data):
                data = np.asarray(data, dtype=np.float64).ravel()
                if name in f:
                    ds = f[name]
                    if tuple(ds.shape) == (data.size,) and str(ds.dtype) == "float64":
                        ds[...] = data
                        return
                    del f[name]
                f.create_dataset(
                    name, data=data, dtype="f8", chunks=(data.size,),
                    compression=self.compression, compression_opts=self.clevel,
                    shuffle=self.shuffle
                )

            put1d("/TemPix", tem_pix)
            put1d("/ObsPix", obs_pix)

            lam_range = np.array([lam_lo, lam_hi], dtype=np.float64)
            if "/LamRange" in f:
                ds = f["/LamRange"]
                if tuple(ds.shape) == (2,) and str(ds.dtype) == "float64":
                    # Only rewrite if noticeably different
                    if not np.allclose(ds[...], lam_range, rtol=0, atol=1e-12):
                        ds[...] = lam_range
                else:
                    del f["/LamRange"]
                    f.create_dataset("/LamRange", data=lam_range, dtype="f8",
                                     chunks=(2,))
            else:
                f.create_dataset("/LamRange", data=lam_range, dtype="f8",
                                 chunks=(2,))

        logger.log(f"[H5Manager] Stored /TemPix(N={tem_pix.size}), "
                   f"/ObsPix(N={obs_pix.size}), "
                   f"/LamRange=({lam_lo:.6f},{lam_hi:.6f}).")

    def set_velocity_grid(self, vel_pix: np.ndarray) -> int:
        """
        Persist /VelPix (km/s), validate monotonicity, and return nVel.

        vel_pix : 1-D array of velocity-bin centers (km/s) corresponding to
                  the second axis of /LOSVD (shape: nVel).
        """
        v = np.asarray(vel_pix, dtype=np.float64).ravel()
        if v.size < 2:
            raise ValueError("vel_pix must be 1-D with length >= 2")
        if not (np.all(np.isfinite(v)) and np.all(np.diff(v) > 0)):
            raise ValueError("vel_pix must be strictly increasing and finite")

        with self._open_rw() as f:
            if "/VelPix" in f:
                ds = f["/VelPix"]
                if ds.shape == (v.size,) and str(ds.dtype) == "float64":
                    ds[...] = v
                else:
                    del f["/VelPix"]
                    f.create_dataset("/VelPix", data=v, dtype="f8",
                                     chunks=(min(4096, v.size),))
            else:
                f.create_dataset("/VelPix", data=v, dtype="f8",
                                 chunks=(min(4096, v.size),))
            f["/VelPix"].attrs["units"] = "km/s"
            f["/VelPix"].attrs["center_definition"] = "bin_centers"

        return int(v.size)

    def ensure_rebin(self) -> Tuple[int, int]:
        """
        Ensure /RebinMatrix (nL, N_src) and /R_T (N_src, nL) exist, using the
        exact flux-conserving (bin-overlap) rebin that matches linRebin.

        Uses /LamRange (from template grid), and the lengths of /TemPix and
        /ObsPix to determine sizes. Idempotent.

        Returns (N_src, nL).
        """
        with self._open_rw() as f:
            for req in ("/LamRange", "/TemPix", "/ObsPix"):
                if req not in f:
                    raise RuntimeError(
                        f"Missing {req}. Call set_spectral_grids(...) first."
                    )

            lam_lo, lam_hi = map(float, f["/LamRange"][...])
            N_src = int(f["/TemPix"].shape[0])
            nL = int(f["/ObsPix"].shape[0])

            need_R = ("/RebinMatrix" not in f or
                      tuple(f["/RebinMatrix"].shape) != (nL, N_src) or
                      str(f["/RebinMatrix"].dtype) != "float64")
            need_RT = ("/R_T" not in f or
                       tuple(f["/R_T"].shape) != (N_src, nL) or
                       str(f["/R_T"].dtype) != "float64")

            if need_R or need_RT:
                R = _build_linrebin_matrix_from_range((lam_lo, lam_hi),
                                                      N_src, nL)
                if need_R:
                    if "/RebinMatrix" in f:
                        del f["/RebinMatrix"]
                    f.create_dataset(
                        "/RebinMatrix", data=R, dtype="f8",
                        chunks=(min(1024, nL), N_src),
                        compression=self.compression, compression_opts=self.clevel,
                        shuffle=self.shuffle
                    )
                if need_RT:
                    if "/R_T" in f:
                        del f["/R_T"]
                    f.create_dataset(
                        "/R_T", data=R.T, dtype="f8",
                        chunks=(min(4096, N_src), nL),
                        compression=self.compression, compression_opts=self.clevel,
                        shuffle=self.shuffle
                    )
                logger.log(f"[H5Manager] Computed /RebinMatrix({nL},{N_src}) "
                           "and /R_T.")
            else:
                logger.log("[H5Manager] Rebin already present; reusing.")

            return N_src, nL

    def ensure_rebin_and_resample(self) -> tuple[int, int, int]:
        """
        Ensure /R_T, /RebinMatrix and the template FFT caches exist and match current grids.

        Returns
        -------
        (P, T, L): population count, template length (cropped), observed length
        """

        with self._open_rw() as f:
            # ---- required grids ----
            if "/TemPix" not in f or "/ObsPix" not in f:
                raise RuntimeError("Missing /TemPix or /ObsPix; call set_spectral_grids(...) first.")
            tem_pix = np.asarray(f["/TemPix"][...], dtype=np.float64)   # (T,)
            obs_pix = np.asarray(f["/ObsPix"][...], dtype=np.float64)   # (L,)
            T = int(tem_pix.size)
            L = int(obs_pix.size)

            # ---- rebin operator(s): /R_T (T,L), /RebinMatrix (L,T) ----
            need_rt = ("/R_T" not in f) or (f["/R_T"].shape != (T, L))
            need_rm = ("/RebinMatrix" not in f) or (f["/RebinMatrix"].shape != (L, T))
            if need_rt or need_rm:
                R_T = self._build_R_T_dense(tem_pix, obs_pix)  # (T,L) float32
                if need_rt:
                    if "/R_T" in f:
                        del f["/R_T"]
                    f.create_dataset("/R_T", data=R_T, dtype=np.float32)
                if need_rm:
                    if "/RebinMatrix" in f:
                        del f["/RebinMatrix"]
                    f.create_dataset("/RebinMatrix", data=R_T.T, dtype=np.float32)
                # optional: keep them in sync if only one existed but was mismatched
            # sanity (after potential writes)
            R_T = f["/R_T"]
            assert R_T.shape == (T, L)

            # ---- templates present? ----
            if "/Templates" not in f:
                raise RuntimeError("Missing /Templates; call populate_from_arrays(...) first.")
            Tds = f["/Templates"]
            P, T_templates = map(int, Tds.shape)
            if T_templates != T:
                # If someone changed /TemPix after writing /Templates, fix is to re-run populate_from_arrays.
                # We continue using the templates' spectral length for FFTs, but warn via shape mismatch.
                T = T_templates  # keep internal consistency for FFT build below

            # ---- template FFT caches ----
            nfreq = T // 2 + 1  # rfft length
            need_fft  = ("/TemplatesFFT"    not in f) or (f["/TemplatesFFT"].shape    != (P, nfreq))
            need_fftR = ("/TemplatesFFT_R"  not in f) or (f["/TemplatesFFT_R"].shape  != (P, nfreq))

            if need_fft or need_fftR:
                # load templates (P,T) as float64 for numerical stability
                T_mat = np.asarray(Tds[...], dtype=np.float64, order="C")
                F  = np.fft.rfft(T_mat, n=T, axis=1)             # (P, nfreq) complex128
                FR = np.fft.rfft(T_mat[:, ::-1], n=T, axis=1)    # reversed for true convolution

                if need_fft:
                    if "/TemplatesFFT" in f:
                        del f["/TemplatesFFT"]
                    f.create_dataset("/TemplatesFFT", data=F.astype(np.complex64, copy=False))

                if need_fftR:
                    if "/TemplatesFFT_R" in f:
                        del f["/TemplatesFFT_R"]
                    f.create_dataset("/TemplatesFFT_R", data=FR.astype(np.complex64, copy=False))

        # Return basic dims
        return (P, T, L)

# --------------------------------------------------------------------------- #
# Convenience wrappers
# ------------------------------------------------------------------------------

def populate_base_from_arrays(base_h5: str | Path,
                              *,
                              losvd: np.ndarray,
                              datacube: np.ndarray,
                              templates: np.ndarray,
                              mask: Optional[np.ndarray] = None,
                              x_init: Optional[np.ndarray] = None,
                              tem_pix: Optional[np.ndarray] = None,
                              obs_pix: Optional[np.ndarray] = None,
                              vel_pix: Optional[np.ndarray] = None) -> H5Dims:
    """
    Module-level convenience wrapper for H5Manager.populate_from_arrays.
    """
    mgr = H5Manager(base_h5)
    return mgr.populate_from_arrays(
        losvd=losvd, datacube=datacube, templates=templates,
        mask=mask, x_init=x_init,
        tem_pix=tem_pix, obs_pix=obs_pix, vel_pix=vel_pix
    )

def set_spectral_grids(base_h5: str | Path,
                       tem_pix: np.ndarray,
                       obs_pix: np.ndarray) -> Tuple[int, int]:
    """
    Module-level wrapper to persist spectral grids and build rebin artifacts.
    """
    mgr = H5Manager(base_h5)
    return mgr.set_spectral_grids(tem_pix, obs_pix)

# ------------------------------------------------------------------------------

def recompress_hypercube_models(
    base_h5: str | Path,
    *,
    dataset: str = "/HyperCube/models",
    compression: str = "gzip", # or "lzf"
    compression_opts: Optional[int] = 4, # gzip level (1–9), None = lib default
    shuffle: bool = True,
    chunk_overrides: Optional[Tuple[int,int,int,int]] = None,  # (S,C,P,L)
    temp_name: str = "models_tmp",
    keep_backup: bool = False,
) -> dict:
    """
    Re-encode `/HyperCube/models` with new filters by copying chunk-by-chunk to a
    sibling dataset, then swapping names. Keeps a **single writer** open, in line
    with open_h5(...) usage elsewhere. File size on disk may not shrink until you
    run `h5repack` (see compact_file_via_h5repack below).

    Returns a dict with before/after chunking and (if available) on-disk sizes.
    """
    start = time.time()
    base_h5 = Path(base_h5)

    with open_h5(base_h5, "writer") as f:
        if dataset not in f:
            raise RuntimeError(f"{dataset} not found in {base_h5}")
        old = f[dataset]
        parent = old.parent  # /HyperCube

        shape = tuple(int(x) for x in old.shape)           # (S,C,P,L)
        dtype = old.dtype
        old_chunks = old.chunks or (shape[0], 1, min(64, shape[2]), shape[3])
        chunks = tuple(chunk_overrides) if chunk_overrides else old_chunks

        # fresh temp
        if temp_name in parent:
            del parent[temp_name]

        kwargs = dict(shape=shape, dtype=dtype, chunks=chunks)
        if compression not in (None, "none", 0, False):
            kwargs.update(compression=compression,
                          compression_opts=compression_opts,
                          shuffle=bool(shuffle))
        new = parent.create_dataset(temp_name, **kwargs)

        # copy attributes
        for k, v in old.attrs.items():
            new.attrs[k] = v

        # chunked copy using old chunk grid (minimize read amplification)
        S_ch, C_ch, P_ch, L_ch = old_chunks
        for s0 in range(0, shape[0], S_ch):
            s1 = min(s0 + S_ch, shape[0])
            for c0 in range(0, shape[1], C_ch):
                c1 = min(c0 + C_ch, shape[1])
                for p0 in range(0, shape[2], P_ch):
                    p1 = min(p0 + P_ch, shape[2])
                    new[s0:s1, c0:c1, p0:p1, :] = old[s0:s1, c0:c1, p0:p1, :]

        # atomically swap; optionally keep backup
        backup_name = None
        if keep_backup:
            backup_name = f"{Path(dataset).name}_backup_{int(time.time())}"
            parent.move(Path(dataset).name, backup_name)
        else:
            del parent[Path(dataset).name]
        parent.move(temp_name, Path(dataset).name)

        # try to report storage sizes
        old_size = None
        new_size = None
        try:
            if backup_name:
                old_size = parent[backup_name].id.get_storage_size()
            new_size = parent[Path(dataset).name].id.get_storage_size()
        except Exception:
            pass

        return {
            "dataset": dataset,
            "shape": shape,
            "dtype": str(dtype),
            "chunks_old": tuple(old_chunks),
            "chunks_new": tuple(chunks),
            "compression": compression,
            "compression_opts": compression_opts,
            "shuffle": bool(shuffle),
            "old_size_bytes": old_size,
            "new_size_bytes": new_size,
            "elapsed_sec": time.time() - start,
            "backup_dataset": backup_name,
        }

def compact_file_via_h5repack(path: str | Path) -> Path:
    """
    Physically compact an HDF5 file by rewriting it with `h5repack`.
    All handles to the file must be closed before calling this function.
    """
    path = Path(path)
    tmp = path.with_suffix(path.suffix + ".repacked")
    try:
        subprocess.run(["h5repack", "-i", str(path), "-o", str(tmp), "-v"], check=True)
    except FileNotFoundError:
        raise RuntimeError("h5repack not found in PATH; cannot compact file.")
    except subprocess.CalledProcessError as e:
        raise RuntimeError(f"h5repack failed: {e}")
    tmp.replace(path)
    return path

# ------------------------------------------------------------------------------

def _get_obs_grid(f) -> np.ndarray:
    L = int(f["/DataCube"].shape[1])
    if "/ObsPix" in f:
        lam = np.asarray(f["/ObsPix"][...], dtype=np.float64).reshape(L)
    else:
        lam = np.arange(L, dtype=np.float64)
    return lam  # (L,)

def _get_tem_grid(f) -> np.ndarray:
    if "/Templates" not in f:
        raise RuntimeError("Missing /Templates")
    T = int(f["/Templates"].shape[1])
    if "/TemPix" in f:
        lamT = np.asarray(f["/TemPix"][...], dtype=np.float64).reshape(T)
    else:
        lamT = np.arange(T, dtype=np.float64)
    return lamT  # (T,)

def _get_vel_grid(f, V: int) -> np.ndarray:
    # Try common names; otherwise index grid
    for k in ("/VelGrid", "/VelPix", "/VelocityGrid"):
        if k in f:
            vg = np.asarray(f[k][...], dtype=np.float64).reshape(V)
            return vg
    return np.arange(V, dtype=np.float64)

def _load_mask(f, L: int) -> Optional[np.ndarray]:
    if "/Mask" in f:
        m = np.asarray(f["/Mask"][...], dtype=bool).reshape(L)
        return m  # True = keep (reader semantics)
    return None

def read_observed_spectrum(base_h5: str | Path, s: int, *, apply_mask: bool = True) -> Tuple[np.ndarray, np.ndarray]:
    """Return (λ_obs, y_s)."""
    with open_h5(base_h5, "reader") as f:
        Y = f["/DataCube"]
        L = int(Y.shape[1])
        lam = _get_obs_grid(f)
        y = np.asarray(Y[s, :], dtype=np.float64, order="C")
        if apply_mask:
            m = _load_mask(f, L)
            if m is not None:
                lam, y = lam[m], y[m]
        return lam, y

def read_template_spectrum(
    base_h5: str | Path, p: int, *,
    rebin_to_obs: bool = True,
    apply_mask: bool = True
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Return a single template spectrum p.
      If rebin_to_obs=False: (λ_tem, T_native) on native grid (T).
      If rebin_to_obs=True:  (λ_obs,  T_obs)   on observed grid (L) using /RebinMatrix or /R_T.
    """
    with open_h5(base_h5, "reader") as f:
        Tds = f["/Templates"]                # (P, T)
        T_native = np.asarray(Tds[p, :], dtype=np.float64, order="C")  # (T,)
        if not rebin_to_obs:
            return _get_tem_grid(f), T_native

        # Rebin to observed grid (L)
        L = int(f["/DataCube"].shape[1])
        lam_obs = _get_obs_grid(f)
        # Try /RebinMatrix first, else /R_T; handle orientation automatically
        T = T_native.shape[0]
        T_obs = None
        if "/RebinMatrix" in f:
            R = np.asarray(f["/RebinMatrix"][...])
            if R.shape == (L, T):
                T_obs = (R @ T_native)
            elif R.shape == (T, L):
                T_obs = (T_native @ R)
        if T_obs is None and "/R_T" in f:
            RT = np.asarray(f["/R_T"][...])
            if RT.shape == (T, L):
                T_obs = (T_native @ RT)
            elif RT.shape == (L, T):
                T_obs = (RT @ T_native)
        if T_obs is None:
            raise RuntimeError("No compatible /RebinMatrix or /R_T found for rebinning.")
        T_obs = np.asarray(T_obs, dtype=np.float64, order="C")

        if apply_mask:
            m = _load_mask(f, L)
            if m is not None:
                lam_obs, T_obs = lam_obs[m], T_obs[m]
        return lam_obs, T_obs

def read_losvd(base_h5: str | Path, s: int, c: int) -> Tuple[np.ndarray, np.ndarray]:
    """Return (v_grid, LOSVD_{s,c}(v))."""
    with open_h5(base_h5, "reader") as f:
        LSV = f["/LOSVD"]                    # (S, V, C)
        V = int(LSV.shape[1])
        vgrid = _get_vel_grid(f, V)
        los = np.asarray(LSV[s, :, c], dtype=np.float64, order="C")
        return vgrid, los

def read_model_basis(base_h5: str | Path, s: int, c: int, p: int, *, apply_mask: bool = True) -> Tuple[np.ndarray, np.ndarray]:
    """Return (λ_obs, basis) where basis = models[s, c, p, :] — post‑convolution basis for that (s,c,p)."""
    with open_h5(base_h5, "reader") as f:
        M = f["/HyperCube/models"] # (S, C, P, L)
        if M.ndim != 4:
            raise RuntimeError("/HyperCube/models must be rank-4 (S,C,P,L)")
        L = int(M.shape[3])
        lam = _get_obs_grid(f)
        basis = np.asarray(M[s, c, p, :], dtype=np.float64, order="C")
        if apply_mask:
            m = _load_mask(f, L)
            if m is not None:
                lam, basis = lam[m], basis[m]
        return lam, basis

def reconstruct_model_spectrum(base_h5: str | Path, s: int, x_global: np.ndarray, *, apply_mask: bool = True) -> Tuple[np.ndarray, np.ndarray]:
    """
    Return (λ_obs, model_spectrum) for a given spaxel s and global weights x_global (length C*P).
    Uses the same contraction over (C,P) as in your reconstruction helper.  :contentReference[oaicite:12]{index=12}
    """
    with open_h5(base_h5, "reader") as f:
        M = f["/HyperCube/models"]           # (S, C, P, L)
        if M.ndim != 4:
            raise RuntimeError("/HyperCube/models must be rank-4 (S,C,P,L)")
        S, C, P, L = map(int, M.shape)
        lam = _get_obs_grid(f)
        x = np.asarray(x_global, dtype=np.float64).reshape(C, P)  # (C,P)
        slab = np.asarray(M[s, :, :, :], dtype=np.float64, order="C")  # (C,P,L)
        y_model = np.tensordot(slab, x, axes=([0, 1], [0, 1]))         # (L,)
        if apply_mask:
            m = _load_mask(f, L)
            if m is not None:
                lam, y_model = lam[m], y_model[m]
        return lam, y_model

# ------------------------------------------------------------------------------

def plot_prefit_panel(
    base_h5: str | Path,
    s: int, c: int, p: int,
    *,
    include_mix: bool = False,
    x_global: Optional[np.ndarray] = None,
):
    """
    2x2 panel for a given (s,c,p):
      (1) Observed y_s(λ)
      (2) Template T_p rebinned to observed grid (pre‑convolution)
      (3) LOSVD_{s,c}(v)
      (4) Post‑convolution basis models[s,c,p,:]
    If include_mix=True and x_global is provided, overlays reconstructed model A_s^T x.
    """
    galaxy = Path(base_h5).stem.split("_")[0]
    pDir = Path(base_h5).parent/galaxy/'figures'
    pDir.mkdir(parents=True, exist_ok=True)

    lam_obs, y = read_observed_spectrum(base_h5, s, apply_mask=True)
    lam_t,  T  = read_template_spectrum(base_h5, p, rebin_to_obs=True, apply_mask=True)
    v, los     = read_losvd(base_h5, s, c)
    lam_b, b   = read_model_basis(base_h5, s, c, p, apply_mask=True)

    fig, axs = plt.subplots(2, 2, figsize=(10, 7), constrained_layout=True)

    axs[0,0].plot(lam_obs, y)
    axs[0,0].set_title(f"Observed spectrum (s={s})")
    axs[0,0].set_xlabel("λ")
    axs[0,0].set_ylabel("Flux")

    axs[0,1].plot(lam_t, T)
    axs[0,1].set_title(f"Template p={p} (rebinned, pre‑conv)")
    axs[0,1].set_xlabel("λ")

    axs[1,0].plot(v, los)
    axs[1,0].set_title(f"LOSVD (s={s}, c={c})")
    axs[1,0].set_xlabel("velocity bin")

    axs[1,1].plot(lam_b, b, label=f"basis (s={s},c={c},p={p})")
    if include_mix and x_global is not None:
        lam_m, y_m = reconstruct_model_spectrum(base_h5, s, x_global, apply_mask=True)
        axs[1,1].plot(lam_m, y_m, linestyle="--", label="A_s^T x (mixture)")
        axs[1,1].legend()
    axs[1,1].set_title("Post‑convolution basis / mixture")
    axs[1,1].set_xlabel("λ")

    fig.savefig(pDir/'prefit')
    plt.close('all')

# ------------------------------------------------------------------------------

def _component_scale(f, s: int, c: int, eps: float = 1e-30) -> tuple[float, str, float]:
    """
    Compute the scale applied to (s,c) columns in /HyperCube/models.

    Parameters
    ----------
    f : h5py.File
        Open HDF5 handle.
    s : int
        Spaxel index.
    c : int
        Component index.
    eps : float, optional
        Numerical floor to avoid divide-by-zero.

    Returns
    -------
    scale_sc : float
        The multiplicative factor actually applied to every (p,λ) for this
        (s,c) in the models array.
    mode : str
        'data' or 'model' (the normalization mode).
    frac : float
        For data-mode only: A[s,c] / sum_c A[s,c]. For model-mode this is
        0.0 (unused).

    Examples
    --------
    >>> with open_h5(h5, 'reader') as f:
    ...     scale, mode, frac = _component_scale(f, 1033, 124)
    ...     print(scale, mode, frac)
    """
    mode = str(f["/HyperCube"].attrs.get("norm.mode", "model")).lower()
    A_sc = float(f["/HyperCube/norm/losvd_amp"][s, c])  # A[s,c]
    if mode == "data":
        a_sum = float(f["/HyperCube/norm/losvd_amp_sum"][s])  # Σ_c A[s,c]
        Ls = float(f["/HyperCube/data_flux"][s])              # masked mean data
        if a_sum <= 0.0 or Ls <= 0.0:
            return 0.0, mode, 0.0
        frac = A_sc / max(a_sum, eps)
        return Ls * frac, mode, frac
    else:
        return A_sc, mode, 0.0

# ------------------------------------------------------------------------------

def live_prefit_snapshot_from_models(
    h5_path: str,
    *,
    max_spaxels: int = 6,
    max_components: int = 2,
    max_templates: int = 4,         # used in TR panel
    max_sc_pairs: int = 3,          # NEW: number of (s,c) pairs in BR
    templates_per_pair: int = 2,    # NEW: curves per (s,c) in BR
    out_png: str = "prefit_live.png",
    shade_alpha: float = 0.28,
    q_lo: float = 0.02,
    q_hi: float = 0.98,
    pad_frac: float = 0.20,
    seed: int | None = None,
) -> str:
    """
    SWMR-safe diagnostic snapshot while the HyperCube build is running.

    Panels:
      TL  Observed spectra (prefer distinct S-tiles)
      TR  Templates rebinned (pre-convolution)
      BL  LOSVD (native) + resampled kernel
      BR  ACTUAL /HyperCube/models for several diverse (s,c) pairs

    - Masked pixels are shaded but do NOT influence autoscaling.
    - BR now overlays multiple (s,c) pairs from distinct tiles for better coverage.
    """
    rng = np.random.default_rng(seed)

    # ---------- helpers ----------
    def _true_runs(mask_bool_1d: np.ndarray):
        idx = np.flatnonzero(mask_bool_1d)
        if idx.size == 0:
            return []
        brk = np.where(np.diff(idx) != 1)[0] + 1
        starts = np.concatenate([idx[[0]], idx[brk]])
        ends   = np.concatenate([idx[brk - 1], idx[[-1]]])
        return list(zip(starts, ends))

    def _shade_mask(ax, lam_obs, mask_raw):
        if mask_raw is None:
            return
        lam = np.asarray(lam_obs, dtype=float).ravel()
        if lam.size < 2:
            return
        keep_mask = np.asarray(mask_raw, dtype=bool).ravel()   # True == keep
        bad_mask  = ~keep_mask                                  # shaded regions
        dlam = float(np.median(np.diff(lam)))
        half = abs(dlam) * 0.5
        x_min, x_max = float(lam[0]), float(lam[-1])
        for i0, i1 in _true_runs(bad_mask):
            left  = max(float(lam[max(i0, 0)])  - half, x_min)
            right = min(float(lam[min(i1, lam.size - 1)]) + half, x_max)
            if right <= left:
                right = min(left + abs(dlam), x_max)
            ax.axvspan(left, right, color="k", alpha=shade_alpha, lw=0, zorder=5)

    def _set_ylim_from_unmasked(ax, ys, mask_raw, qlo=q_lo, qhi=q_hi, pad=pad_frac):
        vals = []
        if mask_raw is None:
            for y in ys:
                z = np.asarray(y, dtype=float).ravel()
                if z.size:
                    vals.append(z[np.isfinite(z)])
        else:
            keep = np.asarray(mask_raw, dtype=bool).ravel()     # True == keep
            for y in ys:
                z = np.asarray(y, dtype=float).ravel()
                if z.size:
                    if keep.size == z.size:
                        vals.append(z[keep & np.isfinite(z)])
                    else:
                        vals.append(z[np.isfinite(z)])
        if not vals:
            return
        cat = np.concatenate([v for v in vals if v.size], dtype=float)
        if cat.size == 0:
            return
        y0 = float(np.quantile(cat, qlo)); y1 = float(np.quantile(cat, qhi))
        if not np.isfinite(y0) or not np.isfinite(y1) or y1 <= y0:
            y0 = float(np.nanmin(cat)); y1 = float(np.nanmax(cat))
            if not np.isfinite(y0) or not np.isfinite(y1):
                return
            if y1 == y0:
                y1 = y0 + 1.0
        pad_abs = pad * (y1 - y0 if y1 > y0 else 1.0)
        ax.set_ylim(y0 - pad_abs, y1 + pad_abs)

    # ---------- SWMR read ----------
    with h5py.File(h5_path, "r", libver="latest", swmr=True) as f:
        if "/HyperCube/models" not in f or "/HyperCube/_done" not in f:
            raise RuntimeError("Hypercube not initialized yet (missing /HyperCube/models or /HyperCube/_done).")

        lam_obs = np.asarray(f["/ObsPix"][...], dtype=float)
        tem_log = np.asarray(f["/TemPix"][...], dtype=float)
        vel_pix = np.asarray(f["/VelPix"][...], dtype=float)

        # Rebin orientation (T,L)
        R = np.asarray(f["/R_T"][...])
        if R.shape == (tem_log.size, lam_obs.size):
            R_T = R
        elif R.shape == (lam_obs.size, tem_log.size):
            R_T = R.T
        else:
            raise RuntimeError(f"Incompatible /R_T shape {R.shape} vs T={tem_log.size}, L={lam_obs.size}")

        mask = None
        if "/Mask" in f:
            m = np.asarray(f["/Mask"][...]).ravel()
            mask = np.asarray(m != 0, dtype=bool)

        M    = f["/HyperCube/models"]   # (S,C,P,L)
        DONE = f["/HyperCube/_done"]    # (Sgrid,Cgrid,Pgrid)

        for obj in (M, DONE):
            try:
                obj.refresh()
            except Exception:
                try:
                    obj.id.refresh()
                except Exception:
                    pass

        S, C, P, L = map(int, M.shape)
        s_chunk, c_chunk, p_chunk, l_chunk = M.chunks
        done_arr = np.asarray(DONE[...], dtype=bool)

        sgrid = int(np.ceil(S / s_chunk))
        cgrid = int(np.ceil(C / c_chunk))
        pgrid = int(np.ceil(P / p_chunk))
        if done_arr.shape[:3] != (sgrid, cgrid, pgrid):
            sgrid, cgrid, pgrid = done_arr.shape[:3]

        # --- Prefer diverse S-tiles for TL/BR selections
        good_si = np.where(done_arr.any(axis=(1, 2)))[0]
        if good_si.size == 0:
            raise RuntimeError("No completed tiles yet; try again later.")

        def srange(si):
            s0 = int(si * s_chunk); s1 = int(min(S, s0 + s_chunk))
            return s0, s1

        # TL spaxels: try one from each of as many distinct S-tiles as possible
        rng.shuffle(good_si)
        spaxel_list = []
        for si in good_si:
            s0, s1 = srange(si)
            if s1 > s0:
                spaxel_list.append(int(rng.integers(s0, s1)))
            if len(spaxel_list) >= max_spaxels:
                break
        # fallback: fill from first tile if we still need more
        if len(spaxel_list) < max_spaxels:
            s0, s1 = srange(good_si[0])
            need = max_spaxels - len(spaxel_list)
            extras = rng.choice(np.arange(s0, s1), size=min(need, s1 - s0), replace=False)
            spaxel_list.extend(map(int, extras))
        sp_sel = np.array(spaxel_list[:max_spaxels], dtype=int)

        # TR: choose some templates (global)
        TDS = f["/Templates"]
        p_pool = np.arange(P)
        rng.shuffle(p_pool)
        p_sel_tr = np.array(sorted(p_pool[:min(max_templates, P)]), dtype=int)
        T_sel = np.asarray(TDS[p_sel_tr, :], dtype=float)
        # SAFE (BLAS-free) path: (B,T) x (T,L) -> (B,L) via broadcast & sum
        TR_mat = np.sum(T_sel[:, :, None] * R_T[None, :, :], axis=1, dtype=np.float64)

        # BL: components for LOSVD variety
        co_pool = np.arange(C)
        rng.shuffle(co_pool)
        co_sel = np.array(sorted(co_pool[:min(max_components, C)]), dtype=int)

        LOS = f["/LOSVD"]  # (S,V,C)
        def _has_signal(s_idx: int, c_idx: int) -> bool:
            row = np.asarray(LOS[s_idx, :, c_idx], dtype=float)
            return np.isfinite(row).any() and float(row.sum()) > 0.0

        # --- BR: build diverse (s,c) PAIRS from distinct tiles with any P done
        # boolean grid of finished (si,ci) pairs
        done_si_ci = done_arr.any(axis=2)            # (sgrid, cgrid)
        pairs = np.argwhere(done_si_ci)              # list of [si, ci]
        rng.shuffle(pairs)

        sc_pairs = []
        used_si, used_ci = set(), set()

        # pass 1: prefer new si and new ci
        for si, ci in pairs:
            if si not in used_si and ci not in used_ci:
                # pick a concrete s, c index inside those tiles
                s0, s1 = srange(int(si))
                if s1 <= s0:
                    continue
                s_idx = int(rng.integers(s0, s1))
                c_idx = int(min(C - 1, int(ci) * c_chunk + rng.integers(0, max(1, c_chunk))))
                if not _has_signal(s_idx, c_idx):
                    continue
                sc_pairs.append((s_idx, c_idx, int(si), int(ci)))
                used_si.add(int(si)); used_ci.add(int(ci))
                if len(sc_pairs) >= max_sc_pairs:
                    break

        # pass 2: fill remaining slots, relaxing the uniqueness
        if len(sc_pairs) < max_sc_pairs:
            for si, ci in pairs:
                s0, s1 = srange(int(si))
                if s1 <= s0:
                    continue
                s_idx = int(rng.integers(s0, s1))
                c_idx = int(min(C - 1, int(ci) * c_chunk + rng.integers(0, max(1, c_chunk))))
                if not _has_signal(s_idx, c_idx):
                    continue
                sc_pairs.append((s_idx, c_idx, int(si), int(ci)))
                if len(sc_pairs) >= max_sc_pairs:
                    break

        if len(sc_pairs) == 0:
            # Fallback: scan for any non-zero (s,c)
            for s_idx in range(S):
                for c_idx in range(C):
                    if _has_signal(s_idx, c_idx):
                        sc_pairs.append(
                            (s_idx, c_idx, int(s_idx // s_chunk), int(c_idx // c_chunk))
                        )
                        break
                if sc_pairs:
                    break
            if len(sc_pairs) == 0:
                raise RuntimeError("All candidate (s,c) have zero LOSVD mass.")

        # For each (si,ci), pick templates from its finished P-tiles
        pj_done = done_arr  # alias
        per_pair_curves = []
        for s_idx, c_idx, si, ci in sc_pairs:
            pj_tiles = np.where(pj_done[si, ci, :])[0]
            p_cands = []
            for pj in pj_tiles:
                p0 = int(pj * p_chunk); p1 = int(min(P, p0 + p_chunk))
                p_cands.extend(range(p0, p1))
            if not p_cands:
                p_cands = list(range(P))
            rng.shuffle(p_cands)
            p_sel_pair = list(sorted(p_cands[:min(templates_per_pair, len(p_cands))]))
            per_pair_curves.append((s_idx, c_idx, p_sel_pair))
        bl_pairs = [(s_idx, c_idx) for (s_idx, c_idx, _) in per_pair_curves]

        # ---------------- figure ----------------
        fig, axs = plt.subplots(2, 2, figsize=(12, 8), constrained_layout=True)

        # TL: observed spectra (diverse S-tiles)
        DC = f["/DataCube"]  # (S,L)
        Y_rows = [np.asarray(DC[s, :], dtype=float) for s in sp_sel]
        _set_ylim_from_unmasked(axs[0, 0], Y_rows, mask)
        for s, y in zip(sp_sel, Y_rows):
            axs[0, 0].plot(lam_obs, y, lw=1, label=f"s={int(s)}")
        axs[0, 0].set_title("Observed spectra (completed S-tiles)")
        axs[0, 0].set_xlabel("λ (observed grid)")
        axs[0, 0].set_ylabel("Flux")
        _shade_mask(axs[0, 0], lam_obs, mask)
        axs[0, 0].legend(fontsize=9)

        # TR: templates (rebinned, pre-convolution)
        _set_ylim_from_unmasked(axs[0, 1], [row for row in TR_mat], mask)
        for p, y in zip(p_sel_tr, TR_mat):
            axs[0, 1].plot(lam_obs, y, lw=1, label=f"p={int(p)}")
        axs[0, 1].set_title("Templates (rebinned, pre-convolution)")
        axs[0, 1].set_xlabel("λ (observed grid)")
        _shade_mask(axs[0, 1], lam_obs, mask)
        axs[0, 1].legend(fontsize=9)

        # BL: LOSVD (native) + resampled kernel (+ amplitude-scaled kernel on twin y)
        def _kernel_from_losvd(los_row, V):
            dlog = float(np.median(np.diff(tem_log)))
            k_min = int(np.floor(np.log1p(V.min() / C_KMS) / dlog))
            k_max = int(np.ceil (np.log1p(V.max() / C_KMS) / dlog))
            k_offsets = np.arange(k_min, k_max + 1, dtype=int)
            v_for_k  = C_KMS * np.expm1(k_offsets * dlog)
            il = np.searchsorted(V, v_for_k, side="right") - 1
            ir = il + 1
            oob = (il < 0) | (ir >= V.size)
            il = np.clip(il, 0, V.size - 1)
            ir = np.clip(ir, 0, V.size - 1)
            denom = (V[ir] - V[il]); denom[denom == 0.0] = 1.0
            t = (v_for_k - V[il]) / denom
            Hk = (1.0 - t) * los_row[il] + t * los_row[ir]
            Hk[oob] = 0.0
            s = Hk.sum()
            if s > 0:
                Hk /= s  # unit-area kernel for shape-only view
            return v_for_k, Hk

        los_handles = []
        LOS = f["/LOSVD"]  # (S,V,C)

        # twin y-axis for amplitude-scaled kernels (flux units)
        ax_los = axs[1, 0]
        ax_kflux = ax_los.twinx()
        ax_kflux.set_ylabel("kernel × scale(s,c)  (flux units)")

        # global norm mode (same for all components)
        norm_mode = str(f["/HyperCube"].attrs.get("norm.mode", "model")).lower()

        for (s_idx, c_idx) in bl_pairs:
            los = np.asarray(LOS[s_idx, :, c_idx], dtype=float)
            tot = los.sum()
            if tot > 0:
                los = los / tot  # unit-sum for native LOSVD display

            v_for_k, Hk = _kernel_from_losvd(los, vel_pix)
            # actual per-(s,c) scale used in /HyperCube/models
            scale_sc, mode_used, frac = _component_scale(f, int(s_idx), int(c_idx))

            # native LOSVD (solid)
            if norm_mode == "data":
                lbl = f"(s={int(s_idx)}, c={int(c_idx)})  frac={frac:.3e}"
            else:
                lbl = f"(s={int(s_idx)}, c={int(c_idx)})"
            line_los, = ax_los.plot(vel_pix, los, lw=1.5, label=lbl)
            los_handles.append(line_los)

            # unit-area resampled kernel (dashed) — same color as LOSVD
            ax_los.plot(v_for_k, Hk, lw=1.0, ls="--", color=line_los.get_color())

            # amplitude-scaled kernel on twin axis (dotted) — same color
            if scale_sc > 0.0:
                ax_kflux.plot(v_for_k, scale_sc * Hk, lw=1.0, ls=":",
                            alpha=0.9, color=line_los.get_color())

        ax_los.set_title(f"LOSVD (native) and resampled kernel  [mode={norm_mode}]")
        ax_los.set_xlabel("velocity (km/s)")
        ax_los.grid(alpha=0.25)

        # Style legend (explains line styles)
        lg_styles = ax_los.legend(
            handles=[
                Line2D([], [], lw=1.5, label="LOSVD (native)"),
                Line2D([], [], lw=1.0, ls="--", label="kernel (unit-area)"),
                Line2D([], [], lw=1.0, ls=":", label="kernel × scale(s,c)"),
            ],
            loc="lower right", fontsize=8, frameon=False
        )
        ax_los.add_artist(lg_styles)

        # Per-curve legend showing which (spaxel, component) each LOSVD is
        ax_los.legend(
            handles=los_handles, loc="upper left",
            fontsize=7, frameon=False, ncol=2, title="(s, c)"
        )

        # BR: ACTUAL /HyperCube/models for diverse (s,c) pairs
        br_all_rows = []
        for pair_id, (s_idx, c_idx, p_list) in enumerate(per_pair_curves, start=1):
            for p in p_list:
                y = np.asarray(M[s_idx, c_idx, int(p), :], dtype=float)
                br_all_rows.append(y)
        _set_ylim_from_unmasked(axs[1, 1], br_all_rows, mask)
        for pair_id, (s_idx, c_idx, p_list) in enumerate(per_pair_curves, start=1):
            for p in p_list:
                y = np.asarray(M[s_idx, c_idx, int(p), :], dtype=float)
                axs[1, 1].plot(
                    lam_obs, y, lw=1,
                    label=f"p={int(p)} @(s={s_idx}, c={c_idx})"
                )
        axs[1, 1].set_title("Post-convolution (ACTUAL /HyperCube/models)")
        axs[1, 1].set_xlabel("λ (observed grid)")
        _shade_mask(axs[1, 1], lam_obs, mask)
        axs[1, 1].legend(fontsize=8)

        fig.savefig(out_png, dpi=150)
        plt.close(fig)
        return out_png
</file>

<file path="hypercube_builder.py">
# -*- coding: utf-8 -*-
r"""
    hypercube_builder.py
    Adriano Poci
    University of Oxford
    2025

    Platforms
    ---------
    Unix, Windows

    Synopsis
    --------
    Manages Zarr data storage for CubeFit pipeline, including creation, loading,
    and validation of large, chunked arrays (templates, data cube, LOSVD, weights).
    Supports buffered template grids for safe convolution.

    Authors
    -------
    Adriano Poci <adriano.poci@physics.ox.ac.uk>

History
-------
v1.0:   Initial design and validation. 14 August 2025
v1.1:   Added Zarr v3 sharding support and safe-direct writes. 5 September 2025
v1.2:   Complete re-write to use HDF5. 7 September 2025
v1.3:   Improved speed of `col_energy` computation, switching from `einsum` to
            `square`. 4 December 2025


Hypercube builder (LOSVD convolution in log-λ, then rebin to observed grid)

Writes /HyperCube/models with shape (S, C, P, L) float32, chunked, resumable.
Requires the HDF5 to already contain:
  /Templates  (P, T) float64          -- flattened populations on native template grid
  /TemPix     (T,)   float64          -- template grid in log-λ (natural log)
  /ObsPix     (L,)   float64          -- observed wavelength grid (not used directly here)
  /R_T        (T,L) or (L,T)          -- rebin operator (stored by H5Manager)
  /LOSVD      (S,V,C) float64         -- per-spaxel LOSVD histograms
  /VelPix     (V,)   float64          -- velocity bins (km/s) corresponding to LOSVD axis
"""

from __future__ import annotations
from dataclasses import dataclass
from typing import Iterable, Optional, Dict, Any, List, Tuple
import os, math
import numpy as np
from tqdm import tqdm

from dynamics.IFU.Constants import Constants
from CubeFit.hdf5_manager import open_h5
from CubeFit.logger import get_logger

logger = get_logger()

CTS = Constants()
C_KMS = CTS.c

# ------------------------- small utilities ------------------------------------

class _P2Median:
    """
    Streaming P² median estimator (Jain & Chlamtac, 1985) with ~5 markers.

    Stores only O(1) state per tracked series; good accuracy for large S.
    """
    __slots__ = ("_n", "_q", "_nq", "_dn")

    def __init__(self) -> None:
        self._n = 0
        self._q = np.zeros(5, dtype=np.float64)
        self._nq = np.array([0, 1, 2, 3, 4], dtype=np.float64)
        self._dn = np.array([0.0, 0.5, 1.0, 1.5, 2.0], dtype=np.float64)

    def update(self, x: np.ndarray) -> None:
        """
        Vectorized: accept a batch of scalars as a 1D array.
        Initializes each series lazily on first five samples.
        """
        # For simplicity with many series, we call per-scalar here.
        # The outer code will call update() with scalars in a tight loop.
        for v in np.ravel(x):
            n = self._n
            if n < 5:
                self._q[n] = float(v)
                self._n += 1
                if self._n == 5:
                    self._q.sort()
                continue
            # Find cell k such that q[k] <= v < q[k+1]
            if v < self._q[0]:
                self._q[0] = v
                k = 0
            elif v >= self._q[4]:
                self._q[4] = v
                k = 3
            else:
                k = int(np.searchsorted(self._q, v)) - 1
            self._nq[:5] += self._dn
            self._nq[k+1:5] += 1.0
            # Desired marker positions for median (p=0.5)
            m = np.array([0, 0.5*(self._n), 1*self._n, 1.5*(self._n), 2*self._n],
                         dtype=np.float64) / 2.0
            self._n += 1
            # Adjust interior markers (1..3)
            for i in (1, 2, 3):
                d = m[i] - self._nq[i]
                if (d >= 1 and self._nq[i+1] - self._nq[i] > 1) or \
                   (d <= -1 and self._nq[i-1] - self._nq[i] < -1):
                    d = np.sign(d)
                    qi = self._q[i]
                    qip = self._q[i+1]
                    qim = self._q[i-1]
                    di = (self._q[i+1] - qi)/(self._nq[i+1] - self._nq[i]) if self._nq[i+1] != self._nq[i] else 0.0
                    dm = (qi - self._q[i-1])/(self._nq[i] - self._nq[i-1]) if self._nq[i] != self._nq[i-1] else 0.0
                    qnew = qi + d*((self._nq[i] - self._nq[i-1] + d)*dm +
                                   (self._nq[i+1] - self._nq[i] - d)*di) / \
                                   (self._nq[i+1] - self._nq[i-1])
                    # If monotonicity breaks, fall back to linear.
                    if not (qim <= qnew <= qip):
                        qnew = qi + d * (self._q[i + int(d)] - qi) / \
                               (self._nq[i + int(d)] - self._nq[i])
                    self._q[i] = qnew
                    self._nq[i] += d

    def median(self) -> float:
        n = self._n
        if n == 0:
            return 0.0
        if n <= 5:
            return float(np.median(self._q[:n]))
        return float(self._q[2])


def _ensure_mask_indices(f, L: int) -> Optional[np.ndarray]:
    """
    Return wavelength mask indices (int64) if /Mask exists and is same L.
    Otherwise None (use full λ range).
    """
    if "/Mask" in f:
        m = np.asarray(f["/Mask"][...], dtype=bool, order="C")
        if m.ndim == 1 and m.size == L:
            return np.nonzero(m)[0].astype(np.int64)
    return None

def _choose_nfft(T_len: int, k_support: int) -> int:
    """Linear conv length >= T + m - 1; pick power-of-two for speed."""
    L = T_len + k_support - 1
    return _next_pow2(L)

def _make_done_bitmap(f, S: int, C: int, P: int,
                      S_chunk: int, C_chunk: int, P_chunk: int):
    """
    Create or open the /HyperCube/_done bitmap with shape
    (ceil(S/S_chunk), ceil(C/C_chunk), ceil(P/P_chunk)).
    """
    g = f.require_group("/HyperCube")
    grid = (math.ceil(S / S_chunk), math.ceil(C / C_chunk), math.ceil(P / P_chunk))
    if "_done" in g:
        ds = g["_done"]
        if tuple(ds.shape) != grid:
            del g["_done"]
            ds = g.create_dataset("_done", shape=grid, dtype="u1", chunks=True)
    else:
        ds = g.create_dataset("_done", shape=grid, dtype="u1", chunks=True)
    return ds, grid

def _done_get(ds, idx3: Tuple[int, int, int]) -> int:
    try:
        return int(ds[idx3])
    except Exception:
        # some h5py builds dislike 3D scalar reads; slice as 1x1x1
        i, j, k = idx3
        return int(ds[i:i+1, j:j+1, k:k+1][0, 0, 0])

def _done_set(ds, idx3: Tuple[int, int, int]) -> None:
    i, j, k = idx3
    ds[i, j, k] = np.uint8(1)

def _iter_slices(total: int, step: int) -> Iterable[Tuple[int, int]]:
    x = 0
    while x < total:
        y = min(x + step, total)
        yield x, y
        x = y

# ------------------------- LOSVD → pixel-kernel mapping -----------------------

@dataclass(frozen=True)
class KernelMap:
    # Integer pixel offsets on the template grid where the kernel is sampled
    k_offsets: np.ndarray        # (m,) int64
    # For interpolation: left/right indices into VelPix and t in [0,1]
    il: np.ndarray               # (m,) int64
    ir: np.ndarray               # (m,) int64
    t: np.ndarray                # (m,) float64
    # Mask for offsets whose mapped velocity is outside VelPix range
    out_mask: np.ndarray         # (m,) bool
    # Length of kernel support m and centered index (k==0)
    m: int
    center_idx: int

# ------------------------- normalization helpers -----------------------------

def _ensure_norm_group(f):
    """
    Create and return /HyperCube/norm group. Never compress tiny norm arrays.
    """
    g = f.require_group("/HyperCube")
    ng = g.require_group("norm")
    return ng

def _compute_and_store_losvd_amplitudes(f, *, amp_mode: str) -> tuple[str, str]:
    """
    One streaming pass over /LOSVD to compute:
      A[s,c]     := LOSVD amplitude per (spaxel, component)
      A_sum[s]   := sum_c A[s,c]   (per-spaxel total)

    Stores datasets and returns their HDF5 paths.

    amp_mode:
      "sum"   -> amplitude = sum(H_native)
      "trapz" -> amplitude = ∫ H_native dV   (requires /VelPix)
    """
    S, V, C = map(int, f["/LOSVD"].shape)
    ng = _ensure_norm_group(f)

    if "losvd_amp" in ng:
        del ng["losvd_amp"]
    if "losvd_amp_sum" in ng:
        del ng["losvd_amp_sum"]

    A = ng.create_dataset("losvd_amp", shape=(S, C), dtype="f8",
                          chunks=(min(64, S), min(32, C)))
    A_sum = ng.create_dataset("losvd_amp_sum", shape=(S,), dtype="f8",
                              chunks=(min(64, S),))

    if amp_mode == "trapz":
        vel_pix = np.asarray(f["/VelPix"][...], np.float64)
    else:
        vel_pix = None

    # stream over spaxels to bound RAM
    losvd = f["/LOSVD"]
    step = max(1, 64)
    for s0 in range(0, S, step):
        s1 = min(S, s0 + step)
        slab = np.asarray(losvd[s0:s1, :, :], np.float64, order="C")  # (dS,V,C)
        if amp_mode == "trapz":
            a = np.trapz(slab, vel_pix, axis=1)                       # (dS,C)
        else:
            a = slab.sum(axis=1)                                       # (dS,C)
        A[s0:s1, :] = a
        A_sum[s0:s1] = a.sum(axis=1)
    try:
        A.id.flush(); A_sum.id.flush(); f.flush()
    except Exception:
        pass

    return (A.name, A_sum.name)

# ------------------------------------------------------------------------------
# ------------------------- canonical kernel/conv helpers ----------------------

def _next_pow2(n: int) -> int:
    return 1 << (int(n - 1).bit_length())

@dataclass(frozen=True)
class _KM:
    k_offsets: np.ndarray
    il: np.ndarray
    ir: np.ndarray
    t: np.ndarray
    out_mask: np.ndarray
    center_idx: int
    m: int
    dlog: float

def _kernel_map_from_grids(tem_loglam: np.ndarray,
                           vel_pix: np.ndarray) -> _KM:
    logL = np.asarray(tem_loglam, dtype=np.float64)
    dlog = float(np.median(np.diff(logL)))
    V = np.asarray(vel_pix, dtype=np.float64)

    k_min = int(np.floor(np.log1p(np.min(V) / C_KMS) / dlog))
    k_max = int(np.ceil (np.log1p(np.max(V) / C_KMS) / dlog))
    k_offsets = np.arange(k_min, k_max + 1, dtype=np.int64)

    v_for_k = C_KMS * np.expm1(k_offsets * dlog)

    il = np.searchsorted(V, v_for_k, side="right") - 1
    ir = il + 1
    out_mask = (il < 0) | (ir >= V.size)
    il = np.clip(il, 0, V.size - 1)
    ir = np.clip(ir, 0, V.size - 1)

    denom = (V[ir] - V[il]).astype(np.float64, copy=False)
    denom[denom == 0.0] = 1.0
    t = (v_for_k - V[il]) / denom

    center_idx = int(np.searchsorted(k_offsets, 0))

    return _KM(
        k_offsets=k_offsets,
        il=il.astype(np.int64, copy=False),
        ir=ir.astype(np.int64, copy=False),
        t=t.astype(np.float64, copy=False),
        out_mask=out_mask.astype(bool, copy=False),
        center_idx=center_idx,
        m=int(k_offsets.size),
        dlog=dlog,
    )

def _losvd_to_unit_kernel(H_native: np.ndarray, km: _KM) -> np.ndarray:
    H = np.asarray(H_native, dtype=np.float64, order="C")
    Hk = (1.0 - km.t) * H[km.il] + km.t * H[km.ir]
    Hk[km.out_mask] = 0.0
    np.maximum(Hk, 0.0, out=Hk)
    s = float(np.sum(Hk))
    if s == 0.0:
        Hk.fill(0.0)
        Hk[int(km.center_idx)] = 1.0
    else:
        Hk /= s
    return Hk

def _fft_conv_centered(T_fft_slice: np.ndarray,
                       Hk_unit: np.ndarray,
                       km: _KM,
                       n_fft: int,
                       T: int,
                       phase_shift: np.ndarray | None = None) -> np.ndarray:
    """
    FFT-based centered convolution of templates with a unit-area LOSVD kernel,
    followed by a 'same' crop aligned on km.center_idx.

    If `phase_shift` is provided (shape (rfft_len,), complex128), the output is
    additionally shifted by a fractional number of samples `shift_T` via the
    Fourier shift theorem (i.e., multiply the kernel FFT by the phase).
    This implements a global velocity bias without any extra FFTs.

    Parameters
    ----------
    T_fft_slice : (ΔP, rfft_len) complex128
        rFFT of the selected template rows (already zero-padded to n_fft).
    Hk_unit : (m,) float64
        Unit-area kernel on the template grid for a given (s,c).
    km : _KM
        Kernel map (contains center_idx and support length).
    n_fft : int
        FFT length used for the linear convolution.
    T : int
        Template length (pre-convolution).
    phase_shift : (rfft_len,) complex128 or None
        Optional precomputed exp(-2πi * f * shift_T). If None, no shift.

    Returns
    -------
    (ΔP, T) float64
        Convolved (and optionally globally shifted) templates, cropped in the
        centered 'same' sense.
    """
    H = np.maximum(np.asarray(Hk_unit, np.float64, order="C"), 0.0)
    s = float(np.sum(H))
    if s == 0.0:
        H[:] = 0.0
        H[int(km.center_idx)] = 1.0
    else:
        H /= s

    # Kernel FFT (complex128). Keep precision parity with template FFT.
    H_fft = np.fft.rfft(H, n=int(n_fft))

    # Apply optional fractional shift (global Δv) in frequency domain.
    if phase_shift is not None:
        H_fft *= phase_shift  # elementwise, shape (rfft_len,)

    # Standard conv in frequency domain and inverse transform
    Y_fft = T_fft_slice * H_fft[None, :]           # (ΔP, rfft_len)
    conv_f = np.fft.irfft(Y_fft, n=int(n_fft), axis=1)

    # Centered 'same' crop
    start = int(km.center_idx)
    stop  = int(start + T)
    return conv_f[:, start:stop].astype(np.float64, copy=False)

def _direct_centered_conv(Templates: np.ndarray,
                          Hk_unit: np.ndarray,
                          km: _KM) -> np.ndarray:
    X = np.asarray(Templates, np.float64, order="C")
    H = np.maximum(np.asarray(Hk_unit, np.float64, order="C"), 0.0)
    s = float(np.sum(H))
    if s == 0.0:
        H[:] = 0.0
        H[int(km.center_idx)] = 1.0
    else:
        H /= s

    dP, T = X.shape
    y = np.zeros((dP, T), dtype=np.float64)
    for k in range(int(km.m)):
        shift = int(k) - int(km.center_idx)
        if shift >= 0:
            if shift < T:
                y[:, shift:] += H[k] * X[:, :T-shift]
        else:
            sh = -shift
            if sh < T:
                y[:, :T-sh] += H[k] * X[:, sh:]
    return y

def _flat_response_errors(Hk_unit: np.ndarray,
                          km: _KM,
                          n_fft: int,
                          T: int) -> Tuple[float, float, float]:
    """
    Return (err_valid, err_left, err_right) for conv(1, Hk) on the
    centered 'same' crop. Only the interior 'valid' region can be ~1.
    """
    H = np.maximum(np.asarray(Hk_unit, np.float64, order="C"), 0.0)
    s = float(np.sum(H))
    if s == 0.0:
        H[:] = 0.0
        H[int(km.center_idx)] = 1.0
    else:
        H /= s

    flat = np.ones((1, int(T)), dtype=np.float64)
    T_fft = np.fft.rfft(flat, n=int(n_fft), axis=1)
    H_fft = np.fft.rfft(H, n=int(n_fft))  # keep complex128 for precision
    flat_full = np.fft.irfft(T_fft * H_fft[None, :], n=int(n_fft), axis=1)

    start = int(km.center_idx)
    stop = start + int(T)
    y = flat_full[:, start:stop].ravel()

    # Fully overlapped interior indices for the centered crop:
    # n in [ (m-1)-c , T-1 - c ]  (inclusive), clipped to [0, T-1]
    lo = int(np.maximum(0, (km.m - 1) - km.center_idx))
    hi = int(np.minimum(T - 1, (T - 1) - km.center_idx))
    if lo <= hi:
        err_valid = float(np.max(np.abs(y[lo:hi+1] - 1.0)))
    else:
        err_valid = float(np.nan)  # no interior exists (pathological)

    err_left = float(np.max(np.abs(y[:lo] - 1.0))) if lo > 0 else 0.0
    err_right = float(np.max(np.abs(y[hi+1:] - 1.0))) if hi < (T - 1) else 0.0
    return err_valid, err_left, err_right

def _xcorr_int_shift(a: np.ndarray, b: np.ndarray) -> int:
    aa = np.asarray(a, np.float64).ravel()
    bb = np.asarray(b, np.float64).ravel()
    T = int(aa.size)
    corr = np.correlate(aa, bb, mode="full")
    return int(np.argmax(corr) - (T - 1))

def preflight_hypercube_convolution(
    h5_path: str,
    s_list: Optional[Iterable[int]] = None,
    c_list: Optional[Iterable[int]] = None,
    p_list: Optional[Iterable[int]] = None,
    max_spax: int = 3,
    max_comp: int = 2,
    max_pop: int = 6,
    tol_rel: float = 2e-3,
    tol_shift_px: float = 0.5,
    tol_flat_valid: float = 3e-8,
    verbose: bool = True,
) -> Dict[str, Any]:
    """
    Sanity-check the convolution path on a tiny subset *before* building
    the full HyperCube. It proves that the FFT-based convolution (no
    roll + centered crop) exactly matches a direct time-domain
    reference, and that the kernel is unit-area (flat response ~ 1).

    Parameters
    ----------
    h5_path : str
        Path to the HDF5 input. Must contain /Templates, /TemPix,
        /LOSVD, /VelPix, and /R_T (for a flat-response rebin sanity).
    s_list, c_list, p_list : iterable of int, optional
        Explicit indices to test. If any are None, the function picks
        small initial slices from the available ranges.
    max_spax, max_comp, max_pop : int
        Caps used when lists are not provided.
    tol_rel : float
        Relative L2 tolerance for FFT≡Direct on the template grid.
    tol_shift_px : float
        Allowed integer-lag shift (pixels) from cross-correlation.

    Returns
    -------
    out : dict
        Summary with fields:
          - 'passes': list[bool] per triple (s, c, p)
          - 'triples': list[tuple] of tested indices
          - 'rel_err': list[float] relative errors
          - 'shift_px': list[int] integer-lag shifts
          - 'flat_err': list[float] flat-response errors
          - 'all_pass': bool, True if all triples passed
          - 'rt_flat_check': float, max|R_T @ 1 - 1| on obs grid

    Exceptions
    ----------
    RuntimeError
        If required datasets are missing or have incompatible shapes.

    Examples
    --------
    >>> res = preflight_hypercube_convolution("NGC4365_01.h5",
    ...                                       s_list=[24,25,26],
    ...                                       c_list=[124,197],
    ...                                       p_list=[42,44,88,89])
    >>> res["all_pass"]
    True
    """
    with open_h5(h5_path, role="reader") as f:
        if "/Templates" not in f or "/TemPix" not in f:
            raise RuntimeError("Missing /Templates or /TemPix.")
        if "/LOSVD" not in f or "/VelPix" not in f:
            raise RuntimeError("Missing /LOSVD or /VelPix.")
        if "/R_T" not in f:
            raise RuntimeError("Missing /R_T for flat rebin sanity.")

        Templates = np.asarray(f["/Templates"][...], np.float64, order="C")
        TemPix = np.asarray(f["/TemPix"][...], np.float64)
        LOSVD = np.asarray(f["/LOSVD"][...], np.float64, order="C")
        VelPix = np.asarray(f["/VelPix"][...], np.float64)

        P, T = map(int, Templates.shape)
        S, V, C = map(int, LOSVD.shape)

        R_any = np.asarray(f["/R_T"][...])
        L = int(f["/DataCube"].shape[1])
        if R_any.shape == (T, L):
            R_T = R_any.astype(np.float32, copy=False)
        elif R_any.shape == (L, T):
            R_T = R_any.T.astype(np.float32, copy=False)
        else:
            raise RuntimeError("Incompatible /R_T shape.")

    s_sel = list(s_list) if s_list is not None else list(range(0, int(np.minimum(max_spax, S))))
    c_sel = list(c_list) if c_list is not None else list(range(0, int(np.minimum(max_comp, C))))
    p_sel = list(p_list) if p_list is not None else list(range(0, int(np.minimum(max_pop, P))))

    km = _kernel_map_from_grids(TemPix, VelPix)

    p_arr = np.array(p_sel, dtype=np.int64)
    T_slice = Templates[p_arr, :]  # (ΔP, T)

    triples: List[Tuple[int, int, int]] = []
    rel_err: List[float] = []
    shift_px: List[int] = []
    flat_valid: List[float] = []
    flat_left: List[float] = []
    flat_right: List[float] = []
    passes: List[bool] = []

    # Rebin flatness on obs grid
    ones_T = np.ones((1, T), dtype=np.float64)
    flat_obs = ones_T @ R_T
    rt_flat_check = float(np.max(np.abs(flat_obs - 1.0)))

    for s in s_sel:
        for c in c_sel:
            H = LOSVD[s, :, c]
            Hk = _losvd_to_unit_kernel(H, km)

            n_fft = _next_pow2(int(T) + int(km.m) - 1)
            T_fft = np.fft.rfft(T_slice, n=int(n_fft), axis=1)

            y_fft = _fft_conv_centered(T_fft, Hk, km, n_fft, int(T))
            y_dir = _direct_centered_conv(T_slice, Hk, km)

            a = y_fft[0]
            b = y_dir[0]
            num = float(np.linalg.norm(a - b))
            den = float(np.maximum(np.linalg.norm(b), 1.0e-30))
            rel = num / den
            sh = _xcorr_int_shift(a, b)
            fr_valid, fr_left, fr_right = _flat_response_errors(Hk, km, n_fft, int(T))

            ok = (rel <= float(tol_rel)) and \
                 (abs(sh) <= float(tol_shift_px)) and \
                 (np.isnan(fr_valid) or (fr_valid <= float(tol_flat_valid)))

            triples.append((int(s), int(c), int(p_arr[0])))
            rel_err.append(rel)
            shift_px.append(int(sh))
            flat_valid.append(fr_valid)
            flat_left.append(fr_left)
            flat_right.append(fr_right)
            passes.append(bool(ok))

            if verbose:
                # expected pixel shift from LOSVD first moment
                Hpos = np.maximum(H, 0.0)
                denom = float(np.trapezoid(Hpos, VelPix))
                if denom > 0.0:
                    mu_v = float(np.trapezoid(Hpos * VelPix, VelPix) / denom)
                    exp_px = float(np.log1p(mu_v / 299792.458) / km.dlog)
                else:
                    exp_px = 0.0
                print(
                    "[preflight] s={:4d} c={:3d}  rel(FFT≡DIR)={:6.2e}  "
                    "shift={:+4.0f}px  flat_valid={:6.2e}  "
                    "edgeL={:6.2e} edgeR={:6.2e}  exp_px={:+6.2f}  ok={}"
                    .format(s, c, rel, sh, fr_valid, fr_left, fr_right, exp_px, ok)
                )

    out = dict(
        passes=passes,
        triples=triples,
        rel_err=rel_err,
        shift_px=shift_px,
        flat_valid=flat_valid,
        flat_edge_left=flat_left,
        flat_edge_right=flat_right,
        all_pass=bool(np.all(np.asarray(passes, bool))),
        rt_flat_check=rt_flat_check,
    )
    return out

# ------------------------------------------------------------------------------

def estimate_global_velocity_bias_prebuild(h5_path: str,
                                           n_spax: int = 96,
                                           n_features: int = 24,
                                           window_len: int = 31,
                                           lag_px: int = 12,
                                           use_mask: bool = True,
                                           amp_mode: str = "trapz",
                                           progress: bool = True) -> dict:
    """
    Estimate a single global velocity bias (Δv in km/s) *before* building the
    hypercube, by cross-correlating a synthesized reference spectrum (built
    directly from Templates+LOSVD+R_T) against the observed data.

    Method:
      1) Build a global reference model M_ref on the observed grid:
         - Compute per-component mean LOSVD kernels by stacking LOSVD over
           spaxels with amplitude weights (trapz or sum).
         - Convolve the average template with each mean kernel on the template
           grid, then rebin to the observed grid via R_T.
         - Weight components by global LOSVD amplitudes and sum.
      2) Continuum-normalize via a running-average subtraction (odd window).
      3) Select the deepest absorption features from the reference only; form a
         boolean mask over ±window_len//2 around each selected feature.
      4) FFT cross-correlate each masked, detrended data spectrum with the
         masked, detrended reference over small lags ±lag_px; collect per-spaxel
         integer-pixel shifts and take the median → global shift; convert to km/s.

    Parameters
    ----------
    h5_path : str
        Path to HDF5 with /Templates, /TemPix, /LOSVD, /VelPix, /R_T, /DataCube,
        /ObsPix, and optionally /Mask.
    n_spax : int
        Number of spaxels to sample evenly across S.
    n_features : int
        Number of deepest features (from reference) to keep.
    window_len : int
        Odd length for running-average baseline and feature half-width*2+1.
    lag_px : int
        Max tested shift (±lag_px pixels) around zero.
    use_mask : bool
        Apply /Mask when present.
    amp_mode : {"trapz","sum"}
        LOSVD amplitude mode for global weighting.
    progress : bool
        Show tqdm progress bars.

    Returns
    -------
    dict
        {
          "vel_bias_kms": float,
          "shift_px": float,
          "median_px": float,
          "mad_px": float,
          "n_spax_used": int,
          "L_eff": int,
          "dlog_obs_med": float,
          "n_features": int,
          "window_len": int,
          "quality": "ok" | "warn",
        }
    """
    # ---------- read static arrays into RAM ----------
    with open_h5(h5_path, role="reader") as f:
        Templates = np.asarray(f["/Templates"][...], dtype=np.float64, order="C")
        TemPix    = np.asarray(f["/TemPix"][...],    dtype=np.float64)
        LOSVD     = np.asarray(f["/LOSVD"][...],     dtype=np.float64, order="C")
        VelPix    = np.asarray(f["/VelPix"][...],    dtype=np.float64)
        R_any     = np.asarray(f["/R_T"][...])
        ObsPix    = np.asarray(f["/ObsPix"][...],    dtype=np.float64)

        S, V, C = LOSVD.shape
        P, T    = Templates.shape
        L       = int(f["/DataCube"].shape[1])

        if R_any.shape == (T, L):
            R_T = R_any.astype(np.float64, copy=False)
        elif R_any.shape == (L, T):
            R_T = R_any.T.astype(np.float64, copy=False)
        else:
            raise RuntimeError("Incompatible /R_T shape vs Templates/DataCube.")

        mask = None
        if use_mask and "/Mask" in f:
            m = np.asarray(f["/Mask"][...], dtype=bool).ravel()
            if m.size == L and np.any(m):
                mask = m

    if mask is None:
        keep_idx = None
        L_eff = int(L)
    else:
        keep_idx = np.flatnonzero(mask)
        L_eff = int(keep_idx.size)

    # ---------- build global reference spectrum ----------
    Tbar = np.mean(Templates, axis=0).astype(np.float64, copy=False)

    if amp_mode == "trapz":
        dH = np.trapezoid(LOSVD, VelPix, axis=1)   # (S, C)
    else:
        dH = np.sum(LOSVD, axis=1)             # (S, C)

    dH = np.maximum(dH, 0.0)
    H_sum_s = np.sum(dH, axis=1)               # (S,)
    H_sum_s = np.where(H_sum_s > 0.0, H_sum_s, 1.0)
    w_s = H_sum_s / np.sum(H_sum_s)

    w_c = np.sum(dH * w_s[:, None], axis=0)    # (C,)
    w_c = np.where(np.sum(w_c) > 0.0, w_c / np.sum(w_c), w_c)

    km = _kernel_map_from_grids(TemPix, VelPix)
    n_fft = 1 << (int(2 * T + km.m - 1 - 1).bit_length())

    Tbar_fft = np.fft.rfft(Tbar[None, :], n=int(n_fft), axis=1)  # (1, rfft_len)

    M_ref = np.zeros((L,), dtype=np.float64)
    for c in range(int(C)):
        Hc = np.tensordot(w_s, LOSVD[:, :, c], axes=(0, 0))      # (V,)
        Hk = _losvd_to_unit_kernel(Hc, km)                       # (m,)
        yT = _fft_conv_centered(Tbar_fft, Hk, km, int(n_fft), int(T)).ravel()
        Mc = yT @ R_T                                            # (L,)
        M_ref += w_c[c] * np.maximum(Mc, 0.0)

    if keep_idx is not None:
        M_ref = M_ref[keep_idx]

    # ---------- continuum removal & feature mask ----------
    def _detrend(y: np.ndarray, win: int) -> np.ndarray:
        k = int(np.maximum(1, win))
        if (k % 2) == 0:
            k = int(k + 1)
        ker = np.ones((k,), dtype=np.float64) / np.float64(k)
        baseline = np.convolve(y, ker, mode="same")
        return y - baseline

    M_ref_dt = _detrend(M_ref, int(window_len))

    order = np.argsort(M_ref_dt)  # deepest (most negative) first
    n_pick = int(np.minimum(int(n_features), M_ref_dt.size))
    pick = order[:n_pick]

    half = int(window_len // 2)
    mask_feat = np.zeros((L_eff,), dtype=bool)
    for p in pick:
        a = int(np.maximum(0, p - half))
        b = int(np.minimum(L_eff, p + half + 1))
        mask_feat[a:b] = True

    M_ref_f = np.where(mask_feat, M_ref_dt, 0.0)

    # ---------- per-spaxel xcorr, file open DURING the loop ----------
    if n_spax >= S:
        s_sel = np.arange(S, dtype=np.int64)
    else:
        s_sel = np.linspace(0, S - 1, int(n_spax), dtype=np.int64)

    n_corr = 1 << int((2 * L_eff - 1 - 1).bit_length())

    Mr = M_ref_f - np.mean(M_ref_f)
    nr = np.linalg.norm(Mr)
    Mr = Mr / (nr if nr > 0.0 else 1.0)

    pad_ref = np.zeros((n_corr,), dtype=np.float64)
    pad_ref[:L_eff] = Mr
    F_ref = np.fft.rfft(pad_ref)

    shifts = np.empty((s_sel.size,), dtype=np.float64)
    used = 0

    with open_h5(h5_path, role="reader") as f:
        DataCube = f["/DataCube"]
        try:
            # give the dataset a friendlier chunk cache if h5py supports it
            DataCube.id.set_chunk_cache(521, 8 * 1024 * 1024, 1.0)
        except Exception:
            pass

        itS = tqdm(s_sel, desc="[bias] xcorr", dynamic_ncols=True) if progress else s_sel
        for i, s in enumerate(itS):
            y = np.asarray(DataCube[int(s), :], dtype=np.float64, order="C")
            if keep_idx is not None:
                y = y[keep_idx]
            y = np.where(np.isfinite(y), y, 0.0)

            y_dt = _detrend(y, int(window_len))
            y_f  = np.where(mask_feat, y_dt, 0.0)

            y0 = y_f - np.mean(y_f)
            ny = np.linalg.norm(y0)
            if not np.isfinite(ny) or ny == 0.0:
                continue
            y0 = y0 / ny

            pad_dat = np.zeros((n_corr,), dtype=np.float64)
            pad_dat[:L_eff] = y0
            F_dat = np.fft.rfft(pad_dat)

            cc = np.fft.irfft(F_dat * np.conj(F_ref), n_corr)

            lo = int(0)
            hi = int(np.minimum(n_corr, int(lag_px) + 1))
            left  = cc[n_corr - int(lag_px):] if int(lag_px) > 0 else np.empty((0,))
            right = cc[lo:hi]
            win   = np.concatenate([left, right], axis=0)

            j = int(np.argmax(win))
            shift = j - int(lag_px)
            shifts[used] = np.float64(shift)
            used += 1

    if used == 0:
        raise RuntimeError("No usable spaxels for bias estimation.")

    shifts = shifts[:used]
    med = np.median(shifts)
    mad = np.median(np.abs(shifts - med))

    dlog = np.median(np.diff(ObsPix if keep_idx is None else ObsPix[keep_idx]))
    v_per_px = C_KMS * np.expm1(dlog)
    vel_bias = med * v_per_px

    with open_h5(h5_path, role="writer") as f:
        f["/HyperCube"].attrs["vel_bias_kms"] = float(vel_bias)
        f["/HyperCube"].attrs["vel_bias_method"] = "feature_xcorr_v1"

    return dict(
        vel_bias_kms=float(vel_bias),
        shift_px=float(np.median(shifts)),
        median_px=float(med),
        mad_px=float(mad),
        n_spax_used=int(used),
        L_eff=int(L_eff),
        dlog_obs_med=float(dlog),
        n_features=int(n_pick),
        window_len=int(window_len),
        quality="ok" if np.isfinite(vel_bias) else "warn",
    )

# ------------------------------------------------------------------------------

def _frac_shift_last_axis(X: np.ndarray, shift: float) -> np.ndarray:
    """
    Fractional circular shift of each row of X along the last axis by `shift`
    samples using the Fourier shift theorem. X is (..., T).
    """
    T = int(X.shape[-1])
    F = np.fft.rfft(X, n=T, axis=-1)
    freq = np.fft.rfftfreq(T)                 # cycles/sample
    phase = np.exp(-2j * np.pi * freq * float(shift))  # (rfft_len,)
    Y = np.fft.irfft(F * phase[(...,) + (None,)* (F.ndim-1 - 1)], n=T, axis=-1)
    return Y

# ------------------------- main builder ---------------------------------------

def build_hypercube(
    base_h5: str,
    *,
    norm_mode: str = "data",        # "model" or "data"
    amp_mode: str = "sum",           # "sum" or "trapz" for LOSVD amplitude
    cp_flux_ref_mode: str = "median",   # "median" | "mean" | "off"
    floor: float = 1e-12,
    S_chunk: int = 128,
    C_chunk: int = 1,
    P_chunk: int = 360,
    compression: str | None = None,  # keep None for speed; compress later
    vel_bias_kms: float = 0.0,
) -> None:
    """
    Build /HyperCube/models with LOSVD convolution on the template grid,
    rebin to observed grid, and scale once according to the selected
    normalization. The output is "ready to use" by the solver; no runtime
    scaling is needed.

    Additionally, this version computes the global column energy on-the-fly:
        E[c,p] = sum_{s, λ (masked if present)} models[s,c,p,λ]^2
    and stores it at /HyperCube/col_energy (float64) at the end of the build.
    """

    if norm_mode not in ("model", "data"):
        raise ValueError("norm_mode must be 'model' or 'data'.")
    if cp_flux_ref_mode not in ("median", "mean", "off"):
        raise ValueError("cp_flux_ref_mode must be 'median', 'mean', or 'off'.")


    # -------- Fast preflight: exit early if fully complete (reader-only)
    with open_h5(base_h5, "reader") as f_rd:
        P, T = map(int, f_rd["/Templates"].shape)
        S, V, C = map(int, f_rd["/LOSVD"].shape)
        L = int(f_rd["/DataCube"].shape[1])

        g = f_rd.get("/HyperCube", None)
        if g is not None and "models" in g and "_done" in g:
            done = np.asarray(g["_done"][...], dtype=np.uint8)
            all_done = (done.size > 0 and int(done.sum()) == int(done.size))
            if bool(g.attrs.get("complete", False)) and all_done:
                print("[HyperCube] already complete; skip build (no writer).")
                return

        # Read grids/operators/templates once (reader), reuse later
        tem_loglam = np.asarray(f_rd["/TemPix"][...], dtype=np.float64)
        vel_pix    = np.asarray(f_rd["/VelPix"][...], dtype=np.float64)
        R_any      = np.asarray(f_rd["/R_T"][...])
        if R_any.shape == (T, L):
            R_T = R_any.astype(np.float32, copy=False)
        elif R_any.shape == (L, T):
            R_T = R_any.T.astype(np.float32, copy=False)
        else:
            raise RuntimeError(f"/R_T shape {R_any.shape} incompatible with "
                f"T={T}, L={L}")
        Templates = np.asarray(f_rd["/Templates"][...], dtype=np.float64,
            order="C")

        # capture mask once for later energy accumulation
        keep_idx = None
        if "/Mask" in f_rd:
            m = np.asarray(f_rd["/Mask"][...], dtype=bool).ravel()
            if m.size == L and np.any(m):
                keep_idx = np.flatnonzero(m)


        # Optional λ-weights for energy/statistics (apply same floor, use √w)
        w_lam_sqrt = None
        lamw_floor = 1e-6
        if "/HyperCube/lambda_weights" in f_rd:
            _w = np.asarray(f_rd["/HyperCube/lambda_weights"][...],
                            dtype=np.float64).ravel()
            if _w.size == L:
                _w = np.clip(_w, lamw_floor, None)
                if keep_idx is not None:
                    _w = _w[keep_idx]
                w_lam_sqrt = np.sqrt(_w).astype(np.float64, copy=False)  # (Lk,) or (L,)

    # Precompute velocity→pixel mapping (shared for all (s,c))
    km = _kernel_map_from_grids(tem_loglam, vel_pix)
    n_fft = _choose_nfft(T, km.m)

    if vel_bias_kms != 0.0:
        dlog_T = float(np.median(np.diff(tem_loglam)))
        shift_T = float(np.log1p(float(vel_bias_kms) / C_KMS) / dlog_T)  # samples
        # Precompute phase vector once; reuse for all (s,c) in the build.
        freq = np.fft.rfftfreq(int(n_fft))                               # (rfft_len,)
        phase_shift = np.exp(-2j * np.pi * freq * shift_T).astype(np.complex128)
    else:
        phase_shift = None

    # Template FFT at n_fft (zero-padded for linear convolution)
    # Keep complex128 for numerical parity with preflight.
    T_fft = np.fft.rfft(Templates, n=int(n_fft), axis=1)

    # -------- Prepare destination dataset, normalization, and resume bitmap
    with open_h5(base_h5, "writer") as f:
        g = f.require_group("/HyperCube")
        # create models if absent
        if "models" not in g:
            chunks = (min(S_chunk, S), min(C_chunk, C), min(P_chunk, P), L)
            g.create_dataset("models", shape=(S, C, P, L), dtype="f4",
                             chunks=chunks, compression=compression or None)
        models = g["models"]
        # Load per-spaxel data-flux mean (required in data mode)
        if norm_mode == "data":
            if "/HyperCube/data_flux" not in f:
                raise RuntimeError("Missing /HyperCube/data_flux; compute it "
                                   "first (masked mean per spaxel, shape (S,)).")
            L_ds = f["/HyperCube/data_flux"]
            if L_ds.shape != (S,):
                raise RuntimeError(
                    f"/HyperCube/data_flux shape {L_ds.shape} invalid; "
                    f"expected (S,)={S}. It must be the masked mean flux "
                    f"per spaxel."
                )
            L_vec = np.asarray(L_ds[...], dtype=np.float64)  # (S,)
        else:
            L_vec = None

        # normalization datasets: A[s,c], A_sum[s], and optionally data_flux
        ng = _ensure_norm_group(f)
        A_path, A_sum_path = _compute_and_store_losvd_amplitudes(f,
            amp_mode=amp_mode)
        A = f[A_path] # (S,C) float64
        A_sum = f[A_sum_path] # (S,)  float64

        # record attributes once
        g.attrs["norm.mode"] = norm_mode
        g.attrs["losvd_amplitude_mode"] = amp_mode
        g.attrs["kernel_unit_area"] = True

        done, grid = _make_done_bitmap(f, S, C, P, S_chunk, C_chunk, P_chunk)

        # How many tiles remain BEFORE enabling SWMR
        rem = int(np.asarray(done[...], np.uint8).size
                  - np.asarray(done[...], np.uint8).sum())
        if rem <= 0:
            g.attrs["complete"] = True
            g.attrs["shape"] = (S, C, P, L)
            g.attrs["chunks"] = models.chunks
            try:
                f.flush()
            except Exception:
                pass
            print("[HyperCube] already complete; nothing to write.")
            return

        losvd_ds = f["/LOSVD"]

        masked_flag = bool(keep_idx is not None)
        # create dataset & attrs up front (metadata writes pre-SWMR)
        if "/HyperCube/col_energy" in f:
            del f["/HyperCube/col_energy"]
        E_ds = g.create_dataset("col_energy", shape=(C, P), dtype="f8",
                                chunks=(min(C,256), min(P,1024)), compression="gzip")
        # set attrs before swmr_mode
        E_ds.attrs["masked"] = bool(masked_flag)
        E_ds.attrs["lambda_weights"] = bool(w_lam_sqrt is not None)
        E_ds.attrs["lambda_weights_floor"] = 1e-6
        E_ds.attrs["shape"] = (int(C), int(P))
        E_ds.attrs["provenance"] = "sum_{s,λ} models^2 (mask applied)" if masked_flag else "sum_{s,λ} models^2"
        if masked_flag and (w_lam_sqrt is not None):
            E_ds.attrs["provenance"] = "sum_{s,λ} (√w·models)^2 over masked λ"
        elif masked_flag:
            E_ds.attrs["provenance"] = "sum_{s,λ} models^2 over masked λ"
        elif (w_lam_sqrt is not None):
            E_ds.attrs["provenance"] = "sum_{s,λ} (√w·models)^2 over all λ"
        else:
            E_ds.attrs["provenance"] = "sum_{s,λ} models^2 over all λ"

        # --- cp_flux_ref accumulators
        do_ref = (cp_flux_ref_mode != "off")
        if do_ref:
            if cp_flux_ref_mode == "mean":
                ref_sum = np.zeros((C, P), dtype=np.float64)
                ref_cnt = np.zeros((C, P), dtype=np.int64)
            else:  # "median"
                ref_acc = np.empty((C, P), dtype=object)
                for c_ in range(C):
                    for p_ in range(P):
                        ref_acc[c_, p_] = _P2Median()

        # --- SWMR writer mode
        try:
            f.flush()
            # f.swmr_mode = True
            print("[SWMR] writer mode enabled for HyperCube build.")
        except Exception as e:
            print(f"[SWMR] could not enable writer mode: {e}")

        # Initialize global column energy accumulator E[c,p] float64
        E_acc = np.zeros((C, P), dtype=np.float64)

        # --- iterate tiles; skip ones marked done
        def _iter_all_tiles():
            for p0 in range(0, P, P_chunk):
                p1 = min(P, p0 + P_chunk)
                for s0 in range(0, S, S_chunk):
                    is_s = s0 // S_chunk
                    s1 = min(S, s0 + S_chunk)
                    for c0 in range(0, C, C_chunk):
                        ic = c0 // C_chunk
                        c1 = min(C, c0 + C_chunk)
                        ip = p0 // P_chunk
                        yield (p0, p1, s0, s1, c0, c1, (is_s, ic, ip))

        total_tiles = rem
        pbar = tqdm(total=total_tiles, desc="[HyperCube] tiles",
                    mininterval=2.0)

        eps = 1e-30  # avoids 0/0 in data mode
        for (p0, p1, s0, s1, c0, c1, idx3) in _iter_all_tiles():
            if _done_get(done, idx3) != 0:
                continue

            # Frequency-domain template slice for this P-block
            T_fft_slice = T_fft[p0:p1, :]  # (ΔP, rfft_len)

            # For each (s,c) in this tile:
            for s_idx in range(s0, s1):
                a_sum = float(A_sum[s_idx])  # scalar
                for c_idx in range(c0, c1):
                    # 1) build unit-area kernel for (s,c)
                    H_native = np.asarray(losvd_ds[s_idx, :, c_idx],
                                         dtype=np.float64, order="C")
                    Hk_unit = _losvd_to_unit_kernel(H_native, km)

                    # 2) FFT conv (centered 'same' crop on template grid)
                    conv_td = _fft_conv_centered(T_fft_slice, Hk_unit, km, n_fft, int(T),
                             phase_shift=phase_shift)

                    # 3) rebin AFTER convolution
                    Ycp = conv_td @ R_T  # (ΔP, L)
                    np.maximum(Ycp, 0.0, out=Ycp)

                    # 4) apply FINAL scale according to norm_mode
                    if norm_mode == "model":
                        scale = float(A[s_idx, c_idx])
                    else:
                        if a_sum <= 0.0 or L_vec[s_idx] <= 0.0:
                            scale = 0.0
                        else:
                            frac = float(A[s_idx, c_idx]) / np.maximum(a_sum, 1.0e-30)
                            scale = float(L_vec[s_idx]) * frac

                    if scale != 0.0:
                        Ycp *= np.float32(scale)
                    else:
                        Ycp.fill(0.0)

                    # 6b) accumulate global energy E[c,p] with same λ-view:
                    # mask if present, and apply √w if lambda_weights exist.
                    if masked_flag:
                        Yv = Ycp[:, keep_idx]  # (ΔP, Lk)
                    else:
                        Yv = Ycp               # (ΔP, L or Lk)

                    # Compute per-population contribution for this (s_idx, c_idx, P-block)
                    if w_lam_sqrt is not None:
                        # weighted: sum_λ (√w·Y)^2 = sum_λ (w * Y^2)
                        e_local = np.sum(
                            np.square(Yv, dtype=np.float64) * w_lam_sqrt[None, :],
                            axis=1
                        )  # (ΔP,)
                    else:
                        # unweighted: sum_λ Y^2
                        e_local = np.sum(
                            np.square(Yv, dtype=np.float64),
                            axis=1
                        )  # (ΔP,)

                    E_acc[c_idx, p0:p1] += e_local

                    # --- cp_flux_ref: per-(c,p) masked λ-sum, one sample per spaxel
                    if do_ref:
                        if masked_flag:
                            s_lambda = np.sum(Ycp[:, keep_idx], axis=1, dtype=np.float64)  # (ΔP,)
                        else:
                            s_lambda = np.sum(Ycp, axis=1, dtype=np.float64)              # (ΔP,)
                        if cp_flux_ref_mode == "mean":
                            ref_sum[c_idx, p0:p1] += s_lambda
                            ref_cnt[c_idx, p0:p1] += 1
                        else:
                            # median: update sketches per p
                            for dp, val in enumerate(s_lambda):
                                ref_acc[c_idx, p0 + dp].update(float(val))

                    # 7) write
                    models[s_idx, c_idx, p0:p1, 0:L] = Ycp


            _done_set(done, idx3)

            # SWMR visibility
            try:
                models.id.flush()
            except Exception:
                pass
            try:
                done.id.flush()
            except Exception:
                pass
            try:
                f.flush()
            except Exception:
                pass

            pbar.update(1)

        pbar.close()

        E_ds[...] = E_acc

        # --- finalize cp_flux_ref
        if do_ref:
            if "/HyperCube/norm/cp_flux_ref" in f:
                del f["/HyperCube/norm/cp_flux_ref"]
            if cp_flux_ref_mode == "mean":
                ref = np.divide(ref_sum, np.maximum(ref_cnt, 1, dtype=np.int64),
                                dtype=np.float64)
            else:
                ref = np.empty((C, P), dtype=np.float64)
                for c_ in range(C):
                    for p_ in range(P):
                        ref[c_, p_] = ref_acc[c_, p_].median()
            # sanitize + floor
            ref = np.where(np.isfinite(ref), ref, 0.0)
            ref = np.maximum(ref, float(floor))
            g_norm = f.require_group("/HyperCube/norm")
            g_norm.create_dataset(
                "cp_flux_ref", data=ref.astype(np.float64),
                chunks=(min(C,256), min(P,1024)), compression="gzip"
            )
            g_norm.attrs["cp_flux_ref.mode"] = cp_flux_ref_mode
            g_norm.attrs["masked"] = bool(masked_flag)
            g_norm.attrs["definition"] = (
                "per-(c,p) statistic of sum_λ models[s,c,p,λ] over spaxels; "
                "mask applied to λ if present"
            )


        g.attrs["complete"] = True
        if vel_bias_kms is not None:
            g.attrs["vel_bias_kms"] = float(vel_bias_kms)
            g.attrs["vel_bias_note"] = "global LOSVD shift applied at build"
        g.attrs["shape"] = (S, C, P, L)
        g.attrs["chunks"] = models.chunks
        g.attrs["cp_flux_ref_mode"] = cp_flux_ref_mode


        # help h5py close cleanly
        del models, losvd_ds, done, g, E_ds
        try: f.flush()
        except: pass

# ------------------------------------------------------------------------------

def ensure_global_column_energy(
    h5_path: str,
    apply_mask: bool = True,
    dset_name: str = "/HyperCube/col_energy",
    *,
    rdcc_slots: int = 1_000_003,
    rdcc_bytes: int = 256 * 1024**2,
    rdcc_w0: float = 0.90,
) -> np.ndarray:
    """
    Compute/store E[c,p] = sum_{s,λ (masked if requested)} models[s,c,p,λ]^2 in float64.
    Streamed in (S_chunk, C_chunk=1, P_chunk, L) to match dataset chunking.
    """
    with open_h5(h5_path, role="reader") as f:
        M = f["/HyperCube/models"]  # (S,C,P,L) f32
        S, C, P, L = map(int, M.shape)

        # help the dataset cache a bit
        try:
            M.id.set_chunk_cache(int(rdcc_slots), int(rdcc_bytes), float(rdcc_w0))
        except Exception:
            pass

        keep_idx = None
        if apply_mask and "/Mask" in f:
            m = np.asarray(f["/Mask"][...], bool).ravel()
            keep_idx = np.flatnonzero(m) if (m.size == L and np.any(m)) else None

        S_chunk = int(M.chunks[0]) if (M.chunks and M.chunks[0]) else 128
        C_chunk = int(M.chunks[1]) if (M.chunks and M.chunks[1]) else 1
        P_chunk = int(M.chunks[2]) if (M.chunks and M.chunks[2]) else P

        E = np.zeros((C, P), dtype=np.float64)

        nS = int(np.ceil(S / S_chunk))
        nC = int(np.ceil(C / C_chunk))
        total = int(nS * nC)

        pbar = tqdm(total=total, desc="[col_energy]", mininterval=1.5, dynamic_ncols=True)

        for s0 in range(0, S, S_chunk):
            s1 = int(np.min((S, s0 + S_chunk)))
            for c0 in range(0, C, C_chunk):
                c1 = int(np.min((C, c0 + C_chunk)))

                # Read only the band that aligns with chunking
                X = np.asarray(M[s0:s1, c0:c1, :, :], dtype=np.float32, order="C")  # (Sblk,Cb,P,L)
                if keep_idx is not None:
                    X = X[:, :, :, keep_idx]  # (Sblk,Cb,P,Lk)

                # Sum of squares over s and λ → (Cb,P)
                # Avoids giant temporaries by working on the small Cb-band (usually 1)
                e_local = np.sum(np.square(X, dtype=np.float64), axis=(0, 3))  # (Cb,P)
                E[c0:c1, :] += e_local

                pbar.update(1)

        pbar.close()

    # Write once
    with open_h5(h5_path, role="writer") as f:
        if dset_name in f:
            del f[dset_name]
        ds = f.create_dataset(dset_name, data=E, dtype="f8", compression="gzip", compression_opts=1)
        ds.attrs["masked"] = bool(apply_mask)
        ds.attrs["note"] = "sum_{s,λ} models^2 (mask applied if masked=True)"
    return E

# ------------------------------------------------------------------------------

def read_global_column_energy(
    h5_path: str,
    dset_name: str = "/HyperCube/col_energy",
    strict: bool = True,
) -> np.ndarray | None:
    """
    Read E[c,p] from `dset_name`. Validates shape against /HyperCube/models.
    Returns (C,P) float64, or None if missing and strict=False.
    """
    with open_h5(h5_path, role="reader") as f:
        M = f["/HyperCube/models"]      # (S,C,P,L)
        _, C, P, _ = map(int, M.shape)

        if dset_name not in f:
            if strict:
                raise FileNotFoundError(
                    f"{dset_name} not found. Run compute_and_store_global_column_energy(...) once first."
                )
            return None

        E = np.asarray(f[dset_name][...], dtype=np.float64)
        if E.shape != (C, P):
            raise ValueError(f"{dset_name} has shape {E.shape}, expected {(C, P)}.")
        if not np.all(np.isfinite(E)):
            raise ValueError(f"{dset_name} contains non-finite values.")
        print(f"[col_energy] read {dset_name} with shape {E.shape}.")
        return E

# ------------------------------------------------------------------------------

def convert_hypercube_norm(h5_path: str,
                           to_mode: str,
                           *,
                           recompute_energy: bool = True) -> None:
    """
    In-place convert /HyperCube/models normalization between 'data' and 'model'
    using only per-spaxel scaling, without loading huge slabs into RAM.
    After rescaling, optionally recompute /HyperCube/col_energy by calling
    ensure_global_column_energy(apply_mask=True) — which will read /Mask itself.

    Semantics match the original function; only the I/O pattern is chunked.
    """
    to_mode = str(to_mode).lower()
    if to_mode not in ("data", "model"):
        raise ValueError("to_mode must be 'data' or 'model'.")

    with open_h5(h5_path, role="writer") as f:
        g = f["/HyperCube"]
        M = g["models"]                              # (S,C,P,L) float32
        S, C, P, L = map(int, M.shape)
        chunks = M.chunks or (min(S,128), 1, min(P,360), L)
        S_chunk, C_chunk, P_chunk, L_chunk = map(int, chunks)

        cur_mode = str(g.attrs.get("norm.mode", "model")).lower()
        if cur_mode == to_mode:
            print(f"[Norm] Already in '{to_mode}' mode; nothing to do.")
            return

        # Per-spaxel factors (identical to your original logic)
        ng = _ensure_norm_group(f)
        A_sum = np.asarray(ng["losvd_amp_sum"][...], dtype=np.float64)  # (S,)

        need_L_vec = (cur_mode == "model" and to_mode == "data") or \
                     (cur_mode == "data"  and to_mode == "model")
        if need_L_vec:
            if "/HyperCube/data_flux" not in f:
                raise RuntimeError("Missing /HyperCube/data_flux required for 'data' normalization.")
            L_vec = np.asarray(f["/HyperCube/data_flux"][...], dtype=np.float64)  # (S,)
        else:
            L_vec = None

        eps = 1e-30
        if   cur_mode == "data"  and to_mode == "model":
            F = (A_sum / np.maximum(L_vec, eps))
        elif cur_mode == "model" and to_mode == "data":
            F = (L_vec / np.maximum(A_sum, eps))
        else:
            raise RuntimeError(f"Unexpected conversion {cur_mode} → {to_mode}")

        # Clean degenerate entries (leave those spaxels unchanged)
        bad = ~np.isfinite(F) | (F <= 0.0)
        if np.any(bad):
            F = F.copy()
            F[bad] = 1.0

        # Nicely sized chunk cache to keep I/O smooth
        try:
            M.id.set_chunk_cache(521, 8*1024*1024, 1.0)  # slots, bytes, w0
        except Exception:
            pass

        # Stream in storage order: (S, C, P, L) with λ-banding = L_chunk
        from math import ceil
        total_steps = ceil(S/S_chunk) * ceil(C/C_chunk) * ceil(P/P_chunk) * ceil(L/L_chunk)
        pbar = tqdm(total=total_steps, desc=f"[Norm] {cur_mode}→{to_mode}",
                    mininterval=1.0, dynamic_ncols=True)

        for s0 in range(0, S, S_chunk):
            s1 = min(S, s0 + S_chunk)
            scale = F[s0:s1].astype(np.float32).reshape(-1, 1, 1, 1)  # (dS,1,1,1)

            for c0 in range(0, C, C_chunk):
                c1 = min(C, c0 + C_chunk)
                for p0 in range(0, P, P_chunk):
                    p1 = min(P, p0 + P_chunk)
                    for l0 in range(0, L, L_chunk):
                        l1 = min(L, l0 + L_chunk)

                        slab = M[s0:s1, c0:c1, p0:p1, l0:l1][...]  # f32
                        np.multiply(slab, scale, out=slab)        # rescale temp slab
                        M[s0:s1, c0:c1, p0:p1, l0:l1] = slab      # write back

                        pbar.update(1)

                # occasional flush keeps SWMR readers happy
                try:
                    M.id.flush(); f.flush()
                except Exception:
                    pass

        pbar.close()

        # Update the mode stamp
        g.attrs["norm.mode"] = to_mode
        try: f.flush()
        except Exception: pass

    # Keep semantics identical to your original: recompute col_energy VIA the helper
    if recompute_energy:
        # This helper should read /Mask internally (apply_mask=True),
        # so this function doesn’t need to load the mask itself.
        ensure_global_column_energy(h5_path, apply_mask=True)

# ------------------------------------------------------------------------------

def assert_preflight_ok(
    h5_path: str,
    s_list: Optional[Iterable[int]] = None,
    c_list: Optional[Iterable[int]] = None,
    p_list: Optional[Iterable[int]] = None,
    *,
    max_spax: int = 3,
    max_comp: int = 2,
    max_pop: int = 6,
    tol_rel: float = 2e-3,
    tol_shift_px: float = 0.5,
    tol_flat_valid: float = 3e-8,
    require_rt_flat: bool = True,
    rt_flat_tol: float = 3e-8,
    verbose: bool = True,
) -> Dict[str, Any]:
    r"""
    Run the convolution preflight on a tiny subset and raise a hard error
    if any check fails. This is intended to be called immediately before
    launching a full HyperCube build.

    Parameters
    ----------
    h5_path : str
        Path to the HDF5 file.
    s_list, c_list, p_list : iterable of int, optional
        Explicit indices to test. If any are None, a small prefix slice is
        chosen using the *max_* caps.
    max_spax, max_comp, max_pop : int
        Caps for auto-selection when lists are not provided.
    tol_rel : float
        Relative L2 tolerance for FFT≡Direct on the template grid.
    tol_shift_px : float
        Allowed integer-lag shift (pixels) from cross-correlation.
    tol_flat_valid : float
        Max |conv(1,Hk)−1| on the fully overlapped interior region of the
        centered 'same' crop.
    require_rt_flat : bool
        If True, also require that the rebin operator satisfies
        max|R_T @ 1 − 1| ≤ rt_flat_tol on the observed grid.
    rt_flat_tol : float
        Tolerance for the R_T flatness check.
    verbose : bool
        If True, prints the per-case lines emitted by the preflight.

    Returns
    -------
    Dict[str, Any]
        The result dict returned by
        ``preflight_hypercube_convolution(...)``. Always returned on
        success. On failure a ``RuntimeError`` is raised.

    Exceptions
    ----------
    RuntimeError
        If any per-case check fails (FFT vs direct, shift, or flat-valid),
        or if ``require_rt_flat`` is True and the rebin flatness exceeds
        ``rt_flat_tol``.

    Examples
    --------
    >>> assert_preflight_ok("NGC4365/NGC4365_01.h5",
    ...                     s_list=[24, 25, 26],
    ...                     c_list=[124, 197],
    ...                     p_list=[42, 44, 88, 89])
    """
    res = preflight_hypercube_convolution(
        h5_path,
        s_list=s_list,
        c_list=c_list,
        p_list=p_list,
        max_spax=int(max_spax),
        max_comp=int(max_comp),
        max_pop=int(max_pop),
        tol_rel=float(tol_rel),
        tol_shift_px=float(tol_shift_px),
        tol_flat_valid=float(tol_flat_valid),
        verbose=bool(verbose),
    )

    # Optional global rebin flatness guard
    if require_rt_flat and (float(res["rt_flat_check"]) > float(rt_flat_tol)):
        raise RuntimeError(
            "[preflight] R_T flatness failed: max|R_T @ 1 − 1| = "
            f"{float(res['rt_flat_check']):.3e} > {float(rt_flat_tol):.3e}"
        )

    if bool(res["all_pass"]):
        return res

    # Summarize failures
    passes = np.asarray(res["passes"], bool)
    rel = np.asarray(res["rel_err"], float)
    sh = np.asarray(res["shift_px"], int)
    fv = np.asarray(res["flat_valid"], float)
    triples = list(res["triples"])

    bad = np.flatnonzero(~passes)
    worst_rel = float(np.max(rel)) if rel.size else float("nan")
    worst_abs_shift = int(np.max(np.abs(sh))) if sh.size else 0
    # Handle all-NaN interior cases gracefully
    fv_clean = fv[~np.isnan(fv)] if fv.size else np.array([], float)
    worst_flat = float(np.max(fv_clean)) if fv_clean.size else float("nan")

    # Show up to 8 failing triples for quick triage
    show = bad[: np.minimum(bad.size, 8)]
    lines = []
    for i in show:
        lines.append(
            f"  case {i}: (s,c,p0)={triples[i]}  "
            f"rel={rel[i]:.3e}  shift={sh[i]:+d}px  "
            f"flat_valid={fv[i]:.3e}"
        )

    msg = [
        "[preflight] convolution checks failed.",
        f"  worst rel={worst_rel:.3e}  worst |shift|={worst_abs_shift:d}px  "
        f"worst flat_valid={worst_flat:.3e}",
    ]
    if lines:
        msg.append("  examples:")
        msg.extend(lines)

    raise RuntimeError("\n".join(msg))

# ------------------------------------------------------------------------------
</file>

<file path="kz_run.py">
"""
History
-------
v1.0:	Capture exceptions around `genCubeFit` call. 4 December 2025
"""

import numpy as np
import os, re, sys
import pathlib as plp
import argparse

# Custom modules
from CubeFit.kz_fitSpec import genCubeFit
from CubeFit.kz_initNGC4365 import props


def main():
    ap = argparse.ArgumentParser(description="Thin wrapper around genCubeFit")
    ap.add_argument(
        "--run-switch",
        type=str,
        default=None,
        help="Single string passed directly as runSwitch to genCubeFit"
    )
    # boolean redraw with explicit on/off flags
    group = ap.add_mutually_exclusive_group()
    group.add_argument("--redraw", dest="redraw", action="store_true",
                       help="Enable redraw mode")
    group.add_argument("--no-redraw", dest="redraw", action="store_false",
                       help="Disable redraw mode")
    ap.set_defaults(redraw=False)

    args = ap.parse_args()

    # Detect CPUs
    slurm_cpu = os.environ.get('SLURM_CPUS_PER_TASK')
    if slurm_cpu is not None:
        nCPU = int(slurm_cpu)
    else:
        curdir = plp.Path(__file__).parent
        try:
            with open(curdir/'kz_addqueue.sh') as f:
                content = f.read()
            m = re.search(r'^\s*nCPU\s*=\s*(\d+)', content, re.MULTILINE)
            nCPU = int(m.group(1)) if m else 20
        except FileNotFoundError:
            nCPU = 20

    print(f"Setting nCPU to {nCPU} from SLURM_CPUS_PER_TASK or kz_addqueue.sh")
    props['nProcs'] = nCPU

    # Pass-through args
    if args.run_switch is not None:
        props['runSwitch'] = args.run_switch
        print(f"runSwitch = {props['runSwitch']}")
    props['redraw'] = bool(args.redraw)
    print(f"redraw = {props['redraw']}")

    os.environ['CUBEFIT_NNLS_ENABLE']=str(0)
    os.environ['CUBEFIT_NNLS_EVERY']=str(5)
    os.environ['CUBEFIT_NNLS_MIN_IMPROVE']=str(0.9995)
    os.environ['CUBEFIT_NNLS_MAX_COLS']=str(64)
    os.environ['CUBEFIT_NNLS_SUB_L']=str(512)
    os.environ['CUBEFIT_NNLS_SOLVER']='nnls'
    os.environ["CUBEFIT_USE_NNLS_PRIOR"] = "0"
    os.environ['CUBEFIT_NNLS_L2']=str(0)
    os.environ["CUBEFIT_LAMBDA_WEIGHTS_ENABLE"] = "1"
    os.environ["CUBEFIT_KACZ_L2"] = "0.0"
    os.environ["CUBEFIT_RMSE_PROXY_GUARD"] = "0"
    os.environ["CUBEFIT_NNLS_PROP_PER_BAND"] = "0"
    os.environ["CUBEFIT_NNLS_ENABLE"] = "0"
    os.environ["CUBEFIT_ORBIT_BETA"] = "0.0"
    os.environ["CUBEFIT_GLOBAL_STEP_FRAC"] = "1.0"
    os.environ["CUBEFIT_GLOBAL_TAU"] = "0.3"
    os.environ["CUBEFIT_GLOBAL_ENERGY_BLEND"] = "0.0"
    os.environ["CUBEFIT_ZERO_COL_FREEZE"] = str(1)
    os.environ["CUBEFIT_ZERO_COL_REL"] = str(5e-4)
    print(props)

    try:
        genCubeFit(**props)
    except SystemExit:
        # Let explicit sys.exit()s behave normally
        raise
    except BaseException as e:
        # Log + print the traceback explicitly
        import traceback
        print("[kz_run] FATAL: unhandled exception in genCubeFit", file=sys.__stderr__, flush=True)
        traceback.print_exc()
        # This *forces* the interpreter to exit, even under IPython
        sys.exit(1)

if __name__ == "__main__":
    main()
</file>

<file path="nnls_patch.py">
# -*- coding: utf-8 -*-
r"""
    nnls_patch.py
    Adriano Poci
    University of Oxford
    2025

    Platforms
    ---------
    Unix, Windows

    Synopsis
    --------
    Patch-scale non-negative least-squares (NNLS) utilities for CubeFit.

    This module solves a *reduced* NNLS problem on a representative subset of
    spaxels and a restricted set of populations per component, producing a
    compact seed solution ``x_CP`` (shape ``(C, P)``) that can be:

    - written to the main HDF5 file (typically ``/Seeds/x0_nnls_patch``), and
    - used as a warm-start for global solvers (e.g. Kaczmarz-based fits).

    The patch fit can optionally:
    - apply the global wavelength mask (``/Mask``),
    - apply λ-weights (typically ``/HyperCube/lambda_weights``),
    - normalize model columns consistently with the HyperCube normalization,
    - compare population “usage” against an optional orbit-weight prior, and
    - generate quick-look diagnostics (per-spaxel overlays, RMSE/χ² summaries,
      and usage-vs-prior plots).

    The main entry point is :func:`run_patch`, and a small CLI wrapper is
    provided for interactive use and debugging.

    Inputs
    ------
    The HDF5 file is expected to contain, at minimum:
    - ``/DataCube`` (S, L)
    - ``/HyperCube/models`` (S, C, P, L)

    Optional datasets used when enabled:
    - ``/Mask`` (L,)
    - ``/HyperCube/lambda_weights`` (L,)
    - ``/HyperCube/col_energy`` or equivalent global column-energy storage

    Outputs
    -------
    - ``x_CP``: seed weights (C, P) in float64
    - optional HDF5 seed write (e.g. ``/Seeds/x0_nnls_patch``)
    - optional diagnostic PNGs in ``out_dir``

    Authors
    -------
    Adriano Poci <adriano.poci@physics.ox.ac.uk>

History
-------
v1.0:   19 December 2025
"""


from __future__ import annotations
import os, math, pathlib as plp
from typing import Optional, List, Tuple
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import argparse

from CubeFit.hdf5_manager import open_h5
from CubeFit.hypercube_builder import read_global_column_energy
from CubeFit.cube_utils import read_lambda_weights, ensure_lambda_weights,\
    compare_usage_to_orbit_weights

# ----------------------------- helpers ---------------------------------

def _predict_spaxel_sparse_from_models(M,
                                      s_idx,
                                      x_cp,
                                      picks,
                                      keep_idx=None):
    """
    Predict one spaxel spectrum by streaming /HyperCube/models in chunk-aligned
    slabs, avoiding eager materialization of (C, P, L).

    This is intended for diagnostics/plotting (e.g. the NNLS seed patch plots),
    where we only need y_fit for a small number of spaxels and want to avoid:

        np.asarray(M[s, :, :, :])  # huge (C,P,L) read per spaxel.

    The implementation respects the dataset chunking: it reads blocks
    (1, C_chunk, P_block, L) and accumulates contributions in float64.

    Notes
    -----
    - If the dataset is chunked with L_chunk == L (common in our files),
      masking wavelengths (keep_idx) reduces compute but may not reduce I/O
      much, because HDF5 still has to decompress whole L-chunks.
    - We still avoid the large temporary (C,P,Lk) allocation and instead
      stream blocks and accumulate into y_fit.

    Parameters
    ----------
    M : h5py.Dataset
        The /HyperCube/models dataset of shape (S, C, P, L).
    s_idx : int
        Spaxel index.
    x_cp : ndarray
        Weights shaped (C, P), float64 recommended.
    picks : list[list[int]]
        For each component c, a list of population indices to include.
        This is the same structure already used in nnls_patch.py.
    keep_idx : ndarray[int] or None
        Optional wavelength indices (length Lk) to evaluate on. If None,
        predicts the full L grid.

    Returns
    -------
    y_fit : ndarray
        Predicted spectrum for spaxel s_idx with shape (Lk,) if keep_idx is
        not None, else (L,). dtype float64.

    Raises
    ------
    ValueError
        If shapes are inconsistent with (C, P).

    Examples
    --------
    >>> with open_h5(h5_path, role="reader") as f:
    ...     M = f["/HyperCube/models"]
    ...     y_fit = _predict_spaxel_sparse_from_models(
    ...         M, s_idx=0, x_cp=x_CP, picks=picks, keep_idx=keep_idx
    ...     )
    """
    s_idx = int(s_idx)
    C = int(M.shape[1])
    P = int(M.shape[2])
    L = int(M.shape[3])

    x_cp = np.asarray(x_cp, dtype=np.float64, order="C")
    if x_cp.shape != (C, P):
        raise ValueError(f"x_cp shape {x_cp.shape} != (C,P)=({C},{P})")

    if keep_idx is None:
        Lk = L
        keep_idx_local = None
    else:
        keep_idx_local = np.asarray(keep_idx, dtype=np.int64).ravel()
        Lk = int(keep_idx_local.size)

    # Use storage chunking to align reads
    chunks = M.chunks or (1, 1, P, L)
    C_chunk = int(max(1, chunks[1]))
    P_chunk = int(max(1, chunks[2]))

    # Work in float64 accumulator; multiply in float32 for speed.
    y_fit = np.zeros(Lk, dtype=np.float64)

    # Iterate in storage order: components in C_chunk blocks
    for c0 in range(0, C, C_chunk):
        c1 = min(C, c0 + C_chunk)

        # Most of our files are C_chunk == 1, so this is one component.
        for c in range(c0, c1):
            plist = picks[c]
            # Avoid ambiguous truth-value for numpy arrays: check emptiness explicitly
            if (plist is None) or (np.size(plist) == 0):
                continue

            # Determine which P-chunks contain any of the picked populations.
            plist_arr = np.asarray(plist, dtype=np.int64)
            pj = np.unique((plist_arr // P_chunk) * P_chunk)

            for p0 in pj:
                p0 = int(p0)
                p1 = int(min(P, p0 + P_chunk))

                # Select only the picked populations that lie in [p0, p1)
                in_block = (plist_arr >= p0) & (plist_arr < p1)
                if not np.any(in_block):
                    continue
                local_idx = (plist_arr[in_block] - p0).astype(np.int64, copy=False)

                # Read one storage-aligned block: (1, 1, Pb, L) float32
                # (This avoids the full (C,P,L) slab.)
                A32 = M[s_idx:s_idx + 1, c:c + 1, p0:p1, :][...]
                A32 = np.asarray(A32, dtype=np.float32, order="C")[0, 0, :, :]  # (Pb, L)

                # Slice to selected populations in this P-block
                A32 = A32[local_idx, :]  # (Kb, L)

                # Optional wavelength masking (compute reduction; I/O depends on L-chunk)
                if keep_idx_local is not None:
                    A32 = A32[:, keep_idx_local]  # (Kb, Lk)

                # Weights for this block
                w64 = x_cp[c, p0:p1][local_idx]                 # (Kb,) float64
                w32 = w64.astype(np.float32, copy=False)        # float32 GEMV

                # Accumulate: y += A^T w
                y_fit += (A32.T @ w32).astype(np.float64, copy=False)

    return y_fit

def _get_mask_local(f) -> np.ndarray:
    """Return a 1D boolean λ-mask (L,), True = keep."""
    if "/Mask" in f:
        m = np.asarray(f["/Mask"][...], dtype=bool).ravel()
        # Accept only if it matches L
        L = int(f["/DataCube"].shape[1])
        if m.size == L:
            return m
    # fallback: keep all
    return np.ones(int(f["/DataCube"].shape[1]), dtype=bool)

def _pick_spaxels(S: int,
                  s_sel: Optional[str],
                  Ns_default: int = 32) -> np.ndarray:
    """
    Return a 1-D array of spaxel indices to use in the patch NNLS.

    If `s_sel` is None or empty, we pick `Ns_default` spaxels that are
    approximately evenly spaced across [0, S-1], so the seed sees the whole
    field instead of just the first Ns spaxels.

    If `s_sel` is of the form "start:count", we honour that exactly.
    Otherwise we treat it as a comma-separated list of explicit indices.
    """
    if s_sel is None or s_sel.strip() == "":
        Ns = int(min(Ns_default, S))
        if S <= Ns:
            return np.arange(S, dtype=np.int64)

        # Evenly spaced indices across [0, S-1]
        idx = np.linspace(0, S - 1, Ns, dtype=np.int64)
        # Just to be safe, enforce uniqueness and sort
        idx = np.unique(idx)
        return idx.astype(np.int64)

    s_sel = s_sel.strip()
    if ":" in s_sel:
        start_str, cnt_str = s_sel.split(":", 1)
        start = int(start_str)
        cnt   = int(cnt_str)
        stop  = min(S, start + cnt)
        if start < 0 or start >= S or cnt <= 0:
            raise ValueError(f"Bad s_sel='{s_sel}' for S={S}")
        return np.arange(start, stop, dtype=np.int64)

    # Explicit comma-separated list
    parts = [p for p in s_sel.split(",") if p.strip()]
    idx = np.array([int(p) for p in parts], dtype=np.int64)
    if idx.size == 0:
        raise ValueError(f"Empty s_sel='{s_sel}' for S={S}")
    if np.any(idx < 0) or np.any(idx >= S):
        raise ValueError(f"s_sel indices out of range for S={S}: {idx}")
    return np.unique(idx)

def _choose_pops(E: np.ndarray, K_per_comp: int, mode: str = "energy") -> List[np.ndarray]:
    C, P = map(int, E.shape)
    K = int(max(1, min(K_per_comp, P)))
    out: List[np.ndarray] = []
    for c in range(C):
        if mode == "energy":
            order = np.argsort(E[c, :])[::-1]
        elif mode == "random":
            rng = np.random.default_rng(1234 + c)
            order = np.arange(P); rng.shuffle(order)
        else:
            raise ValueError("mode must be 'energy' or 'random'")
        out.append(np.sort(order[:K].astype(np.int64)))
    return out

def _weighted_system(B: np.ndarray, y: np.ndarray, sqrt_w_rows: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
    return B * sqrt_w_rows[:, None], y * sqrt_w_rows

def _topk_by_corr_per_comp(Bw, yw, col_map, k_per_comp_init=32):
    # cosine similarity on the weighted system
    colnorm = np.linalg.norm(Bw, axis=0)
    score = (Bw.T @ yw) / np.maximum(colnorm, 1e-30)
    # group by component
    C_guess = int(np.max([c for c, _ in col_map])) + 1
    by_c = [[] for _ in range(C_guess)]
    for j, (c, p) in enumerate(col_map):
        by_c[c].append((score[j], j))
    keep = []
    for c, items in enumerate(by_c):
        if (items is None) or (len(items) == 0):
            continue
        items.sort(key=lambda t: t[0], reverse=True)
        k_c = int(np.minimum(k_per_comp_init, len(items)))
        keep.extend([items[i][1] for i in range(k_c)])
    keep = np.array(sorted(set(keep)), dtype=np.int64)
    return keep

def _filter_by_coherence(Bw, colnorm_all, keep_idx, cand_idx, rho_max=0.995):
    """
    Keep only candidates whose cosine similarity to ALL already-kept columns
    is <= rho_max. Vectorized; cheap because kincr_per_comp is small.
    """
    if len(cand_idx) == 0 or len(keep_idx) == 0:
        return np.array(cand_idx, dtype=np.int64)
    A_keep = Bw[:, keep_idx] / colnorm_all[keep_idx]          # (rows, Kkeep)
    V_cand = Bw[:, cand_idx] / colnorm_all[cand_idx]          # (rows, Kcand)
    coh = np.abs(A_keep.T @ V_cand)                           # (Kkeep, Kcand)
    ok = np.all(coh <= float(rho_max), axis=0)                # (Kcand,)
    return np.array(cand_idx, dtype=np.int64)[ok]

def _prune_and_resolve_if_needed(Bw, yw, keep, x, colnorm_all, rho_max,
                                 solver, ridge, normalize_columns):
    """
    If the kept set is still highly coherent, prune greedily (keep big coeffs,
    drop new ones that exceed rho_max w.r.t. already-kept), then re-solve.
    """
    if len(keep) <= 1:
        return x, keep

    A = Bw[:, keep] / colnorm_all[keep]
    G = np.abs(A.T @ A)
    np.fill_diagonal(G, 0.0)
    mu = float(np.max(G))
    print(f"[ws-NNLS] max coherence among kept = {mu:.6f}")
    if mu <= float(rho_max):
        return x, keep

    # Greedy prune by descending coefficient magnitude
    order = np.argsort(x[keep])[::-1]
    keep_sorted = np.array(keep, dtype=np.int64)[order]
    new_keep = []
    for j in keep_sorted:
        if len(new_keep) == 0:
            new_keep.append(j)
            continue
        # test coherence vs new_keep
        j_ok = _filter_by_coherence(Bw, colnorm_all, new_keep, [j], rho_max=rho_max)
        if j_ok.size == 1:
            new_keep.append(j)

    new_keep = np.array(sorted(set(new_keep)), dtype=np.int64)
    if new_keep.size == keep.size:
        return x, keep

    print(f"[ws-NNLS] prune: {keep.size} → {new_keep.size} (re-solve)")
    x_new = np.zeros_like(x)
    x_sub = _solve_nnls(Bw[:, new_keep], yw, solver=solver, max_iter=400,
                        ridge=float(ridge), normalize_columns=bool(normalize_columns))
    x_new[new_keep] = x_sub
    return x_new, new_keep

def _global_nnls_workingset(Bw, yw, col_map,
                            k0_per_comp=32, kincr_per_comp=16, rounds=4,
                            solver="nnls", ridge=1e-2, normalize_columns=True,
                            rho_max=0.995):
    """
    Fast global NNLS via a small working set with coherence control.
    Returns (x_full, keep_idx).
    """
    K = int(Bw.shape[1])
    colnorm_all = np.linalg.norm(Bw, axis=0)
    comp_of_j = np.array([c for (c, p) in col_map], dtype=np.int64)
    C = int(np.max(comp_of_j)) + 1

    # initial keep by local correlation, then de-duplicate by coherence
    keep0 = _topk_by_corr_per_comp(Bw, yw, col_map, k_per_comp_init=int(k0_per_comp))
    keep0 = np.intersect1d(keep0, np.flatnonzero(colnorm_all > 1e-12), assume_unique=False)
    # per-component coherence throttling
    keep = []
    for c in range(C):
        cand_c = keep0[comp_of_j[keep0] == c]
        chosen_c = []
        for j in cand_c:
            ok = _filter_by_coherence(Bw, colnorm_all, keep + chosen_c, [j], rho_max=rho_max)
            if ok.size == 1:
                chosen_c.append(j)
        keep.extend(chosen_c)
    keep = np.array(sorted(set(keep)), dtype=np.int64)

    x = np.zeros(K, dtype=np.float64)

    for r in range(int(rounds)):
        # solve on current working set
        Bw_sub = Bw[:, keep]
        x_sub = _solve_nnls(Bw_sub, yw, solver=solver, max_iter=400,
                            ridge=float(ridge), normalize_columns=bool(normalize_columns))
        x.fill(0.0); x[keep] = x_sub

        # residual & gradient
        r_w = Bw @ x - yw
        g = Bw.T @ r_w  # weighted gradient

        # KKT satisfied?
        viol = (x == 0.0) & (g < -1e-6)
        if not np.any(viol):
            break

        # candidate adds per component, with coherence gate
        add = []
        for c in range(C):
            mask = (comp_of_j == c) & viol & (colnorm_all > 1e-12)
            if not np.any(mask): 
                continue
            j_c = np.flatnonzero(mask)
            # sort by most negative gradient
            j_c = j_c[np.argsort(g[j_c])]
            # coherence filter against current keep
            j_c = _filter_by_coherence(Bw, colnorm_all, keep, j_c, rho_max=rho_max)
            if j_c.size:
                k_c = int(np.minimum(kincr_per_comp, j_c.size))
                add.extend(j_c[:k_c].tolist())

        if len(add) == 0:
            break
        keep = np.array(sorted(set(keep.tolist() + add)), dtype=np.int64)
        if keep.size >= K:
            break  # safety

    # optional prune and re-solve if coherence still high
    x, keep = _prune_and_resolve_if_needed(Bw, yw, keep, x, colnorm_all,
                                           rho_max, solver, ridge, normalize_columns)

    res_w = float(np.linalg.norm(Bw @ x - yw))
    print(f"[ws-NNLS] rounds={r+1} support={keep.size}/{K}  ||Bw x - yw||={res_w:.3g}")
    return x, keep

# ------------------------------------------------------------------------------

def apply_orbit_prior_to_seed(Xcp: np.ndarray,
                              orbit_weights,
                              *,
                              E_cp: np.ndarray | None = None,
                              preserve_total: bool = True,
                              min_w_frac: float = 1e-4) -> np.ndarray:
    """
    Rescale rows of Xcp (C,P) so row usage matches orbit_weights fractions.

    If E_cp is provided (C,P), usage is energy-weighted: s_c = sum_p X[c,p]*E[c,p].
    Otherwise usage is plain row sums: s_c = sum_p X[c,p].

    This is a fast, closed-form, mass-preserving projection; O(C*P).
    """
    X = np.nan_to_num(np.asarray(Xcp, np.float64), nan=0.0, posinf=0.0,
                      neginf=0.0, copy=True)
    if X.ndim != 2:
        raise ValueError("Xcp must be 2-D (C,P).")
    C, P = X.shape

    w = np.asarray(orbit_weights, dtype=np.float64).ravel(order="C")
    if w.size not in (C, C * P):
        raise ValueError(f"orbit_weights has size {w.size}, expected {C} or {C*P}")
    if w.size == C * P:
        w = w.reshape(C, P, order="C").sum(axis=1)

    w = np.nan_to_num(w, nan=0.0, posinf=0.0, neginf=0.0)
    Wsum = float(w.sum())
    if Wsum <= 0.0:
        return X  # no prior to enforce
    t = w / Wsum  # target fractions per component in [0,1]

    # Current usage per component
    if E_cp is not None:
        E = np.asarray(E_cp, np.float64, order="C")
        if E.shape != (C, P):
            raise ValueError(f"E_cp shape {E.shape} != (C,P) {(C,P)}")
        s = (X * E).sum(axis=1)  # energy-weighted usage
    else:
        s = X.sum(axis=1)        # plain usage
    S = float(s.sum())

    # If a component has zero usage but positive target, sprinkle tiny mass
    need = (s <= 0.0) & (t > 0.0)
    if np.any(need):
        eps = 1e-12 * (S if S > 0.0 else 1.0)
        X[need, :] = (eps / P)
        # recompute s/S in the same metric
        if E_cp is not None:
            s = (X * E).sum(axis=1)
        else:
            s = X.sum(axis=1)
        S = float(s.sum())

    # Row scale factors so that new usage ∝ target t
    s_safe = np.maximum(s, 1e-30)
    scale = t / s_safe
    X *= scale[:, None]

    if preserve_total:
        # Keep overall mass in the same metric as before
        if E_cp is not None:
            Sp = float((X * E).sum())
        else:
            Sp = float(X.sum())
        if Sp > 0.0 and S > 0.0:
            X *= (S / Sp)

    np.maximum(X, 0.0, out=X)
    np.nan_to_num(X, copy=False, nan=0.0, posinf=0.0, neginf=0.0)
    return X

# ------------------------------------------------------------------------------

def _solve_nnls(Bw: np.ndarray, yw: np.ndarray, solver="nnls", max_iter=200, ridge=0.0,
                normalize_columns: bool = True) -> np.ndarray:
    """
    Solve min ||Bw x - yw||_2 subject to x>=0.
    If normalize_columns, solve on Bn = Bw / col_norm, then map back via x = z / col_norm.
    """
    K = int(Bw.shape[1])
    if normalize_columns:
        col_norm = np.linalg.norm(Bw, axis=0)
        col_norm = np.where(col_norm > 0.0, col_norm, 1.0)
        Bn = Bw / col_norm
    else:
        col_norm = np.ones(K, dtype=np.float64)
        Bn = Bw

    if solver == "nnls":
        from scipy.optimize import nnls
        try:
            z, _ = nnls(Bn, yw)              # solve on normalized system
            return z / col_norm               # map back: x = z / ||Bw[:,j]||
        except Exception as e:
            # Lawson–Hanson hit a singular AtA[P,P]. Fall back to a stable PG solve on Bn.
            # Lightweight projected gradient on normalized system -> z, then x = z/col_norm
            # (same stopping as your PG branch, but inline to avoid re-normalizing twice)
            K = int(Bn.shape[1])
            z = np.zeros(K, dtype=np.float64)
            L_est = float(np.max(np.sum(Bn*Bn, axis=0))) + float(ridge)
            L_est = L_est if np.isfinite(L_est) and L_est > 0.0 else 1.0
            step = 1.0 / L_est
            for _ in range(int(np.minimum(max_iter, 400))):
                r = Bn @ z - yw
                g = Bn.T @ r + float(ridge) * z
                z_try = np.maximum(0.0, z - step * g)
                r_try = Bn @ z_try - yw
                f_old = 0.5 * float(r @ r)     + 0.5 * float(ridge) * float(z @ z)
                f_new = 0.5 * float(r_try @ r_try) + 0.5 * float(ridge) * float(z_try @ z_try)
                bt = 0
                while f_new > f_old and bt < 8:
                    step *= 0.5
                    z_try = np.maximum(0.0, z - step * g)
                    r_try = Bn @ z_try - yw
                    f_new = 0.5 * float(r_try @ r_try) + 0.5 * float(ridge) * float(z_try @ z_try)
                    bt += 1
                z = z_try
                if np.linalg.norm(g, ord=np.inf) * step < 1e-9:
                    break
            return z / col_norm

    if solver == "lsq":
        from scipy.optimize import lsq_linear
        if ridge > 0.0:
            I = np.sqrt(ridge) * np.eye(K, dtype=np.float64)
            Bw_aug = np.vstack([Bn, I])
            yw_aug = np.concatenate([yw, np.zeros(K, dtype=np.float64)], axis=0)
            res = lsq_linear(Bw_aug, yw_aug, bounds=(0.0, np.inf), method="trf",
                             max_iter=int(max_iter), verbose=0)
        else:
            res = lsq_linear(Bn, yw, bounds=(0.0, np.inf), method="trf",
                             max_iter=int(max_iter), verbose=0)
        return np.maximum(0.0, res.x) / col_norm

    # simple PGD with backtracking (supports ridge)
    xz = np.zeros(K, dtype=np.float64)  # this is 'z' in the normalized space
    L_est = float((Bn**2).sum(axis=0).max() + ridge) or 1.0
    step = 1.0 / L_est
    for _ in range(int(max_iter)):
        r = Bn @ xz - yw
        g = Bn.T @ r + ridge * xz
        z_try = np.maximum(0.0, xz - step * g)
        r_try = Bn @ z_try - yw
        f_old = 0.5 * float(r @ r)     + 0.5 * ridge * float(xz @ xz)
        f_new = 0.5 * float(r_try @ r_try) + 0.5 * ridge * float(z_try @ z_try)
        bt = 0
        while f_new > f_old and bt < 8:
            step *= 0.5
            z_try = np.maximum(0.0, xz - step * g)
            r_try = Bn @ z_try - yw
            f_new = 0.5 * float(r_try @ r_try) + 0.5 * ridge * float(z_try @ z_try)
            bt += 1
        xz = z_try
        if np.linalg.norm(g, ord=np.inf) * step < 1e-9:
            break
    return xz / col_norm

# -------------------------- main routine -------------------------------------

def run_patch(h5_path: str,
              s_sel: Optional[str] = None,
              k_per_comp: int = 12,
              pick_mode: str = "energy",
              solver: str = "nnls",
              ridge: float = 0.0,
              use_mask: bool = True,
              use_lambda: bool = True,
              lam_dset: str = "/HyperCube/lambda_weights",
              out_dir: Optional[str | os.PathLike] = None,
              write_seed: bool = False,
              seed_path: str = "/Seeds/x0_nnls_patch",
              normalize_columns: bool = True,
              orbit_weights: np.ndarray | None = None):

    out_dir = None if out_dir is None else str(out_dir)
    if out_dir:
        plp.Path(out_dir).mkdir(parents=True, exist_ok=True)

    # --- read dims, global mask, λ-weights, picks
    with open_h5(h5_path, role="reader") as f:
        DC = f["/DataCube"]; M = f["/HyperCube/models"]
        S, L = map(int, DC.shape)
        _, C, P, Lm = map(int, M.shape)
        if Lm != L:
            raise RuntimeError(f"models L={Lm} != data L={L}")
        norm_mode = str(f["/HyperCube"].attrs.get("norm.mode", "model")).lower()
        mask = _get_mask_local(f) if use_mask else np.ones(L, bool)
        keep_idx = np.flatnonzero(mask) if mask is not None else None
        Lk = int(keep_idx.size) if keep_idx is not None else L
        # observed λ grid for plotting
        obs = f["/ObsPix"][...] if "/ObsPix" in f else np.arange(L, dtype=np.float64)
        lam_plot = obs[keep_idx] if keep_idx is not None else obs

    if use_lambda:
        try:
            w_full = read_lambda_weights(h5_path, dset_name=lam_dset, floor=1e-6)
        except Exception:
            w_full = ensure_lambda_weights(h5_path, dset_name=lam_dset)
    else:
        w_full = np.ones(L, dtype=np.float64)
    w_lam = w_full if keep_idx is None else w_full[keep_idx]

    s_idx = _pick_spaxels(S, s_sel, Ns_default=32)
    s_idx = np.sort(np.asarray(s_idx, dtype=int))
    if s_idx.size < 1:
        raise RuntimeError("Empty spaxel selection.")

    # ---- quick triage (paste right after w_lam and s_idx) ----
    print(f"[triage] S_sel={s_idx.size}")

    Lk = int(keep_idx.size) if keep_idx is not None else int(len(w_full))
    print(f"[triage] Lk (masked wavelengths) = {Lk}")
    if Lk == 0:
        raise SystemExit("[triage] /Mask removes all wavelengths for this run "
            "(Lk=0). Try --no-mask or fix /Mask.")

    # finite counts per spaxel under the mask
    with open_h5(h5_path, role="reader") as f:
        DC = f["/DataCube"]
        finite_counts = []
        for s in s_idx:
            spec = np.asarray(DC[s, :], dtype=np.float64)
            spec = spec[keep_idx] if keep_idx is not None else spec
            finite_counts.append(int(np.count_nonzero(np.isfinite(spec))))
    finite_counts = np.asarray(finite_counts, int)
    print(f"[triage] finite pixels per spaxel (min/med/max): "
        f"{finite_counts.min()}/{int(np.median(finite_counts))}/{finite_counts.max()}")

    if finite_counts.min() == 0:
        print("[triage] Every selected spaxel with 0 finite pixels under the mask "
            "will contribute zero rows. If they are ALL zero, "
            "Bw==0 and you’ll see the 'all columns ~zero' message. "
            "Try --no-mask or pick spaxels with finite data.")

    # λ-weights sanity
    w_masked = w_lam
    print(f"[triage] lambda weights on mask min/med/max: "
        f"{w_masked.min():.3g}/{np.median(w_masked):.3g}/{w_masked.max():.3g}")

    # Subsample the selected spaxels for this expensive check
    if s_idx.size > 12:
        rng = np.random.default_rng(12345)
        tri_s = np.sort(
            rng.choice(s_idx, size=12, replace=False)
        )
    else:
        tri_s = s_idx

    with open_h5(h5_path, role="reader") as f:
        mode = str(f["/HyperCube"].attrs.get("norm.mode", "model"))
        df   = (np.asarray(f["/HyperCube/data_flux"][s_idx], float)
                if "/HyperCube/data_flux" in f else None)
        la   = (np.asarray(f["/HyperCube/norm/losvd_amp_sum"][s_idx], float)
                if "/HyperCube/norm/losvd_amp_sum" in f else None)

        E_cp = np.asarray(f["/HyperCube/col_energy"][...], float)  # (C,P)
        S, L = map(int, f["/DataCube"].shape)
        rms_cp = np.sqrt(E_cp / max(1, S * L)) # RMS amplitude

    print(f"[triage] norm.mode={mode}")
    print(
        "[triage] model RMS amplitude per (c,p) column (min/med/max): "
        f"{rms_cp.min():.3g}/{np.median(rms_cp):.3g}/{rms_cp.max():.3g}"
    )
    if df is not None:
        print(
            "[triage] data_flux (min/med/max): "
            f"{df.min():.3g}/{np.median(df):.3g}/{df.max():.3g}"
        )
    if la is not None:
        print(
            "[triage] losvd_amp_sum (min/med/max): "
            f"{la.min():.3g}/{np.median(la):.3g}/{la.max():.3g}"
        )
    
    E = read_global_column_energy(h5_path)  # (C,P)
    picks = _choose_pops(E, k_per_comp, pick_mode)

    # --- Build RHS y and per-row weights (spaxel-wise finite mask)
    rows = int(s_idx.size * Lk)
    y = np.empty(rows, dtype=np.float64)
    sqrt_w_rows = np.empty(rows, dtype=np.float64)

    print(f"[patch] Building RHS and row-weights: S_sel={s_idx.size}, Lk={Lk}")
    finite_counts = []
    pos = 0
    with open_h5(h5_path, role="reader") as f:
        DC = f["/DataCube"]
        it = tqdm(s_idx, desc="[patch] RHS", mininterval=0.5, dynamic_ncols=True)
        for s in it:
            spec = np.asarray(DC[s, :], dtype=np.float64)
            if keep_idx is not None:
                spec = spec[keep_idx]
            # per-spaxel finite mask
            good = np.isfinite(spec)
            # store y with NaNs → 0 (they get zero weight)
            spec_sanit = np.where(good, spec, 0.0)
            y[pos:pos+Lk] = spec_sanit
            # per-row weights: √w_lambda * 1[finite]
            sqrt_w_rows[pos:pos+Lk] = np.sqrt(w_lam) * good.astype(np.float64)
            finite_counts.append(int(np.count_nonzero(good)))
            pos += Lk
    finite_counts = np.asarray(finite_counts, int)
    print(f"[patch] finite pixels per spaxel (min/median/max): "
          f"{finite_counts.min()}/{int(np.median(finite_counts))}/{finite_counts.max()}")

    # --- Build design B: concat columns (one per chosen (c,p))
    K = int(sum(len(pc) for pc in picks))
    B = np.empty((rows, K), dtype=np.float64)
    col_map: List[Tuple[int, int]] = []

    # Map (c, p) -> column index in B
    cp_to_col: dict[Tuple[int, int], int] = {}
    j = 0
    for c, plist in enumerate(picks):
        for p_idx in plist:
            p_int = int(p_idx)
            cp_to_col[(c, p_int)] = j
            col_map.append((c, p_int))
            j += 1

    print(f"[patch] Building design (blockwise): rows={rows}, K={K}")

    # Reshape row weights once; they depend only on (spaxel, λ)
    # sqrt_w_rows has length rows = Ns * Lk
    Ns = int(s_idx.size)
    sqrt_w_2d = sqrt_w_rows.reshape(Ns, Lk)
    row_mask = (sqrt_w_2d > 0.0)  # True where data are finite / used

    with open_h5(h5_path, role="reader") as f:
        M = f["/HyperCube/models"]  # (S, C, P, L)

        for c, plist in enumerate(
            tqdm(
                picks,
                desc="[patch] comps",
                mininterval=0.5,
                dynamic_ncols=True,
            )
        ):
            # plist can be a list or a numpy array; in both cases len(...) works.
            if plist is None or len(plist) == 0:
                continue

            # (optional) normalize to a 1D iterable of ints
            plist_iter = [int(p) for p in plist]

            # Block read: all selected spaxels, all pops for this component
            A_c = np.asarray(M[s_idx, c, :, :], dtype=np.float32)

            if keep_idx is not None:
                A_c = A_c[:, :, keep_idx]  # (Ns, P, Lk)

            A_c = np.nan_to_num(
                A_c, nan=0.0, posinf=0.0, neginf=0.0, copy=False
            )

            A_c *= row_mask[:, None, :]   # (Ns, P, Lk)

            for p_idx in plist_iter:
                p_int = int(p_idx)
                j = cp_to_col[(c, p_int)]
                B[:, j] = A_c[:, p_int, :].reshape(rows, order="C")


    # final sanitation guard (paranoia)
    if not np.isfinite(B).all():
        bad_cols = np.any(~np.isfinite(B), axis=0).nonzero()[0]
        print(
            f"[patch] WARNING: non-finite values in {bad_cols.size} columns; "
            "sanitizing to 0."
        )
        B[:, bad_cols] = np.nan_to_num(B[:, bad_cols], copy=False)

    # quick column-norm diagnostics on the weighted system
    colnorm_w = np.linalg.norm(B * sqrt_w_rows[:, None], axis=0)
    print("[patch] column norms (weighted) min/median/max: "
          f"{colnorm_w.min():.3g}/{np.median(colnorm_w):.3g}/{colnorm_w.max():.3g}")
    if np.all(colnorm_w < 1e-12):
        print("[patch] All columns are ~zero after masking/weighting → check "
              "hypercube normalization and LOSVD amplitudes for these spaxels.")

    # --- Apply row weights (λ and finite mask) to (B,y)
    Bw, yw = _weighted_system(B, y, sqrt_w_rows)

    # --- column/target sanity on the weighted system
    colnorm_all = np.linalg.norm(Bw, axis=0)
    nz_cols = int(np.count_nonzero(colnorm_all > 0))
    print(f"[patch] weighted col-norms: nz={nz_cols}/{colnorm_all.size}")

    # raw correlations; if all <= 0 then NNLS optimum is x=0
    corr = Bw.T @ yw
    pos_corr = int(np.count_nonzero(corr > 0))
    print(f"[patch] corr>0 = {pos_corr}/{corr.size}  min/med/max = "
        f"{float(np.min(corr)):.3g}/{float(np.median(corr)):.3g}/{float(np.max(corr)):.3g}")

    if nz_cols == 0:
        raise RuntimeError("[patch] All columns are zero after masking/weighting for these spaxels.")

    # --- Fast working-set GLOBAL NNLS on a pruned basis
    print(f"[patch] Working-set global NNLS (init=32/comp, +16/round, rounds=4)...")
    x_full, keep = _global_nnls_workingset(
        Bw, yw, col_map,
        k0_per_comp=32, kincr_per_comp=16, rounds=4,
        solver=solver, ridge=float(ridge), normalize_columns=bool(normalize_columns)
    )
    # keep col_map aligned with x_patch for the unpack loop
    x_patch = x_full[keep].copy()                # compress to working set
    col_map  = [col_map[j] for j in keep]        # now 1:1 aligned
    K = int(len(col_map))  # (optional) if you print/return K later

    # --- Unpack to full (C,P)
    x_CP = np.zeros((C, P), dtype=np.float64)
    for j, (c, p) in enumerate(col_map):
        x_CP[c, p] = x_patch[j]
    # --- Enforce orbit prior on the seed (energy metric) --- FAST and strict
    if orbit_weights is not None:
        x_CP = apply_orbit_prior_to_seed(x_CP, orbit_weights,
            E_cp=E, preserve_total=True, min_w_frac=1e-4)

    # --- Reconstruct + diagnostics per spaxel
    rmse = np.zeros(s_idx.size, dtype=np.float64)
    chi2 = np.zeros_like(rmse)
    pos = 0
    print("[patch] Reconstructing and plotting spectra...")
    with open_h5(h5_path, role="reader") as f:
        M = f["/HyperCube/models"]
        for i, s in enumerate(tqdm(s_idx, desc="[patch] plots", mininterval=0.5, dynamic_ncols=True)):
            y_fit = np.zeros(Lk, dtype=np.float64)
            # weighted least-squares sense: compare on masked/log grid unweighted
            y_fit = _predict_spaxel_sparse_from_models(
                M=M,
                s_idx=int(s),
                x_cp=x_CP,
                picks=picks,
                keep_idx=keep_idx,
            )
            y_obs = y[pos:pos+Lk]
            wrow  = sqrt_w_rows[pos:pos+Lk]
            pos += Lk
            r = y_obs - y_fit
            # RMSE (unweighted) and χ² (λ-weighted on finite)
            rmse[i] = float(np.sqrt(np.mean((r)**2)))
            w2 = wrow * wrow
            den = float(np.sum(w2)) or 1.0
            chi2[i] = float(np.sum((r * wrow)**2) / den)

            if out_dir:
                fig = plt.figure(figsize=(9.5, 3.2))
                ax = fig.add_subplot(111)
                ax.plot(lam_plot, y_obs, lw=1.0, label="data")
                ax.plot(lam_plot, y_fit, lw=1.0, label="model")
                ax.set_title(f"spaxel {int(s)}  rmse={rmse[i]:.3f}")
                ax.set_xlabel("λ (ObsPix)")
                ax.set_ylabel("flux")
                ax.legend(loc="best", fontsize=8)
                fig.savefig(os.path.join(out_dir,
                    f"patch_spax{int(s):05d}.png"), dpi=120)
                plt.close(fig)

    if out_dir:
        flat = x_CP.ravel(order="C")
        nnz = int(np.count_nonzero(flat))
        if nnz == 0:
            print("[plot] x_CP is all zeros (no bars to draw).")
        else:
            # (optional) plot only the top 1000 by magnitude to make the bars visible
            order = np.argsort(flat)[::-1][:int(np.minimum(1000, nnz))]
            fig = plt.figure(figsize=(10, 3))
            ax = fig.add_subplot(111)
            ax.bar(np.arange(order.size), flat[order])
            ax.set_title("x_CP top coefficients (sorted)")
            ax.set_xlabel("rank"); ax.set_ylabel("weight")
            fig.savefig(os.path.join(out_dir, "xcp_bar.png"), dpi=120)
            plt.close(fig)

    if write_seed:
        with open_h5(h5_path, role="writer") as f:
            g = f.require_group("/Seeds")
            if seed_path in f:
                del f[seed_path]
            ds = f.create_dataset(seed_path, data=x_CP.astype(np.float64),
                dtype="f8")
            ds.attrs["origin"] = "nnls_patch"
            ds.attrs["s_idx"]  = s_idx
            ds.attrs["k_per_comp"] = int(k_per_comp)
            ds.attrs["pick_mode"]  = str(pick_mode)
            ds.attrs["solver"]     = str(solver)
            ds.attrs["ridge"]      = float(ridge)
            ds.attrs["norm.mode_at_fit"] = norm_mode
    
    usage_png = os.path.join(out_dir, "nnlsPatchUsage.png") if out_dir else None

    metrics = None
    if write_seed:
        # Seed is in the main HDF5 at `seed_path`
        try:
            metrics = compare_usage_to_orbit_weights(
                h5_path,
                sidecar=None,
                x_dset=seed_path, # e.g. "/Seeds/x0_nnls_patch"
                normalize="unit_sum",
                out_png=usage_png,
                usage_metric="energy", # use energy–weighted usage
                E_cp=E, # (C,P) from read_global_column_energy
            )
        except Exception as e:
            print(f"[nnls_patch] usage-vs-prior (seed) failed: {e}")

    else:
        import tempfile
        tmp_sidecar = None
        try:
            tmp = tempfile.NamedTemporaryFile(
                prefix=os.path.basename(h5_path)+".fit.",
                suffix=".h5", delete=False
            )
            tmp_sidecar = tmp.name
            tmp.close()

            import h5py
            with h5py.File(tmp_sidecar, "w") as G:
                G.create_dataset("/Fit/x_best", data=x_CP.astype("f8"),
                                 dtype="f8",)

            metrics = compare_usage_to_orbit_weights(
                h5_path,
                sidecar=tmp_sidecar,
                x_dset="/Fit/x_best",
                normalize="unit_sum",
                out_png=usage_png,
                usage_metric="energy",
                E_cp=E,
            )
        except Exception as e:
            print(f"[nnls_patch] usage-vs-prior (temp sidecar) failed: {e}")
        finally:
            if tmp_sidecar and os.path.exists(tmp_sidecar):
                try:
                    os.remove(tmp_sidecar)
                except OSError:
                    pass

    if metrics is not None:
        print(
            "[nnls_patch] usage-vs-prior:"
            f" L1={metrics.get('l1', float('nan')):.3e}"
            f"  L∞={metrics.get('linf', float('nan')):.3e}"
            f"  cos={metrics.get('cosine', float('nan')):.4f}"
            f"  r={metrics.get('pearson_r', float('nan')):.4f}"
            f"  plot={metrics.get('plot_path')}"
        )

    return dict(
        x_CP=x_CP, picks=picks, s_idx=s_idx, rmse=rmse, chi2=chi2,
        meta=dict(norm_mode=norm_mode, rows=int(rows), cols=int(K),
                  mask_used=bool(use_mask), lambda_used=bool(use_lambda),
                  solver=solver, ridge=float(ridge),
                  finite_min=int(finite_counts.min()),
                  finite_med=int(np.median(finite_counts)),
                  finite_max=int(finite_counts.max()))
    )

# -------------------------------- CLI ----------------------------------------

def main():
    ap = argparse.ArgumentParser(description="Patch-scale NNLS diagnostic fitter (self-contained)")
    ap.add_argument("h5", help="Path to CubeFit HDF5")
    ap.add_argument("--spax", default=None, help="spaxel selection: 'start:count' or 'i,j,k'")
    ap.add_argument("--k-per-comp", type=int, default=12, help="populations per component")
    ap.add_argument("--pick", default="energy", choices=["energy","random"], help="how to pick pops")
    ap.add_argument("--solver", default="nnls", choices=["nnls","lsq","pg"], help="NNLS solver")
    ap.add_argument("--ridge", type=float, default=0.0, help="L2 ridge (lsq/pg)")
    ap.add_argument("--no-mask", action="store_true", help="ignore /Mask")
    ap.add_argument("--no-lambda", action="store_true", help="ignore /HyperCube/lambda_weights")
    ap.add_argument("--lam-dset", default="/HyperCube/lambda_weights", help="λ-weights dataset")
    ap.add_argument("--out", default=None, help="directory for diagnostic plots")
    ap.add_argument("--write-seed", action="store_true", help="write /Seeds/x0_nnls_patch")
    ap.add_argument("--seed-path", default="/Seeds/x0_nnls_patch", help="dataset to write seed")
    args = ap.parse_args()

    res = run_patch(
        h5_path=args.h5,
        s_sel=args.spax,
        k_per_comp=int(args.k_per_comp),
        pick_mode=args.pick,
        solver=args.solver,
        ridge=float(args.ridge),
        use_mask=not args.no_mask,
        use_lambda=not args.no_lambda,
        lam_dset=args.lam_dset,
        out_dir=args.out,
        write_seed=bool(args.write_seed),
        seed_path=args.seed_path,
    )
    small = {k: (f"array{v.shape}" if isinstance(v, np.ndarray) else v) for k, v in res.items() if k != "picks"}
    print("[nnls_patch] done.", small)
    print(res['x_CP'])

if __name__ == "__main__":
    main()
</file>

<file path="live_fit_dashboard.py">
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import argparse, glob, os
from pathlib import Path
import h5py
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import Normalize

from CubeFit.hypercube_reader import HyperCubeReader, ReaderCfg
from CubeFit.hdf5_manager import open_h5
from CubeFit.logger import get_logger

logger = get_logger()

divcmap = 'GECKOSdr'
moncmap = 'inferno'
moncmapr = 'inferno_r'

# ------------------------------------------------------------------------------

def render_aperture_fits_separate(h5_path: str,
                                  out_dir: str,
                                  apertures: list[int],
                                  sidecar: str | None = None,
                                  show_residual: bool = True,
                                  title_prefix: str | None = None,
                                  dpi: int = 150):
    """
    Write one figure per spaxel (isolated), using the same masked reader the solver uses.
    File names: <out_dir>/fit_spax<NNNN>.png
    """
    os.makedirs(out_dir, exist_ok=True)

    # pick x (same priority order as dashboard)
    with open_h5(h5_path, role="reader") as F:
        if sidecar and os.path.exists(sidecar):
            with open_h5(sidecar, role="reader", swmr=True) as G:
                x = _read_latest_x(G, F)
        else:
            x = _read_latest_x(None, F)
    X_vec = np.asarray(x, float).ravel()

    reader = HyperCubeReader(h5_path, ReaderCfg(dtype_models="float32", apply_mask=True))
    try:
        for s in apertures:
            A, y = reader.read_spaxel_plane(int(s))  # masked A:(N,L_eff), y:(L_eff,)
            yhat = A.T @ X_vec
            lam = np.arange(y.size, dtype=int)
            good = np.isfinite(y)  # mirror solver's finite-row rule

            # choose layout
            if show_residual:
                fig, axes = plt.subplots(2, 1, figsize=(12, 6.5), sharex=True)
                ax0, ax1 = axes
            else:
                fig, ax0 = plt.subplots(1, 1, figsize=(12, 3.8))
                ax1 = None

            # top: data & model
            ax0.plot(lam[good], y[good],    lw=1.0, alpha=0.75, label="observed")
            ax0.plot(lam[good], yhat[good], lw=1.0, alpha=0.95, ls="--", label="model")
            ax0.set_ylabel("flux")
            ax0.legend(loc="best")
            t = f"Spaxel {s}"
            if title_prefix:
                t = f"{title_prefix} — {t}"
            ax0.set_title(t)

            # bottom: residual (optional)
            if show_residual and ax1 is not None:
                r = yhat - y
                ax1.plot(lam[good], r[good], lw=0.9)
                ax1.axhline(0.0, ls="--", lw=0.8)
                ax1.set_xlabel("pixel")
                ax1.set_ylabel("model - data")

            try:
                fig.tight_layout()
            except Exception:
                pass

            out_path = os.path.join(out_dir, f"fit_spax{int(s):05d}.png")
            fig.savefig(out_path, dpi=dpi)
            plt.close(fig)
            print(f"[Aperture] wrote {out_path}")
    finally:
        reader.close()

# --- Jacobi diagnostics helpers ----------------------------------------------

def alpha_star_stats(h5_path, x_vec, n_spax=32, tile=None, swmr_main: bool | None = None):
    """
    Compute alpha* over a small sample. For main file reads during diagnostics,
    keep swmr_main=None (no SWMR) to avoid flag conflicts. Use SWMR only for
    sidecar reads where a writer is active.
    """
    def _compute(F):
        M = F["/HyperCube/models"]; Y = F["/DataCube"]
        Mask = np.asarray(F["/Mask"][...], bool).ravel() if "/Mask" in F else None
        S, C, P, L = map(int, M.shape)
        keep = np.flatnonzero(Mask) if Mask is not None else None

        if tile is None:
            s0, s1 = 0, int(np.min((S, n_spax)))
        else:
            s0, s1 = int(tile[0]), int(np.min((tile[1], S)))
        idx = np.linspace(s0, s1 - 1, int(np.min((n_spax, s1 - s0))), dtype=int)

        X = np.asarray(x_vec, float).reshape(C, P)
        alphas = []
        for s in idx:
            slab = np.asarray(M[s, :, :, :], float)
            if keep is not None:
                slab = slab[:, :, keep]
            yhat = np.tensordot(slab, X, axes=([0, 1], [0, 1]))
            y    = np.asarray(Y[s, :], float)
            if keep is not None:
                y = y[keep]
            num = float(np.dot(y, yhat))
            den = float(np.dot(yhat, yhat) + 1e-300)
            alphas.append(num / den)
        a = np.array(alphas, float)
        return {
            "median": float(np.median(a)),
            "min": float(np.min(a)),
            "max": float(np.max(a)),
            "n": int(a.size),
        }

    # Preferred: no SWMR for the main H5 during diagnostics
    try:
        with open_h5(h5_path, role="reader", swmr=(True if swmr_main else None)) as f:
            return _compute(f)
    except OSError as e:
        # If someone left a conflicting handle open with a different SWMR flag,
        # retry without SWMR to avoid “SWMR read access flag … already open”.
        if "SWMR read access flag" in str(e):
            with open_h5(h5_path, role="reader") as f:
                return _compute(f)
        raise

def render_aperture_fits_with_x(
    h5_path,
    x_vec,
    out_png,
    apertures,
    show_residual=True,
    title=None,
):
    """
    Render fits for selected spaxels using a global solution x_vec, but
    stream the HyperCube per-component to avoid a large (C, P, L) slab
    in memory.
    """
    with open_h5(h5_path, role="reader") as f:
        M = f["/HyperCube/models"]   # (S, C, P, L)
        Y = f["/DataCube"]           # (S, L)

        Mask = (
            np.asarray(f["/Mask"][...], bool).ravel()
            if "/Mask" in f
            else None
        )

        S, C, P, L = map(int, M.shape)

        X = np.asarray(x_vec, float).reshape(C, P)   # same basis as x_vec

        # Precompute λ indices once
        if Mask is not None:
            keep = np.flatnonzero(Mask)
        else:
            keep = None

        fig, axes = plt.subplots(
            len(apertures),
            1,
            figsize=(12, 2.7 * len(apertures)),
        )
        if len(apertures) == 1:
            axes = [axes]

        for ax, s in zip(axes, apertures):
            if not (0 <= s < S):
                raise IndexError(f"spaxel index {s} out of range [0, {S})")

            # Observed spectrum
            y = np.asarray(Y[s, :], float)   # (L,)

            # Model spectrum: stream over components c
            yhat = np.zeros(L, float)
            for c in range(C):
                # A_c has shape (P, L) for this spaxel / component
                A_c = np.asarray(M[s, c, :, :], float)  # (P, L)
                # dot over P: (P,) @ (P, L) -> (L,)
                yhat += X[c, :].astype(float, copy=False) @ A_c

            if keep is not None:
                y_plot = y[keep]
                yhat_plot = yhat[keep]
                good = np.isfinite(y_plot)
                lam_idx = np.flatnonzero(good)
                ax.plot(lam_idx, y_plot[good], label="data")
                ax.plot(lam_idx, yhat_plot[good], label="model")
                if show_residual:
                    ax.plot(
                        lam_idx,
                        (y_plot - yhat_plot)[good],
                        label="residual",
                    )
            else:
                good = np.isfinite(y)
                lam_idx = np.flatnonzero(good)
                ax.plot(lam_idx, y[good], label="data")
                ax.plot(lam_idx, yhat[good], label="model")
                if show_residual:
                    ax.plot(
                        lam_idx,
                        (y - yhat)[good],
                        label="residual",
                    )

            ax.set_title(f"spaxel {s}")
            ax.set_xlabel("λ (log space)")
            ax.set_ylabel("flux")
            ax.legend(loc="upper right")

        if title:
            fig.suptitle(title)
        fig.tight_layout(rect=(0, 0, 1, 0.96) if title else None)
        fig.savefig(out_png, dpi=130)
        plt.close(fig)

def render_sfh_from_x(h5_path: str,
                      x_flat: np.ndarray,
                      out_png: str):
    """
    Simple Age×Z panels (per α) from x. Uses /Templates.attrs['pop_shape'].
    """

    x_flat = np.asarray(x_flat, dtype=np.float64).ravel()
    with open_h5(h5_path, role="reader") as f:
        C = int(f["/HyperCube/models"].shape[1])
        T = f["/Templates"]
        pop_shape = T.attrs.get("pop_shape", None)
        if pop_shape is None:
            # fallback: treat templates as 1-D population axis
            P = int(T.shape[0])
            pop_shape = (P,)
        pop_shape = tuple(int(v) for v in pop_shape)
        P = int(np.prod(pop_shape))
        if x_flat.size != C * P:
            raise ValueError(f"x has length {x_flat.size}, expected C*P={C*P}.")

        # reshape to (C, nZ, nT, nA) or (C, P) degenerate
        X = x_flat.reshape((C,) + pop_shape, order="C")
        if len(pop_shape) == 1:
            # nothing to panel; just a single heatmap
            W = X.sum(axis=0)  # (P,)
            fig, ax = plt.subplots(1, 1, figsize=(6, 3))
            ax.plot(np.arange(P), W, lw=1)
            ax.set_title("Population weights (flattened)")
            fig.savefig(out_png, dpi=130); plt.close(fig); return

        # assume (nZ, nT, nA) order
        nZ, nT, nA = pop_shape
        W = X.sum(axis=0) # (nZ, nT, nA)
        vmax = float(np.max(np.log10(W))) if np.isfinite(W).any() else 1.0

        fig, axes = plt.subplots(1, nA, figsize=(3.0*nA, 3.4), squeeze=False)
        axes = axes[0]
        for a in range(nA):
            ax = axes[a]
            im = ax.imshow(np.log10(W[:, :, a]), origin="lower", aspect="auto",
                cmap=moncmapr, norm=Normalize(vmin=0.0, vmax=vmax))
            ax.set_title(f"α index {a}")
            ax.set_xlabel("Age index")
            if a == 0:
                ax.set_ylabel("Metal index")
            if not ax.get_subplotspec().is_first_col():
                ax.set_yticklabels([])
        fig.colorbar(im, ax=axes.tolist(), shrink=0.8, pad=0.02)
        fig.savefig(out_png, dpi=130); plt.close(fig)

    logger.log(f"[JacobiDiag] wrote {out_png}")

# ---------- helpers -----------------------------------------------------------

def _minutes_since_start(ts):
    if ts.size == 0:
        return ts
    t0 = float(ts[0])
    return (ts - t0) / 60.0

def _find_latest_sidecar(main_path: str) -> str | None:
    """
    Look for sidecars created by the new FitTracker:
      <main>.fit.<pid>.<ts>.h5
    Return the newest one by mtime, or None if not found.
    """
    pat = f"{os.fspath(Path(main_path))}.fit.*.h5"
    cand = glob.glob(pat)
    if not cand:
        return None
    cand.sort(key=lambda p: os.path.getmtime(p))
    return cand[-1]

def _ewma(x, alpha=0.2):
    if x.size == 0:
        return x
    y = np.empty_like(x, dtype=float)
    s = 0.0
    for i, v in enumerate(np.asarray(x, float)):
        s = alpha * v + (1.0 - alpha) * (s if i else v)
        y[i] = s
    return y

def _load_metrics_new(side_f: h5py.File):
    """
    New tracker: histories live as flat datasets. Time stamps are not
    guaranteed; we synthesize a monotonic axis using indices (minutes).
    """
    gfit = side_f["/Fit"]
    epoch = side_f["/Fit/epoch_hist"][...] if "/Fit/epoch_hist" in side_f else np.array([], int)
    rmse  = side_f["/Fit/rmse_hist"][...]  if "/Fit/rmse_hist"  in side_f else np.array([], float)

    # monotone "time" in minutes (index-based)
    if epoch.size:
        time_sec = np.linspace(0.0, float(epoch.size), epoch.size, dtype=float)
    else:
        time_sec = np.array([], float)

    train_rmse_ewma = _ewma(rmse, alpha=0.2) if rmse.size else np.array([], float)

    # placeholders for fields the old dashboard expects
    val_rmse  = np.array([], float)
    dx_rel    = np.array([], float)
    nnz       = np.array([], float)
    l1        = np.array([], float)
    l2        = np.array([], float)
    me_L1     = np.array([], float)
    me_Linf   = np.array([], float)
    return (epoch.astype(int), time_sec, train_rmse_ewma, val_rmse,
            dx_rel, nnz, l1, l2, me_L1, me_Linf)

def _load_metrics_old(main_f: h5py.File):
    """
    Legacy tracker: metrics group with multiple 1D series.
    """
    if "/Fit/metrics" not in main_f:
        z = np.array([], float); zi = np.array([], int)
        return (zi, z, z, z, z, z, z, z, z, z)
    m = main_f["/Fit/metrics"]
    g = lambda n: m[n][...] if n in m else np.array([], float)
    return (g("epoch").astype(int), g("time_sec"), g("train_rmse_ewma"),
            g("val_rmse"), g("delta_x_rel"), g("nnz"), g("l1_norm"),
            g("l2_norm"), g("mass_err_L1"), g("mass_err_Linf"))

def _choose_sample_spaxels(main_f: h5py.File, n: int) -> np.ndarray:
    # Try new/old sample lists, else pick evenly spaced indices
    if "/Fit/sample/spaxels" in main_f:
        v = np.asarray(main_f["/Fit/sample/spaxels"][...], int)
        if v.size:
            return v[:min(n, v.size)]
    S = int(main_f["/DataCube"].shape[0])
    if n >= S:
        return np.arange(S, dtype=int)
    return np.linspace(0, S - 1, n, dtype=int)

def _read_latest_x(side_f: h5py.File | None, main_f: h5py.File) -> np.ndarray:
    """
    Preference order (sidecar first):
      1) /Fit/x_best
      2) /Fit/x_last
      3) /Fit/x_epoch_last
      4) last row of /Fit/x_snapshots  (ring/snapshots)
      5) last row of /Fit/x_hist       (legacy ring)
    Fallbacks (main file):
      6) /Fit/x_latest
      7) /X_global
    """
    def _row_or_vec(dset) -> np.ndarray:
        arr = np.asarray(dset[...], float)
        # accept 1-D (N,) or 2-D (T,N); if 2-D, take last row
        if arr.ndim == 2 and arr.shape[0] > 0:
            return arr[-1, :].astype(float, copy=False)
        return arr.ravel().astype(float, copy=False)

    # --- Sidecar first (if present)
    if side_f is not None:
        if "/Fit/x_best" in side_f:
            print("[Dashboard] Using /Fit/x_best from sidecar.")
            return _row_or_vec(side_f["/Fit/x_best"])

        if "/Fit/x_last" in side_f:
            print("[Dashboard] Using /Fit/x_last from sidecar.")
            return _row_or_vec(side_f["/Fit/x_last"])

        if "/Fit/x_epoch_last" in side_f:
            print("[Dashboard] Using /Fit/x_epoch_last from sidecar.")
            return _row_or_vec(side_f["/Fit/x_epoch_last"])

        if "/Fit/x_snapshots" in side_f and side_f["/Fit/x_snapshots"].shape[0] > 0:
            print("[Dashboard] Using last row of /Fit/x_snapshots from sidecar.")
            return _row_or_vec(side_f["/Fit/x_snapshots"])

        if "/Fit/x_hist" in side_f and side_f["/Fit/x_hist"].shape[0] > 0: # legacy
            print("[Dashboard] Using last row of /Fit/x_hist from sidecar.")
            return _row_or_vec(side_f["/Fit/x_hist"])

    # --- Main file fallbacks (legacy)
    if "/Fit/x_latest" in main_f:
        print("[Dashboard] Using /Fit/x_latest from main file.")
        return _row_or_vec(main_f["/Fit/x_latest"])

    if "/X_global" in main_f:
        print("[Dashboard] Using /X_global from main file.")
        return _row_or_vec(main_f["/X_global"])

    raise RuntimeError("No solution vector found in sidecar or main file.")

def _read_orbit_weights(side_f: h5py.File | None, main_f: h5py.File) -> np.ndarray | None:
    if side_f is not None and "/CompWeights" in side_f:
        return np.asarray(side_f["/CompWeights"][...], float)
    if "/Fit/orbit_weights" in main_f:
        return np.asarray(main_f["/Fit/orbit_weights"][...], float)
    if "/CompWeights" in main_f:
        return np.asarray(main_f["/CompWeights"][...], float)
    return None

# ---------- main API: dashboard ------------------------------------------------

def render_dashboard(h5_path: str, out_png: str,
                     n_spaxels: int = 8, downsample: int = 512,
                     figsize=(14, 10), sidecar: str | None = None):
    """
    Render the live dashboard (new sidecar or legacy). Uses HyperCubeReader with
    apply_mask=True so A and y match the solver exactly.
    """
    sidecar = sidecar or _find_latest_sidecar(h5_path)

    print(f"[Dashboard] Rendering {out_png} from {h5_path} "+
          (f"and sidecar {sidecar}" if sidecar else "(legacy)"))

    with open_h5(h5_path, role="reader", swmr=True) as F:
        # ---- metrics + solution vector ---------------------------------------
        if sidecar and os.path.exists(sidecar):
            with open_h5(sidecar, role="reader", swmr=True) as G:
                epoch, time_sec, tr_ewma, val_rmse, dx_rel, nnz, l1, l2, me1, meinf = _load_metrics_new(G)
                x = _read_latest_x(G, F)
                ow = _read_orbit_weights(G, F)
        else:
            epoch, time_sec, tr_ewma, val_rmse, dx_rel, nnz, l1, l2, me1, meinf = _load_metrics_old(F)
            x = _read_latest_x(None, F)
            ow = _read_orbit_weights(None, F)

        fig, axes = plt.subplots(2, 2, figsize=figsize)

        # ---- (0,0) Convergence ------------------------------------------------
        ax = axes[0, 0]
        tmin = _minutes_since_start(time_sec) if time_sec.size else np.arange(tr_ewma.size, dtype=float)
        if tr_ewma.size:
            ax.plot(tmin, tr_ewma, label="train_rmse_ewma")
        if val_rmse.size:
            n = min(val_rmse.size, tmin.size if tmin.size else val_rmse.size)
            x_ax = tmin[:n] if tmin.size else np.arange(n, dtype=float)
            ax.plot(x_ax, val_rmse[:n], marker="o", linestyle="none", label="val_rmse")
        ax.set_xlabel("minutes since start" if time_sec.size else "updates")
        ax.set_ylabel("RMSE")
        ax.set_title("Convergence")
        if tr_ewma.size or val_rmse.size:
            ax.legend(loc="best")

        # ---- (0,1) Params & mass errors (plot only if present) ---------------
        ax = axes[0, 1]
        def _plot_series(series: np.ndarray, label: str) -> bool:
            if series.size == 0:
                return False
            if time_sec.size:
                n = min(series.size, time_sec.size)
                ax.plot(_minutes_since_start(time_sec)[:n], series[:n], label=label)
            else:
                ax.plot(np.arange(series.size, dtype=float), series, label=label)
            return True

        any_line = False
        any_line |= _plot_series(l1,     "||x||₁")
        any_line |= _plot_series(l2,     "||x||₂")
        any_line |= _plot_series(nnz,    "nnz(x)")
        any_line |= _plot_series(dx_rel, "Δx_rel")
        any_line |= _plot_series(me1,    "mass_err_L1")
        any_line |= _plot_series(meinf,  "mass_err_L∞")
        ax.set_xlabel("minutes since start" if time_sec.size else "updates")
        ax.set_title("Params & mass errors")
        if any_line:
            ax.legend(loc="best")

        # ---- (1,0) Observed vs model (subset) with spectral mask -------------
        ax = axes[1, 0]
        reader = HyperCubeReader(h5_path, ReaderCfg(dtype_models="float32", apply_mask=True))
        try:
            C, P = reader.nComp, reader.nPop
            X_vec = np.asarray(x, float).ravel()  # length N=C*P
            spaxels = _choose_sample_spaxels(F, n_spaxels)
            for s in spaxels:
                A, y = reader.read_spaxel_plane(int(s))  # A:(N,L_eff), y:(L_eff,)
                L_eff = y.size

                if downsample and L_eff > downsample:
                    idx = np.linspace(0, L_eff - 1, downsample, dtype=int)
                    lam_plot = idx
                    y_plot = y[idx]
                    yhat_plot = (A[:, idx].T @ X_vec)
                else:
                    lam_plot = np.arange(L_eff, dtype=int)
                    y_plot = y
                    yhat_plot = (A.T @ X_vec)

                # mirror solver: only data finiteness defines the mask
                good = np.isfinite(y_plot)

                ax.plot(lam_plot[good], y_plot[good],    alpha=0.6,  lw=1.0, label=None)
                ax.plot(lam_plot[good], yhat_plot[good], alpha=0.95, lw=1.0, ls="--", label=None)
        finally:
            reader.close()
        ax.set_xlabel("pixel")
        ax.set_title("Observed vs model (subset)")

        # ---- (1,1) Component usage vs target (needs X reshaped C×P) ----------
        ax = axes[1, 1]
        # Use reader's C,P to reshape safely
        X_cp = np.asarray(x, float).ravel().reshape(C, P)
        usage = X_cp.sum(axis=1)
        ax.bar(np.arange(C), usage, label="usage", color='r')
        if ow is not None and ow.size == usage.size:
            ax.plot(np.arange(C), ow, marker="o", linestyle="--", label="target w_c", color='k')
            maxerr = float(np.max(np.abs(usage - ow))) if usage.size else np.nan
            ax.set_title(f"Component usage vs target (max|e|={maxerr:.3g})")
            ax.set_ylim(0.0, float(1.05 * np.max(ow)) if ow.size else 1.0)
        else:
            ax.set_title("Component usage (sum over P)")
            ax.set_ylim(0.0, float(1.05 * np.max(usage)) if usage.size else 1.0)
        ax.set_xlabel("component c")
        ax.legend(loc="best")

        fig.savefig(out_png, dpi=140)
        plt.close(fig)
        print(f"[Dashboard] Wrote {out_png}  [{Path(sidecar).name if sidecar else 'legacy'}]")

def render_aperture_fits(h5_path: str, out_png: str,
                         apertures: list[int],
                         sidecar: str | None = None,
                         show_residual: bool = True,
                         title: str | None = None):
    """
    Full-resolution observed vs model (and residual) for specific spaxels.
    Uses the masked reader so A,y match the solver.
    """
    sidecar = sidecar or _find_latest_sidecar(h5_path)

    # pick x (same priority order as dashboard)
    with open_h5(h5_path, role="reader") as F:
        if sidecar and os.path.exists(sidecar):
            with open_h5(sidecar, role="reader", swmr=True) as G:
                x = _read_latest_x(G, F)
        else:
            x = _read_latest_x(None, F)
    X_vec = np.asarray(x, float).ravel()

    reader = HyperCubeReader(h5_path, ReaderCfg(dtype_models="float32", apply_mask=True))
    try:
        n = len(apertures)
        ncols = 2 if show_residual else 1
        figsize = (14, 3.5 * n) if ncols == 1 else (14, 5.5 * n)
        fig, axes = plt.subplots(n, ncols, figsize=figsize, squeeze=False)

        for row, s in enumerate(apertures):
            A, y = reader.read_spaxel_plane(int(s))  # masked A:(N,L_eff), y:(L_eff,)
            yhat = A.T @ X_vec
            lam = np.arange(y.size, dtype=int)

            ax0 = axes[row, 0]
            ax0.plot(lam, y,    lw=1.0, alpha=0.7, label="observed")
            ax0.plot(lam, yhat, lw=1.0, alpha=0.9, label="model")
            ax0.set_ylabel(f"spax {s}")
            ax0.legend(loc="best")
            ax0.set_xlabel("pixel")

            if show_residual:
                r = y - yhat
                ax1 = axes[row, 1]
                ax1.plot(lam, r, lw=0.9)
                ax1.axhline(0.0, ls="--", lw=0.8)
                ax1.set_ylabel("residual")
                ax1.set_xlabel("pixel")

        if title:
            fig.suptitle(title, y=0.995, fontsize=12)
        fig.savefig(out_png, dpi=150)
        plt.close(fig)
        print(f"Wrote {out_png}")
    finally:
        reader.close()

# -----------------------------------------------------------------------------

def main():
    ap = argparse.ArgumentParser(
        description="Render live Kaczmarz fit dashboard (new sidecar or legacy) "
                    "and optional full-resolution per-aperture overlays."
    )
    ap.add_argument("h5_path", help="Main HDF5 path (contains /HyperCube and /DataCube)")
    ap.add_argument("--out", default="fit_live.png")
    ap.add_argument("--n-spaxels", type=int, default=8)
    ap.add_argument("--downsample", type=int, default=512)
    ap.add_argument("--sidecar", default=None,
                    help="Optional explicit sidecar path; otherwise auto-detects")
    ap.add_argument("--apertures", default="",
                    help="Comma-separated spaxel indices for detailed full-L fits")
    ap.add_argument("--apertures-separate-dir", default="",
                help="If set (and --apertures provided), also write one PNG per spaxel to this directory.")
    args = ap.parse_args()

    # 1) Always write the main dashboard
    render_dashboard(args.h5_path, args.out, n_spaxels=args.n_spaxels,
        downsample=args.downsample, sidecar=args.sidecar)

    # 2) Optionally also write the per-aperture full-resolution fits (combined)
    if args.apertures.strip():
        aps = [int(t) for t in args.apertures.replace(" ", "").split(",") if t]
        base = Path(args.out)
        fits_out = str(base.with_name(base.stem + "_fits" + base.suffix))
        render_aperture_fits(args.h5_path, fits_out, apertures=aps,
                            sidecar=args.sidecar, show_residual=True,
                            title=f"Observed vs model (spaxels={aps})")

        # 3) If requested, write separate figures per spaxel
        if args.apertures_separate_dir.strip():
            render_aperture_fits_separate(
                args.h5_path,
                args.apertures_separate_dir,
                apertures=aps,
                sidecar=args.sidecar,
                show_residual=True,
                title_prefix="Observed vs model"
            )

if __name__ == "__main__":
    main()

# ipython -- /data/phys-gal-dynamics/phys2603/CubeFit/live_fit_dashboard.py /data/phys-gal-dynamics/phys2603/CubeFit/NGC4365/NGC4365_12.h5 --out /data/phys-gal-dynamics/phys2603/CubeFit/NGC4365/figures/fit_live.png --apertures 0,2000,1400,740 --apertures-separate-dir /data/phys-gal-dynamics/phys2603/CubeFit/NGC4365/figures/apertures/
</file>

<file path="kz_2Fit.sh">
#!/bin/bash -l
# #SBATCH -A durham
# #SBATCH -D "/cosma5/data/durham/dc-poci1/muse"
# #SBATCH --output="/cosma5/data/durham/dc-poci1/muse/slurm_tri_losvdDecomp.log" --open-mode=append
# #SBATCH --error="/cosma5/data/durham/dc-poci1/muse/slurm_tri_losvdDecomp.log" --open-mode=append

# #SBATCH -A oz059
# #SBATCH -D "/fred/oz059/poci/muse"
# #SBATCH --output="/fred/oz059/poci/muse/slurm_tri_losvdDecomp.log" --open-mode=append
# #SBATCH --error="/fred/oz059/poci/muse/slurm_tri_losvdDecomp.log" --open-mode=append

#SBATCH -D "/data/phys-gal-dynamics/phys2603/muse"
#SBATCH --output="/data/phys-gal-dynamics/phys2603/CubeFit/log_2Fit.log" --open-mode=append
#SBATCH --error="/data/phys-gal-dynamics/phys2603/CubeFit/log_2Fit.log" --open-mode=append
#SBATCH -p short

#SBATCH --job-name="CubeFit_2Fit"
#SBATCH --time=0-12:00
#SBATCH -N 1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=48
#SBATCH --mem=200G
#SBATCH --hint=nomultithread
#SBATCH --exclusive
#SBATCH --mail-type=ALL
#SBATCH --mail-user=adriano.poci@physics.ox.ac.uk

module purge
module load foss/2023a
module load Python/3.11.3-GCCcore-12.3.0

# --- BLAS / OpenMP threading (per worker) ---
export OMP_NUM_THREADS=12
export OPENBLAS_NUM_THREADS=12
export MKL_NUM_THREADS=12
export NUMEXPR_NUM_THREADS=12
export OMP_PROC_BIND=spread
export OMP_PLACES=cores
export OMP_DYNAMIC=FALSE
export MKL_DYNAMIC=FALSE
# For OpenBLAS, avoid accidental main-thread reuse:
export OPENBLAS_VERBOSE=0

# --- HDF5 raw-data chunk cache (reader side) ---
export CUBEFIT_RDCC_NBYTES=$((16*1024*1024*1024))  # 16 GiB
export CUBEFIT_RDCC_NSLOTS=400003                   # large-ish prime
export CUBEFIT_RDCC_W0=0.9

export HDF5_USE_FILE_LOCKING=FALSE

# File descriptors
ulimit -n 8192

cd /data/phys-gal-dynamics/phys2603/CubeFit

# sanity print (once) to confirm cpuset and BLAS threads
srun -n1 -c${SLURM_CPUS_PER_TASK} --cpu-bind=cores \
  python - <<'PY'
import os, json
print(f"[sanity] cpuset cores: {len(os.sched_getaffinity(0))}")
try:
    from threadpoolctl import threadpool_info
    print("[sanity] BLAS pools:", json.dumps(threadpool_info(), indent=2)[:600], "...")
except Exception as e:
    print("[sanity] threadpoolctl not available:", e)
PY

# run your job as a Slurm step (gives you the full cpuset)
srun -n1 -c${SLURM_CPUS_PER_TASK} --cpu-bind=cores \
  python -m IPython kz_run.py -- --run-switch fit
</file>

<file path="COMMIT_MESSAGE.txt">
* Switched to diagonal-preconditioned Spectral Projected Gradient method to replace Kaczmarz updates in `kaczmarz_solver_cchunk_mp_nnls.solve_global_kaczmarz_global_step_mp`
</file>

<file path="cube_utils.py">
# thread_utils.py
from __future__ import annotations
from contextlib import contextmanager
from typing import Optional, Sequence
import numpy as np
import os, glob, time, re
import pathlib as plp
from dataclasses import dataclass
import matplotlib.pyplot as plt

from dynamics.IFU.Constants import Constants
from CubeFit.hypercube_reader import HyperCubeReader, ReaderCfg
from CubeFit.hypercube_builder import read_global_column_energy
from CubeFit.hdf5_manager import open_h5

CTS = Constants()
C_KMS = CTS.c

@contextmanager
def blas_threads_ctx(n: Optional[int]):
    """
    Limit BLAS threads to `n` inside the context using threadpoolctl.
    If n is None, do nothing and let environment (e.g. OMP_NUM_THREADS)
    control threading.
    """
    if n is None:
        # respect environment; do not override
        yield
        return
    try:
        from threadpoolctl import threadpool_limits
    except Exception:
        # threadpoolctl not available; best-effort no-op
        yield
        return
    # Ensure positive int; treat <=0 as 'no limit' (no-op)
    n_int = int(n)
    if n_int <= 0:
        yield
        return
    with threadpool_limits(n_int):
        yield

def _cfg_float(cfg, name, default):
    v = getattr(cfg, name, None)
    return default if v is None else float(v)

def _cfg_int(cfg, name, default):
    v = getattr(cfg, name, None)
    return default if v is None else int(v)

def _cfg_bool(cfg, name, default):
    v = getattr(cfg, name, None)
    return default if v is None else bool(v)

# ------------------------------------------------------------------------------

def _find_latest_sidecar(main_path: str) -> str | None:
    pat = f"{os.fspath(main_path)}.fit.*.h5"
    cand = glob.glob(pat)
    if not cand:
        return None
    return max(cand, key=os.path.getmtime)

def _default_sidecar_path(main_h5: str) -> str:
    """
    Compose a unique, stable sidecar path next to the main HDF5.
    Example: /path/file.h5.fit.<JOBID or pid>.<unix_ts>.h5
    """
    ts = int(time.time())
    jid = os.environ.get("SLURM_JOB_ID")
    sid = os.environ.get("SLURM_STEP_ID")
    tag = f"{jid}.{sid}" if jid else f"pid{os.getpid()}"
    return f"{str(main_h5)}.fit.{tag}.{ts}.h5"

# ------------------------------------------------------------------------------

def cpuset_count():
    """
    Return (n_cores, mask_string) for current process cpuset.

    Tries, in order:
      1) /proc/self/cgroup → cpuset slice → {v1,v2} cpuset.cpus(.effective)
      2) root-level fallbacks
      3) sched_getaffinity(0)

    On failure returns (None, None).
    """
    import os

    def _parse_mask(s: str) -> tuple[int, str]:
        s = s.strip().strip("\n")
        if not s:
            return 0, ""
        n = 0
        out_parts = []
        for part in s.split(","):
            part = part.strip()
            if not part:
                continue
            if "-" in part:
                a, b = part.split("-", 1)
                a, b = int(a), int(b)
                if b < a:
                    a, b = b, a
                n += (b - a + 1)
                out_parts.append(f"{a}-{b}")
            else:
                a = int(part)
                n += 1
                out_parts.append(str(a))
        return n, ",".join(out_parts)

    # --- discover cpuset cgroup path for this process
    cg_cpuset = None
    try:
        with open("/proc/self/cgroup") as f:
            for line in f:
                # format: "<id>:<controllers>:<path>"
                parts = line.strip().split(":", 2)
                if len(parts) == 3:
                    ctrls = parts[1].split(",")
                    if "cpuset" in ctrls:
                        cg_cpuset = parts[2] or "/"
                        break
    except Exception:
        cg_cpuset = None

    candidates = []
    if cg_cpuset:
        # v1 mount style
        candidates.append(f"/sys/fs/cgroup/cpuset{cg_cpuset}/cpuset.cpus.effective")
        candidates.append(f"/sys/fs/cgroup/cpuset{cg_cpuset}/cpuset.cpus")
        # v2 unified style
        candidates.append(f"/sys/fs/cgroup{cg_cpuset}/cpuset.cpus.effective")
        candidates.append(f"/sys/fs/cgroup{cg_cpuset}/cpuset.cpus")

    # root-level fallbacks
    candidates += [
        "/sys/fs/cgroup/cpuset.cpus.effective",
        "/sys/fs/cgroup/cpuset/cpuset.cpus.effective",
        "/sys/fs/cgroup/cpuset/cpuset.cpus",
    ]

    for p in candidates:
        try:
            with open(p) as f:
                s = f.read().strip()
            if s:
                return _parse_mask(s)
        except Exception:
            continue

    # last resort: affinity
    try:
        cores = sorted(os.sched_getaffinity(0))
        if not cores:
            return None, None
        # compress to "0-3,5,7-8"
        ranges = []
        start = prev = None
        for c in cores:
            if start is None:
                start = prev = c
            elif c == prev + 1:
                prev = c
            else:
                ranges.append((start, prev))
                start = prev = c
        if start is not None:
            ranges.append((start, prev))
        parts = [f"{a}-{b}" if a != b else str(a) for a, b in ranges]
        return len(cores), ",".join(parts)
    except Exception:
        return None, None

# ------------------------------------------------------------------------------

def ensure_lambda_weights(
    h5_path: str,
    dset_name: str = "/HyperCube/lambda_weights",
    *,
    apply_mask: bool = True,
    floor: float = 1e-6,
    min_w: float = 0.2,
    smooth1: int = 7,
    smooth2: int = 31,
    pct: float = 95.0,
    alpha: float = 0.9,   # modest contrast; set 1.0 to disable
) -> np.ndarray:
    """Build line-emphasis weights w(λ) from the median data spectrum.
       w = min_w + (1-min_w) * normalize(|boxcar(med, s1)-boxcar(med, s2)|)^alpha
       → clip to [floor, 1] and write to HDF5.
    """
    smooth1 = int(max(1, smooth1))
    smooth2 = int(max(smooth1+2, smooth2))
    alpha   = float(alpha)
    min_w   = float(min_w)
    floor   = float(floor)

    def _boxcar(x, win):
        k = np.ones(win, dtype=np.float64) / win
        return np.convolve(x, k, mode="same")

    with open_h5(h5_path, role="reader") as f:
        DC = f["/DataCube"]  # (S,L)
        S, L = map(int, DC.shape)
        mask = None
        if apply_mask and "/Mask" in f:
            m = np.asarray(f["/Mask"][...], bool).ravel()
            if m.size == L:
                mask = m

        med = np.median(np.asarray(DC[...], np.float64), axis=0)  # (L,)
        if mask is not None:
            med = med * mask

    lp1 = _boxcar(med, smooth1)
    lp2 = _boxcar(med, smooth2)
    dog = np.abs(lp1 - lp2)

    # robust scale to [0,1]
    pos = dog[dog > 0]
    s   = np.percentile(pos, pct) if pos.size else 1.0
    z   = np.clip(dog / (s + 1e-12), 0.0, 1.0)

    w = min_w + (1.0 - min_w) * (z ** alpha)
    if mask is not None:
        w = w * mask  # masked λ -> 0 (will be clipped to floor)
    w = np.clip(w, floor, 1.0).astype(np.float64)

    with open_h5(h5_path, role="writer") as f:
        if dset_name in f:
            del f[dset_name]
        ds = f.create_dataset(dset_name, data=w, dtype="f8", compression="gzip")
        ds.attrs.update(dict(method="median-DoG", min_w=min_w, alpha=alpha,
                             smooth1=smooth1, smooth2=smooth2, pct=float(pct),
                             masked=bool(apply_mask)))
    return w

def read_lambda_weights(h5_path: str, dset_name: str = "/HyperCube/lambda_weights",
                        floor: float = 1e-6) -> np.ndarray:
    with open_h5(h5_path, role="reader") as f:
        if dset_name not in f:
            raise RuntimeError(f"Missing {dset_name}")
        w = np.asarray(f[dset_name][...], dtype=np.float64).ravel()
    return np.maximum(w, float(floor))

# --------------------------- HDF5 helpers ------------------------------

def _get_mask(f):
    """Return /Mask as boolean with strict semantics: True == keep."""
    if "/Mask" not in f:
        return None
    return np.asarray(f["/Mask"][...], dtype=bool).ravel()

# ------------------------------------------------------------------------------

def _bin_edges_from_loglam(loglam: np.ndarray) -> np.ndarray:
    loglam = np.asarray(loglam, float)
    mid = 0.5 * (loglam[1:] + loglam[:-1])
    edges = np.empty(loglam.size + 1, float)
    edges[1:-1] = mid
    # linear extrapolation at ends
    edges[0]  = loglam[0]  - (mid[0]  - loglam[0])
    edges[-1] = loglam[-1] + (loglam[-1] - mid[-1])
    return edges

def _delta_lambda_from_loglam(loglam: np.ndarray) -> np.ndarray:
    edges = _bin_edges_from_loglam(loglam)
    lam_edges = np.exp(edges)
    dlam = lam_edges[1:] - lam_edges[:-1]
    return dlam

def _apply_RT(t_row: np.ndarray, R_any: np.ndarray) -> np.ndarray:
    # /R_T is (T,L) or (L,T). We want result (L,)
    if R_any.ndim != 2:
        raise ValueError(f"/R_T must be 2D, got {R_any.shape}")
    T, = t_row.shape
    if R_any.shape == (T,):
        raise ValueError("Degenerate /R_T")
    if R_any.shape[0] == T:
        return (t_row.astype(np.float64) @ R_any.astype(np.float64))  # (L,)
    elif R_any.shape[1] == T:
        return (R_any.astype(np.float64) @ t_row.astype(np.float64))  # (L,)
    else:
        raise ValueError(f"/R_T shape {R_any.shape} incompatible with T={T}")

def _xcorr_shift(a: np.ndarray, b: np.ndarray) -> float:
    """Return subpixel shift (in pixels) to align b to a: shift>0 means b→right."""
    a = np.asarray(a, float); b = np.asarray(b, float)
    a = a - np.nanmean(a); b = b - np.nanmean(b)
    corr = np.correlate(a, b, mode="full")
    k0 = int(np.argmax(corr)) - (a.size - 1)
    i  = k0 + (a.size - 1)
    # quadratic refinement
    if 1 <= i < corr.size - 1:
        y0, y1, y2 = corr[i-1], corr[i], corr[i+1]
        denom = (y0 - 2*y1 + y2)
        delta = 0.5*(y0 - y2)/denom if denom != 0 else 0.0
    else:
        delta = 0.0
    return float(k0 + delta)

def _vel_to_pix_shift(vel: float, dlog: float, c_kms: float) -> float:
    # pixels ≈ ln(1+v/c)/dlog (exact on log-λ grid)
    return float(np.log1p(vel / c_kms) / dlog)

def run_wavelength_checks(h5_path: str,
                          s: int,
                          c: int,
                          p_list: Optional[Sequence[int]] = None,
                          max_p_auto: int = 6) -> None:
    """
    Step-by-step wavelength alignment checks for one (s, c) and a handful of p.
    - Validates grids and /R_T
    - Compares rebin(Templates[p]) vs models[s,c,p,:]
    - Prints expected vs measured pixel shift
    """
    with open_h5(h5_path, "reader") as f:
        TPT = np.asarray(f["/Templates"][...], float)        # (P,T)
        Tem = np.asarray(f["/TemPix"][...], float)           # (T,) log-λ (natural)
        Obs = np.asarray(f["/ObsPix"][...], float)           # (L,) log-λ (natural)
        DCs = np.asarray(f["/DataCube"][s, :], float)        # (L,)
        R_any = np.asarray(f["/R_T"][...])
        Msc = np.asarray(f["/HyperCube/models"][s, c, :, :], float)  # (P,L)
        H   = np.asarray(f["/LOSVD"][s, :, c], float)        # (V,)
        Vel = np.asarray(f["/VelPix"][...], float)           # (V,) km/s

    # 0) Grids sanity
    dtem = np.diff(Tem); dobs = np.diff(Obs)
    print("[grids] TemPix monotonic:", bool(np.all(dtem > 0)),
          "  ObsPix monotonic:", bool(np.all(dobs > 0)))
    print("[grids] median Δlogλ: tem =", float(np.median(dtem)), " obs =", float(np.median(dobs)))
    # Data vs ObsPix: they share the L axis; this confirms *wavelengths* are consistent inputs

    # 1) Flux-conserving /R_T sanity
    dlam_T = _delta_lambda_from_loglam(Tem)   # per-bin λ width
    dlam_O = _delta_lambda_from_loglam(Obs)
    # Push a flat-in-λ flux vector through R_T: input proportional to Δλ_T → expect Δλ_O
    y_ref = _apply_RT(dlam_T, R_any)                     # (L,)
    rel_err = float(np.max(np.abs(y_ref - dlam_O)) / np.maximum(np.max(dlam_O), 1e-30))
    print(f"[R_T] max rel error mapping Δλ_T→Δλ_O = {rel_err:.3e} (should be ≲1e-12–1e-8)")

    # 2) Choose populations p to test
    if p_list is None:
        # Auto-pick by column energy at this component, if available
        with open_h5(h5_path, "reader") as f:
            if "/HyperCube/col_energy" in f:
                E = np.asarray(f["/HyperCube/col_energy"][c, :], float)  # (P,)
                cand = np.argsort(E)[::-1][:int(np.minimum(max_p_auto, E.size))]
                p_list = list(map(int, cand))
            else:
                p_list = list(range(int(np.minimum(max_p_auto, TPT.shape[0]))))
    print(f"[pick] testing p indices: {p_list}")

    # 3) LOSVD mean velocity and expected pixel shift (sign!)
    amp = float(np.sum(H)) if np.sum(H) > 0 else 0.0
    mu_v = float(np.sum(H * Vel) / np.sum(H)) if amp > 0 else 0.0  # km/s
    dlog_tem = float(np.median(np.diff(Tem)))
    k_exp = _vel_to_pix_shift(mu_v, dlog_tem, C_KMS)
    print(f"[losvd] sum={amp:.6g}  mean(v)={mu_v:.3f} km/s  expected pixel shift ≈ {k_exp:+.3f} px")

    # 4) For each p: rebin template (no convolution) vs stored model column
    for p in p_list:
        t_row = TPT[p, :]                        # (T,)
        pre = _apply_RT(t_row, R_any)            # (L,)
        post = Msc[p, :]                         # (L,)

        # Remove overall scale for a clean shape comparison
        a = pre - np.nanmean(pre);  b = post - np.nanmean(post)
        na = float(np.linalg.norm(a)); nb = float(np.linalg.norm(b))
        if na > 0: a = a / na
        if nb > 0: b = b / nb

        k_meas = _xcorr_shift(a, b)
        # also align and report residual
        shift_px = int(np.round(k_meas))
        if shift_px > 0:
            b_al = np.pad(b, (shift_px, 0), mode="constant")[:b.size]
        elif shift_px < 0:
            b_al = np.pad(b, (0, -shift_px), mode="constant")[-shift_px:]
            b_al = np.pad(b_al, (0, b.size - b_al.size), mode="constant")
        else:
            b_al = b

        rmse_al = float(np.sqrt(np.mean((a - b_al)**2)))
        print(f"[p={int(p):3d}] xcorr shift={k_meas:+.3f} px   rmse(after int-align)={rmse_al:.4f}")

    # 5) Quick data vs best pre-convolution template similarity (sanity)
    #    (not a fit — just to ensure there isn't a huge calibration drift)
    # find template row whose pre-rebin resembles the data the most
    # (cheap cosine similarity)
    with open_h5(h5_path, "reader") as f:
        # use a small random subset if P is huge for speed; otherwise do all
        P = int(TPT.shape[0])
        take = np.arange(P) if P <= 2000 else np.random.default_rng(7).choice(P, 2000, replace=False)
    S_pre = []
    for p in take:
        pre = _apply_RT(TPT[p, :], R_any)
        u = pre - np.nanmean(pre)
        v = DCs  - np.nanmean(DCs)
        num = float(np.dot(u, v))
        den = float(np.linalg.norm(u) * np.linalg.norm(v)) or 1.0
        S_pre.append(num/den)
    S_pre = np.asarray(S_pre, float)
    print(f"[data↔pre-rebin] best cosine over {S_pre.size} templates: "
          f"{float(np.max(S_pre)):.4f}  median={float(np.median(S_pre)):.4f}")

    # stochastic row/column test (depending on orientation)
    T, L = TPT.shape[1], len(Obs)
    ones_T = np.ones(T)
    r = _apply_RT(ones_T, R_any)     # rebin a flat-in-pixel spectrum
    print("max|r-1| =", float(np.max(np.abs(r - 1.0))))

# ------------------------------------------------------------------------------

def estimate_global_velocity_bias_features(
    h5_path: str,
    *,
    sidecar: str | None = None,
    sample_spaxels: int = 192,
    feature_spaxels: int = 96,
    n_features: int = 24,
    half_window: int = 15,
    search_range_px: int = 6,
    cont_window: int | None = None,
    use_mask: bool = True,
    seed: int = 1234,
    show_progress: bool = True,
) -> dict:
    """
    Estimate a single global velocity bias (km/s) by correlating observed
    spectra with model predictions over *only* the strongest absorption
    features. This preserves pixel-level accuracy (no spectral downsampling),
    but dramatically reduces work by restricting to compact windows around
    deep features and by sampling fewer spaxels.

    The method:
      1) Build a robust reference line-depth profile from a subset of
         spaxels. Use a smoothed continuum to get residuals; pick the K
         deepest feature centers with a minimum spacing.
      2) For a (larger) set of spaxels, compute normalized dot products
         between observed spectra and model spectra over the union of
         those feature windows for integer pixel shifts k ∈ [-R, +R].
      3) Pick the best shift by aggregated score, then refine to sub-pixel
         via a quadratic fit around the peak. Report a per-spax sanity
         distribution (median/MAD) from the same procedure.

    Notes
    -----.
    * Preserves the solver's masked grid exactly (no regridding).
    * The search range should cover your suspected offset (e.g., ±6 px).
    * Feature windows are *global* and identical for all spaxels (no
      per-spax elastic shifts).

    Parameters
    ----------
    h5_path : str
        Path to the main HDF5 file containing ``/DataCube`` and
        ``/HyperCube/models``.
    sidecar : str or None, optional
        Path to a live-fit sidecar for reading ``x`` (prefers
        ``/Fit/x_best`` → ``/Fit/x_last`` → ``/Fit/x_epoch_last`` →
        ``/Fit/x_snapshots``). Falls back to main file (``/Fit/x_latest``,
        then ``/X_global``).
    sample_spaxels : int, default 192
        Number of spaxels used to accumulate the *global* correlation score.
    feature_spaxels : int, default 96
        Number of spaxels used to select deepest features and build the
        reference residual profile. Can be smaller than ``sample_spaxels``.
    n_features : int, default 24
        Number of feature centers (global) to select.
    half_window : int, default 15
        Half-width (in pixels) for each feature window; total window length
        is ``2*half_window + 1``. Windows are unioned.
    search_range_px : int, default 6
        Integer shift search half-range. Candidate shifts are
        ``[-search_range_px, ..., +search_range_px]``.
    cont_window : int or None, optional
        Boxcar width for continuum smoothing (pixels). If ``None``, uses
        a default of ~L/128 rounded to an odd integer. The window is
        clipped to at least 5 and at most L-1.
    use_mask : bool, default True
        If ``True``, apply ``/Mask`` so A and y match the solver grid.
    seed : int, default 1234
        RNG seed for reproducible spaxel selection.
    show_progress : bool, default True
        If ``True``, show tqdm progress bars.

    Returns
    -------
    dict
        Dictionary with keys:
        - ``vel_bias_kms`` : float
            The global velocity bias (km/s), positive = model needs to be
            redshifted relative to data by this amount.
        - ``shift_px`` : float
            Estimated sub-pixel shift on the masked observed grid.
        - ``median_px`` : float
            Median of per-spax shifts from the sanity subset.
        - ``mad_px`` : float
            Median absolute deviation (MAD) of per-spax shifts.
        - ``n_spax_used`` : int
            Number of spaxels used for the aggregate correlation.
        - ``L_eff`` : int
            Effective masked spectral length.
        - ``dlog_obs_med`` : float
            Median spacing of ``ln λ`` on the masked observed grid.
        - ``n_features`` : int
            Number of feature centers used.
        - ``window_len`` : int
            Window length per feature (``2*half_window + 1``).
        - ``quality`` : str
            ``"ok"`` if finite; otherwise ``"bad"``.

    Raises
    ------
    RuntimeError
        If no solution vector is found or the masked length is zero.

    Examples
    --------
    >>> est = estimate_global_velocity_bias_features(
    ...     "galaxy.h5",
    ...     sidecar="galaxy.h5.fit.123.h5",
    ...     sample_spaxels=160,
    ...     feature_spaxels=80,
    ...     n_features=20,
    ...     half_window=12,
    ...     search_range_px=6,
    ...     show_progress=True,
    ... )
    >>> est["shift_px"]
    1.2
    """

    # tqdm (optional, soft dependency)
    try:
        from tqdm import tqdm as _tqdm
    except Exception:  # pragma: no cover
        def _tqdm(iterable=None, **kwargs):
            return iterable if iterable is not None else range(
                int(kwargs.get("total", 0))
            )

    # ------------------------------- helpers ---------------------------------
    def _read_latest_x(side_f, main_f):
        def _row_or_vec(dset):
            arr = np.asarray(dset[...], dtype=float)
            if arr.ndim == 2 and arr.shape[0] > 0:
                return arr[-1, :].astype(float, copy=False)
            return arr.ravel().astype(float, copy=False)

        if side_f is not None:
            for key in ("/Fit/x_best", "/Fit/x_last", "/Fit/x_epoch_last",
                        "/Fit/x_snapshots", "/Fit/x_hist"):
                if key in side_f:
                    return _row_or_vec(side_f[key])
        for key in ("/Fit/x_latest", "/X_global"):
            if key in main_f:
                return _row_or_vec(main_f[key])
        raise RuntimeError("No solution vector x found in sidecar or main HDF5.")

    def _choose_spaxels(S: int, k: int, rng: np.random.Generator) -> np.ndarray:
        k_eff = int(np.minimum(k, S))
        if k_eff >= S:
            return np.arange(S, dtype=np.int64)
        # quasi-uniform linspace, then add a tiny jitter for diversity
        base = np.linspace(0, S - 1, k_eff, dtype=np.int64)
        if k_eff > 4:
            # jitter within ±1 (clipped), unique to avoid duplicates
            jitter = rng.integers(-1, 2, size=k_eff, dtype=np.int64)
            cand = np.clip(base + jitter, 0, S - 1)
            return np.unique(cand)
        return base

    def _moving_avg_same(x: np.ndarray, w: int) -> np.ndarray:
        # 1D boxcar (odd length) with symmetric pad via reflection
        L = int(x.size)
        half = int((w - 1) // 2)
        # Reflect padding
        pre = x[1:half+1][::-1] if half > 0 else np.empty(0, x.dtype)
        suf = x[L-half-1:L-1][::-1] if half > 0 else np.empty(0, x.dtype)
        z = np.concatenate([pre, x, suf])
        k = np.ones((w,), dtype=x.dtype) / np.float64(w)
        y = np.convolve(z, k, mode="valid")  # length L
        return y

    def _pick_feature_centers(resid_ref: np.ndarray,
                              n_feat: int,
                              min_gap: int) -> np.ndarray:
        # pick most negative residuals with spacing >= min_gap
        order = np.argsort(resid_ref)  # ascending: most negative first
        sel = np.empty(0, dtype=np.int64)
        for j in order:
            if sel.size == 0:
                sel = np.append(sel, j)
                if sel.size >= n_feat:
                    break
                continue
            # enforce separation
            if np.all(np.abs(j - sel) >= min_gap):
                sel = np.append(sel, j)
                if sel.size >= n_feat:
                    break
        # sort centers
        return np.sort(sel)

    def _norm_win(y: np.ndarray, idx_win: np.ndarray) -> np.ndarray:
        # z-score on windowed region only
        ww = y[idx_win]
        mu = float(np.mean(ww))
        sd = float(np.std(ww))
        sd = sd if (np.isfinite(sd) and sd > 0.0) else 1.0
        return (y - mu) / sd

    def _dot_windows(y: np.ndarray, m: np.ndarray,
                     centers: np.ndarray,
                     half_w: int,
                     shift: int) -> float:
        # Sum of dot products over disjoint windows for a given integer shift.
        total = 0.0
        L = int(y.size)
        for c in centers:
            a = int(np.maximum(0, c - half_w))
            b = int(np.minimum(L, c + half_w + 1))  # [a, b)
            # valid overlap under shift (shift applied to model m index)
            a_m = int(a - shift)
            b_m = int(b - shift)
            # clip range for both signals
            a_eff = int(np.maximum(a, 0))
            b_eff = int(np.minimum(b, L))
            a_m = int(np.maximum(a_m, 0))
            b_m = int(np.minimum(b_m, L))
            # adjust to keep lengths equal
            len_ab = int(b_eff - a_eff)
            len_m  = int(b_m - a_m)
            if (len_ab <= 0) or (len_m <= 0):
                continue
            d = int(np.minimum(len_ab, len_m))
            total += float(np.dot(y[a_eff:a_eff+d], m[a_m:a_m+d]))
        return total

    def _best_shift_for_pair(y: np.ndarray, m: np.ndarray,
                             centers: np.ndarray, half_w: int,
                             R: int) -> float:
        # window-normalize both sequences (using all windows together)
        # Build a single index vector for window union to estimate stats.
        L = int(y.size)
        mask = np.zeros(L, dtype=bool)
        for c in centers:
            a = int(np.maximum(0, c - half_w))
            b = int(np.minimum(L, c + half_w + 1))
            mask[a:b] = True
        idx = np.flatnonzero(mask)
        y_n = _norm_win(y, idx)
        m_n = _norm_win(m, idx)

        ks = np.arange(-R, R + 1, dtype=np.int64)
        scores = np.empty(ks.size, dtype=np.float64)
        for i, k in enumerate(ks):
            scores[i] = _dot_windows(y_n, m_n, centers, half_w, int(k))

        # parabolic subpixel around best integer k
        i_best = int(np.argmax(scores))
        k_best = float(ks[i_best])
        if (i_best > 0) and (i_best < (scores.size - 1)):
            y0 = scores[i_best - 1]
            y1 = scores[i_best]
            y2 = scores[i_best + 1]
            denom = (y0 - 2.0 * y1 + y2)
            frac = 0.0 if (np.abs(denom) < 1.0e-30) else 0.5 * (y0 - y2) / denom
            return float(k_best + frac)
        return float(k_best)

    # -------------------------- read dims/mask/x ------------------------------
    with open_h5(h5_path, role="reader", swmr=True) as F:
        L_obs = int(F["/DataCube"].shape[1])

        keep = None
        if bool(use_mask) and ("/Mask" in F):
            msk = np.asarray(F["/Mask"][...], dtype=bool).ravel()
            if (msk.size == L_obs) and np.any(msk):
                keep = np.flatnonzero(msk)

        ObsPix = np.asarray(F["/ObsPix"][...], dtype=np.float64)
        ObsPix_used = ObsPix if keep is None else ObsPix[keep]
        if ObsPix_used.size == 0:
            raise RuntimeError("Masked ObsPix has zero length.")

        dlog_obs_med = float(np.median(np.diff(ObsPix_used)))

        # read x (prefer sidecar)
        if sidecar and plp.Path(sidecar).exists():
            with open_h5(sidecar, role="reader", swmr=True) as G:
                x = _read_latest_x(G, F)
        else:
            x = _read_latest_x(None, F)

        S = int(F["/DataCube"].shape[0])
        C = int(F["/HyperCube/models"].shape[1])
        P = int(F["/HyperCube/models"].shape[2])

    if x.size != (C * P):
        raise RuntimeError(f"x has length {x.size}, expected C*P={C*P}.")

    rng = np.random.default_rng(int(seed))

    # ----------------------------- sampling -----------------------------------
    spx_feat = _choose_spaxels(S, int(feature_spaxels), rng)
    spx_corr = _choose_spaxels(S, int(sample_spaxels),  rng)

    reader = HyperCubeReader(
        h5_path,
        ReaderCfg(dtype_models="float32", apply_mask=bool(use_mask))
    )

    try:
        # One read to get L_eff
        A0, y0 = reader.read_spaxel_plane(int(spx_feat[0]))
        L_eff = int(y0.size)

        # continuum window default (odd)
        if cont_window is None:
            cw = int(np.maximum(5, np.minimum(L_eff - 1, np.round(L_eff / 128.0))))
            if (cw % 2) == 0:
                cw = int(cw + 1)
        else:
            cw_clip = int(np.clip(cont_window, 5, np.maximum(5, L_eff - 1)))
            cw = int(cw_clip + (1 - (cw_clip % 2)))  # make odd

        # ------------------ (1) build reference residual profile ---------------
        accum = np.zeros(L_eff, dtype=np.float64)
        count = 0

        it_feat = spx_feat
        if show_progress:
            it_feat = _tqdm(
                spx_feat,
                desc="[bias] building feature profile",
                mininterval=0.5,
                dynamic_ncols=True,
            )

        for s in it_feat:
            _, y = reader.read_spaxel_plane(int(s))
            # simple continuum via boxcar; residual = y - cont
            cont = _moving_avg_same(y, cw)
            r = y - cont
            # normalize residual per spaxel by its robust scale
            sig = float(np.std(r))
            sig = sig if (np.isfinite(sig) and sig > 0.0) else 1.0
            accum += (r / sig)
            count += 1

        resid_ref = accum / np.maximum(1.0, float(count))

        # pick centers: deepest negative residuals with spacing ≥ half_window
        centers = _pick_feature_centers(
            resid_ref,
            int(n_features),
            int(np.maximum(half_window, 1)),
        )
        # if not enough due to spacing, relax by 1 px spacing until filled
        gap = int(np.maximum(half_window, 1))
        while centers.size < int(n_features) and gap > 1:
            gap = int(gap - 1)
            centers = _pick_feature_centers(resid_ref, int(n_features), gap)

        if centers.size == 0:
            # fallback: use evenly spaced centers
            centers = np.linspace(0, L_eff - 1, int(np.maximum(4, n_features)), dtype=np.int64)

        # ------------------ (2) aggregated integer-shift scores ----------------
        R = int(np.maximum(1, search_range_px))
        ks = np.arange(-R, R + 1, dtype=np.int64)
        scores = np.zeros(ks.size, dtype=np.float64)

        it_corr = spx_corr
        if show_progress:
            it_corr = _tqdm(
                spx_corr,
                desc="[bias] accumulating correlation",
                mininterval=0.5,
                dynamic_ncols=True,
            )

        for s in it_corr:
            A, y = reader.read_spaxel_plane(int(s))
            m = (A.T @ x).astype(np.float64, copy=False)

            # pre-normalize both on union of windows (shared stats)
            mask = np.zeros(L_eff, dtype=bool)
            for c in centers:
                a = int(np.maximum(0, c - half_window))
                b = int(np.minimum(L_eff, c + half_window + 1))
                mask[a:b] = True
            idx = np.flatnonzero(mask)
            y_n = _norm_win(y, idx)
            m_n = _norm_win(m, idx)

            for i, k in enumerate(ks):
                scores[i] += _dot_windows(y_n, m_n, centers, int(half_window), int(k))

        i_best = int(np.argmax(scores))
        k_best = float(ks[i_best])
        # quadratic subpixel refine
        if (i_best > 0) and (i_best < (scores.size - 1)):
            y0 = scores[i_best - 1]
            y1 = scores[i_best]
            y2 = scores[i_best + 1]
            denom = (y0 - 2.0 * y1 + y2)
            frac = 0.0 if (np.abs(denom) < 1.0e-30) else 0.5 * (y0 - y2) / denom
            shift_px = float(k_best + frac)
        else:
            shift_px = float(k_best)

        # ------------------ (3) per-spax sanity distribution -------------------
        k_chk = int(np.minimum(np.maximum(16, centers.size), spx_corr.size))
        s_small = spx_corr[:k_chk]
        per_px = np.empty(k_chk, dtype=np.float64)

        it_chk = enumerate(s_small)
        if show_progress:
            it_chk = _tqdm(
                enumerate(s_small),
                total=k_chk,
                desc="[bias] sanity subset",
                mininterval=0.5,
                dynamic_ncols=True,
            )

        for i, s in it_chk:
            A, y = reader.read_spaxel_plane(int(s))
            m = (A.T @ x).astype(np.float64, copy=False)
            per_px[i] = _best_shift_for_pair(y, m, centers, int(half_window), int(R))

        median_px = float(np.median(per_px))
        mad_px = float(np.median(np.abs(per_px - median_px)))

    finally:
        reader.close()

    # ------------------------ pixel → ln λ → km/s -----------------------------
    dln = dlog_obs_med * float(shift_px)
    vel_bias = float(C_KMS * np.expm1(dln))

    return dict(
        vel_bias_kms=vel_bias,
        shift_px=float(shift_px),
        median_px=median_px,
        mad_px=mad_px,
        n_spax_used=int(spx_corr.size),
        L_eff=int(L_eff),
        dlog_obs_med=dlog_obs_med,
        n_features=int(centers.size),
        window_len=int(2 * half_window + 1),
        quality="ok" if np.isfinite(vel_bias) else "bad",
    )

# ------------------------------------------------------------------------------

@dataclass
class RatioCfg:
    """
    Mixture control for component usage s_c = Σ_p x[c,p].

    Parameters
    ----------
    use : bool
        Enable the mixture update.
    eta : float
        Step size for the multiplicative correction in log space.
    gamma : float
        Per-update clamp on multiplicative change; factors are clipped
        into [1/gamma, gamma] before renormalization.
    prob : float
        Probability of updating a given component in a tile (stochastic
        thinning). 1.0 = update all.
    batch : int
        If >0, cap on the number of components updated per tile.
    minw : float
        Floor for target and empirical mixtures to avoid log(0).
    anchor : str
        'target'  -> drive toward orbit_weights (normalized)
        'x0'      -> drive toward the mixture in the provided x0
        'auto'    -> use 'x0' if non-empty, else 'target'
    tile_every : int
        Apply the update once every this many tiles (1 = every tile).
    epoch_renorm : bool
        If True, recompute the per-epoch target normalization so the
        target is comparable to the current mass.

    Notes
    -----
    The update is mass-preserving and nonnegativity-preserving:
        1) compute ṡ = s / Σ s, t = target (Σ t = 1)
        2) f = exp(-eta * (log(ṡ) - log(t)))  (clipped)
        3) normalize f by ⟨ṡ, f⟩ to keep total mass
        4) x[c,:] ← f[c] * x[c,:]
    """
    use: bool = True
    eta: float = 0.8
    gamma: float = 1.3
    prob: float = 1.0
    batch: int = 0
    minw: float = 1e-6
    anchor: str = "auto"       # {'target','x0','auto'}
    tile_every: int = 1
    epoch_renorm: bool = True

    # strong epoch-end projector
    epoch_project: bool = True  # do a global pass at epoch end
    epoch_eta: float = 1.0      # stronger than tile eta
    epoch_gamma: float = 10.0   # allow larger rebalancing
    epoch_beta: float = 1.0     # mixing: 1.0=full replace, <1 = blend

# ------------------------------------------------------------------------------

def compare_usage_to_orbit_weights(h5_path: str,
                                   sidecar: str | None = None,
                                   x_dset: str | None = None,
                                   normalize: str = "unit_sum",
                                   out_png: str | None = None,
                                   *,
                                   usage_metric: str = "sum",
                                   E_cp: np.ndarray | None = None) -> dict:
    """
    Compare the final component usage (sum over populations) against the
    input orbital weights, both read from the HDF5 store.

    The function prefers solution vectors in a sidecar if present, then
    falls back to the main file. It also prefers orbital weights stored
    in the sidecar, then falls back to the main file.

    Parameters
    ----------
    h5_path : str
        Path to the main HDF5 file.
    sidecar : str or None
        Optional explicit path to a sidecar. If None, the function will
        try to auto-detect one matching '<main>.fit.*.h5' and use the
        newest by mtime.
    x_dset : str or None
        Optional explicit dataset path for X in either the sidecar or
        main file (e.g., '/Fit/x_best'). If None, the search order is:
        sidecar: '/Fit/x_best', '/Fit/x_last', '/Fit/x_epoch_last',
                 last row of '/Fit/x_snapshots', last row of '/Fit/x_hist';
        main:    '/X_global', '/Fit/x_latest'.
    normalize : {'unit_sum', 'match_sum'}
        Normalization applied before comparison. With 'unit_sum', both
        vectors are scaled to sum to 1. With 'match_sum', the usage is
        scaled so its sum matches the orbital weights sum. Use
        'unit_sum' for shape-only comparison.
    out_png : str or None
        If provided, write a small bar+marker figure overlaying usage
        and target weights.
    usage_metric : {'sum', 'energy'}, optional
        How to aggregate usage over populations. With 'sum', usage is
        sum_p X[c,p]. With 'energy', usage is sum_p X[c,p]*E[c,p], where
        E is the global column energy from the HyperCube build.
    E_cp : ndarray or None, optional
        Optional (C,P) array of global column energies. If None and
        usage_metric=='energy', this will be read from the main HDF5 via
        `read_global_column_energy(h5_path)`.

    Returns
    -------
    out : dict
        Dictionary with:
          - 'usage_raw'   : (C,) float64, aggregated usage (per metric)
          - 'weights_raw' : (C,) float64, as read
          - 'usage'       : (C,) float64, normalized
          - 'weights'     : (C,) float64, normalized
          - 'l1'          : float, L1 error (|u-w|).sum()
          - 'linf'        : float, L∞ error |u-w|.max()
          - 'cosine'      : float, cosine similarity
          - 'pearson_r'   : float, Pearson correlation (NaN if C<2)
          - 'argmax_err'  : int, index of max |u-w|
          - 'plot_path'   : str or None
    """
    # ------------------ helpers ------------------
    def _find_latest_sidecar(main_path: str) -> str | None:
        import glob  # ensure available even if caller didn't import at top
        pat = f"{os.fspath(main_path)}.fit.*.h5"
        cand = glob.glob(pat)
        if not cand:
            return None
        cand.sort(key=lambda p: os.path.getmtime(p))
        return cand[-1]

    def _read_C_P(f) -> tuple[int, int]:
        M = f["/HyperCube/models"]
        _, C_, P_, _ = map(int, M.shape)
        return C_, P_

    def _row_or_vec(ds, C: int, P: int) -> np.ndarray:
        arr = np.asarray(ds[...], dtype=np.float64)
        if arr.ndim == 2:
            if arr.shape == (C, P):
                return arr.reshape(C * P).astype(np.float64, copy=False)
            if arr.shape == (P, C):
                return arr.T.reshape(C * P).astype(np.float64, copy=False)
            if arr.shape[1] == C * P:
                return arr[-1, :].astype(np.float64, copy=False)
            if arr.shape == (1, C * P):
                return arr[0, :].astype(np.float64, copy=False)
            if arr.shape == (C * P, 1):
                return arr[:, 0].astype(np.float64, copy=False)
        return arr.ravel().astype(np.float64, copy=False)

    def _read_X(f_side, f_main, C, P) -> np.ndarray:
        # explicit override
        if x_dset is not None:
            src = f_side if (f_side is not None and x_dset in f_side) else f_main
            if src is None or x_dset not in src:
                raise RuntimeError(f"Requested x_dset '{x_dset}' not found.")
            return _row_or_vec(src[x_dset], C, P)

        # sidecar-first
        if f_side is not None:
            for name in ("/Fit/x_best", "/Fit/x_last", "/Fit/x_epoch_last"):
                if name in f_side:
                    return _row_or_vec(f_side[name], C, P)
            for name in ("/Fit/x_snapshots", "/Fit/x_hist"):
                if name in f_side and f_side[name].shape[0] > 0:
                    return _row_or_vec(f_side[name], C, P)

        # main fallbacks
        for name in ("/X_global", "/Fit/x_latest"):
            if name in f_main:
                return _row_or_vec(f_main[name], C, P)

        raise RuntimeError("No solution vector found in sidecar or main file.")

    def _read_weights(f_side, f_main) -> np.ndarray:
        if f_side is not None and "/CompWeights" in f_side:
            return np.asarray(f_side["/CompWeights"][...], dtype=np.float64)
        if "/Fit/orbit_weights" in f_main:
            return np.asarray(f_main["/Fit/orbit_weights"][...], dtype=np.float64)
        if "/CompWeights" in f_main:
            return np.asarray(f_main["/CompWeights"][...], dtype=np.float64)
        raise RuntimeError("No orbital weights found in sidecar or main file.")

    def _safe_norm(v: np.ndarray, mode: str, ref_sum: float | None = None
                   ) -> np.ndarray:
        v = np.asarray(v, dtype=np.float64)
        s = float(np.sum(v))
        eps = 1.0e-30
        if mode == "unit_sum":
            if s <= eps:
                return np.zeros_like(v)
            return v / s
        # match_sum
        tgt = float(ref_sum) if ref_sum is not None else s
        if s <= eps:
            return np.zeros_like(v)
        return v * (tgt / s)

    # ------------------ open files ------------------
    if sidecar is None:
        sidecar = _find_latest_sidecar(h5_path)

    with open_h5(h5_path, role="reader", swmr=True) as F:
        C, P = _read_C_P(F)
        if sidecar is not None and os.path.exists(sidecar):
            with open_h5(sidecar, role="reader", swmr=True) as G:
                x = _read_X(G, F, C, P)
                w = _read_weights(G, F)
        else:
            x = _read_X(None, F, C, P)
            w = _read_weights(None, F)

    # ------------------ assemble & compare ------------------
    x = np.asarray(x, dtype=np.float64).ravel(order="C")
    x = np.nan_to_num(x, copy=False, nan=0.0, posinf=0.0, neginf=0.0)

    if x.size not in (C, C * P):
        raise ValueError(
            f"x length {x.size} is incompatible; expected C={C} or C*P={C*P}."
        )

    # Usage metric
    usage_metric = str(usage_metric).lower()
    if usage_metric not in ("sum", "energy"):
        raise ValueError("usage_metric must be 'sum' or 'energy'.")

    if x.size == C * P:
        Xcp = x.reshape(C, P, order="C")
    else:
        # If only a C-vector is provided, treat it as already aggregated.
        Xcp = None

    if usage_metric == "energy":
        # Prefer passed-in E_cp; otherwise read once from main file.
        if E_cp is None:
            from CubeFit.hypercube_builder import read_global_column_energy
            E_cp = read_global_column_energy(h5_path)  # (C,P) float64
        E = np.asarray(E_cp, np.float64, order="C")
        if E.shape != (C, P):
            raise ValueError(f"E_cp shape {E.shape} != (C,P) {(C,P)}")
        if Xcp is None:
            # x is (C,), nothing to multiply; interpret as already-aggregated.
            usage_raw = np.maximum(x.copy(), 0.0)
        else:
            usage_raw = np.maximum((Xcp * E).sum(axis=1), 0.0)
    else:
        # 'sum' metric
        if Xcp is None:
            usage_raw = np.maximum(x.copy(), 0.0)
        else:
            usage_raw = np.maximum(Xcp.sum(axis=1), 0.0)

    # Accept orbit_weights as (C,) or (C*P,)
    w_vec = np.asarray(w, dtype=np.float64).ravel(order="C")
    if w_vec.size == C:
        weights_raw = np.maximum(w_vec, 0.0)
    elif w_vec.size == C * P:
        weights_raw = np.maximum(
            w_vec.reshape(C, P, order="C").sum(axis=1), 0.0
        )
    else:
        raise ValueError(
            f"weights length {w_vec.size} is incompatible; expected C={C} "
            f"or C*P={C*P}."
        )

    if normalize not in ("unit_sum", "match_sum"):
        raise ValueError("normalize must be 'unit_sum' or 'match_sum'.")

    if normalize == "unit_sum":
        usage = _safe_norm(usage_raw, "unit_sum")
        weights = _safe_norm(weights_raw, "unit_sum")
    else:
        usage = _safe_norm(
            usage_raw, "match_sum", ref_sum=float(np.sum(weights_raw))
        )
        weights = weights_raw.copy()

    diff = usage - weights
    l1 = float(np.sum(np.abs(diff)))
    linf = float(np.max(np.abs(diff))) if diff.size else np.nan
    denom = np.linalg.norm(usage) * np.linalg.norm(weights)
    cosine = float((usage @ weights) / np.maximum(denom, 1.0e-30)) \
        if denom > 0.0 else np.nan
    pearson_r = float(np.corrcoef(usage, weights)[0, 1]) \
        if usage.size >= 2 else np.nan
    argmax_err = int(np.argmax(np.abs(diff))) if diff.size else -1

    plot_path = None
    if out_png is not None:
        fig = plt.figure(figsize=(9, 3.2))
        ax = fig.add_subplot(111)
        idx = np.arange(C, dtype=int)
        ax.bar(idx, usage, width=0.8, alpha=0.85, label="usage (sum over P)",
               color='r')
        ax.plot(idx, weights, marker="o", linestyle="--", label="target w_c",
                color='k')
        mx = np.max(weights) if C > 0 else 1.0
        ax.set_ylim(0.0, float(1.05 * mx))
        ax.set_xlabel("component c")
        ax.set_ylabel("normalized weight")
        ttl = (f"component usage vs target  L1={l1:.3g}  L∞={linf:.3g}  "
               f"cos={cosine:.3f}  r={pearson_r:.3f}")
        ax.set_title(ttl)
        ax.legend(loc="best", fontsize=8)
        fig.savefig(out_png, dpi=140)
        plt.close(fig)
        plot_path = out_png

    out = dict(
        usage_raw=usage_raw,
        weights_raw=weights_raw,
        usage=usage,
        weights=weights,
        l1=l1,
        linf=linf,
        cosine=cosine,
        pearson_r=pearson_r,
        argmax_err=argmax_err,
        plot_path=plot_path,
    )
    print("[usage_vs_weights] L1={:.4g}  L∞={:.4g}  cos={:.4f}  r={:.4f}  "
          "argmax_err={}".format(l1, linf, cosine, pearson_r, argmax_err))
    return out

# ------------------------------------------------------------------------------

def apply_component_softbox(
    x_cp: np.ndarray,
    w_c: np.ndarray,
    *,
    band: float = 0.30,
    step: float = 0.25,
    min_target: float = 1e-10,
) -> None:
    """
    Softly pull per-component usage s_c = sum_p x[c,p] toward a target
    proportional to the prior weights w_c, within a (1±band) tube.
    In-place, O(C·P). Accepts w_c of length C or C*P.

    Parameters
    ----------
    x_cp : (C,P) float
        Current component×population weights (modified in place).
    w_c : (C,) or (C*P,) float
        Component priors. If length C*P, they are summed over P → (C,).
    band : float
        Allowed relative deviation around target (e.g. 0.30 → ±30%).
    step : float
        Relaxation fraction toward the band edge per call (0..1).
    min_target : float
        Floor for targets to avoid division-by-zero.
    """
    x = np.asarray(x_cp, dtype=np.float64, order="C")
    if x.ndim != 2:
        raise ValueError(f"x_cp must be 2-D (C,P); got shape {x.shape}.")
    C, P = x.shape

    w = np.asarray(w_c, dtype=np.float64).ravel(order="C")
    if w.size == C:
        wC = w
    elif w.size == C * P:
        wC = w.reshape(C, P).sum(axis=1)
    else:
        raise ValueError(f"w_c length {w.size} incompatible with C={C}, P={P}.")

    w_sum = np.sum(wC)
    if not np.isfinite(w_sum) or w_sum <= 0.0:
        return  # nothing to do

    # Targets proportional to priors, matched to total mass in x
    total = np.sum(x)
    if not np.isfinite(total) or total <= 0.0:
        return

    w_norm = wC / np.maximum(w_sum, 1.0e-300)         # (C,)
    t = np.maximum(min_target, w_norm * total)        # (C,)
    s = np.sum(x, axis=1)                             # (C,)

    # Tube bounds and which rows violate
    lo = (1.0 - band)
    hi = (1.0 + band)
    too_high = s > (hi * t)
    too_low  = s < (lo * t)
    need = (too_high | too_low)
    if not np.any(need):
        return

    # Scale factors (smooth pull toward tube edges)
    f = np.ones(C, dtype=np.float64)
    s_safe = np.maximum(s, min_target)

    # map s -> hi*t for high violators; s -> lo*t for low violators
    # blend with (1-step) to avoid shocks
    f_hi = (hi * t) / s_safe
    f_lo = (lo * t) / s_safe

    f[too_high] = (1.0 - step) + step * f_hi[too_high]
    f[too_low]  = (1.0 - step) + step * f_lo[too_low]

    # Apply only to rows that need adjustment
    idx = np.flatnonzero(need)
    x[idx, :] *= f[idx, None]
    np.maximum(x[idx, :], 0.0, out=x[idx, :])  # keep nonnegativity

# ------------------------------------------------------------------------------

def apply_component_softbox_energy(
    x_cp: np.ndarray,
    E_cp: np.ndarray,          # read_global_column_energy(h5_path) → (C,P)
    w_c: np.ndarray,           # /CompWeights: len C or C*P
    *,
    band: float = 0.30,        # allowed deviation band ±30%
    step: float = 0.20,        # fraction of the way to the band boundary
    min_target: float = 1e-10,
    row_chunk: int = 8192,
) -> None:
    """
    Soft "box" constraint in energy-weighted usage space. Scales rows of
    x_cp toward the nearest band edge around the target usage, without
    forming large temporaries or using advanced indexing.

    Parameters
    ----------
    x_cp : ndarray, shape (C, P)
        Coefficient matrix to adjust in-place.
    E_cp : ndarray, shape (C, P)
        Global column energy per (c, p).
    w_c : ndarray, shape (C,) or (C*P,)
        Component prior weights; if length C*P, reduced to component level.
    band : float, optional
        Half-width of allowed band, i.e., s ∈ [(1-band)t, (1+band)t].
    step : float, optional
        Fractional move toward the boundary (0→no-op, 1→land exactly).
    min_target : float, optional
        Floor on target weights before normalization.
    row_chunk : int, optional
        Row-chunk size for streamed scaling.

    Returns
    -------
    None
    """
    x = np.asarray(x_cp, np.float64, order="C")
    E = np.asarray(E_cp, np.float64, order="C")
    C, P = x.shape
    if E.shape != (C, P):
        raise ValueError(f"E_cp shape {E.shape} != (C,P) {(C, P)}")

    w = np.asarray(w_c, np.float64).ravel(order="C")
    if w.size == C:
        t = w
    elif w.size == C * P:
        t = w.reshape(C, P).sum(axis=1)
    else:
        raise ValueError(f"w_c length {w.size} incompatible with C={C}, P={P}.")

    # energy-weighted usage per component (proportional to flux)
    # avoids materializing x*E as a (C,P) temp
    s = np.einsum("cp,cp->c", x, E, dtype=np.float64)      # (C,)

    # normalize target & usage
    t = np.maximum(t, float(min_target))
    t /= np.sum(t)
    s_sum = float(np.sum(s))
    if not np.isfinite(s_sum) or s_sum <= 0.0:
        return
    s /= s_sum

    lo = (1.0 - float(band))
    hi = (1.0 + float(band))

    over = s > (hi * t)
    under = s < (lo * t)

    # Build a single per-row scaling vector G (defaults to 1.0)
    G = np.ones(C, dtype=np.float64)

    if np.any(over):
        # exact factor to land on upper edge, then partial move
        f_over = (hi * t[over]) / s[over]
        G[over] = (1.0 - float(step)) + float(step) * f_over

    if np.any(under):
        # exact factor to land on lower edge, then partial move
        f_under = (lo * t[under]) / s[under]
        G[under] = (1.0 - float(step)) + float(step) * f_under

    # Streamed in-place row scaling: x[i,:] *= G[i]
    for i0 in range(0, C, int(row_chunk)):
        i1 = min(C, i0 + int(row_chunk))
        x[i0:i1, :] *= G[i0:i1, None]

    np.maximum(x, 0.0, out=x)  # keep nonnegativity

# ------------------------------------------------------------------------------

def _ensure_cp_flux_ref(h5_path: str,
                        keep_idx: np.ndarray | None,
                        dset: str = "/HyperCube/norm/cp_flux_ref",
                        *,
                        max_samples: int = 256,
                        floor: float = 1e-12) -> np.ndarray:
    """
    Ensure /HyperCube/norm/cp_flux_ref (C,P) exists. If missing, compute
    a robust per-(c,p) flux reference as the **median over spaxels** of
    the λ-sum of the (c,p) slice, using the current spectral mask.

    We sample up to `max_samples` spaxels (evenly spaced) for speed.
    """
    # Fast path: already present
    with open_h5(h5_path, role="reader") as f:
        if dset in f:
            ref = np.asarray(f[dset][...], dtype=np.float64, order="C")
            return ref

    # Build (approximate) median over spaxels of sum_λ A[s,c,p,λ]
    with open_h5(h5_path, role="reader") as f:
        M = f["/HyperCube/models"]  # (S,C,P,L)
        if M.ndim != 4:
            raise RuntimeError(f"Unexpected /HyperCube/models rank {M.ndim}")
        S, C, P, L = map(int, M.shape)

        if keep_idx is None:
            Lk = L
        else:
            keep_idx = np.asarray(keep_idx, dtype=np.int64)
            Lk = int(keep_idx.size)

        # choose evenly spaced spaxels
        Ns = int(min(S, max_samples))
        if Ns <= 0:
            raise RuntimeError("No spaxels available for cp_flux_ref.")
        picks = np.unique(np.linspace(0, S - 1, Ns).astype(int))

        # accumulate per-sample λ-sums → (Ns, C, P)
        ref_samp = np.empty((picks.size, C, P), dtype=np.float64)
        for j, s in enumerate(picks):
            A = np.asarray(M[s, :, :, :], dtype=np.float32, order="C")  # (C,P,L)
            if keep_idx is not None:
                A = A[:, :, keep_idx]                                    # (C,P,Lk)
            # sum over λ (keep_idx) in float64
            ref_samp[j, :, :] = np.sum(A.astype(np.float64, copy=False), axis=2)

        ref = np.median(ref_samp, axis=0)  # (C,P)
        ref = np.where(np.isfinite(ref), ref, 0.0)
        ref = np.maximum(ref, float(floor))

    # persist
    with open_h5(h5_path, role="writer") as f:
        g = f.require_group("/HyperCube/norm")
        if dset in f:
            del f[dset]
        d = f.create_dataset(dset, data=ref.astype(np.float64), chunks=(1, ref.shape[1]))
        g.attrs["basis.mode"] = "cp_flux_ref"  # mark what we applied

    return ref

# ------------------------------------------------------------------------------

def kacz_weighted_tile_whitelight(h5_path, x_flat, s0=0, Sblk=128, use_mask=True):
    """
    Rebuild y_hat for Sblk spaxels using the *weighted* operator (A_w, b_w)
    exactly like the solver, then collapse to white-light with the *same*
    weights/mask. Returns (data_sum, model_sum) arrays of length dS.
    """
    with open_h5(h5_path, role="reader") as f:
        M = f["/HyperCube/models"]       # (S, C, P, L), float32
        D = f["/DataCube"]               # (S, L), data
        wlam = f["/HyperCube/lambda_weights"][...] if "/HyperCube/lambda_weights" in f else None
        mask = f["/Masks/fit_mask"][...] if (use_mask and "/Masks/fit_mask" in f) else None

        S, C, P, L = map(int, M.shape)
        dS = int(min(Sblk, S - s0))
        x = np.asarray(x_flat, np.float64).ravel(order="C")
        assert x.size == C * P, f"x size {x.size} != C*P={C*P}"
        X = x.reshape(C, P)

        # λ selection consistent with the fit
        lam_sel = np.ones(L, dtype=bool)
        if mask is not None:
            lam_sel &= (mask != 0)
        lam_sel = np.where(lam_sel)[0]
        if lam_sel.size == 0:
            raise RuntimeError("Empty lambda selection.")

        # weights used by the solver (sqrt-w per row)
        w = np.ones(lam_sel.size, dtype=np.float64)
        if wlam is not None:
            w = np.sqrt(np.asarray(wlam, np.float64)[lam_sel])

        # build weighted rhs and model on the *same* λ grid
        D_sub = np.asarray(D[s0:s0+dS, lam_sel], np.float64, order="C")
        D_w   = D_sub * w[None, :]

        Y_w = np.zeros_like(D_w)  # (dS, L_sel), weighted model
        for c in range(C):
            A_sc = np.asarray(M[s0:s0+dS, c, :, lam_sel], np.float32, order="C")  # (dS, P, L_sel)
            # tensordot over P, then apply sqrt-w
            Y_w += np.tensordot(X[c, :].astype(np.float64, copy=False),
                                A_sc.astype(np.float64, copy=False),
                                axes=(0, 1)) * w[None, :]

        # white-light collapse with the *same* weights & λ selection
        data_sum  = D_w.sum(axis=1)
        model_sum = Y_w.sum(axis=1)
        return data_sum, model_sum

# Example usage (pick a central tile)
# data_sum, model_sum = kacz_weighted_tile_whitelight(h5_path, x_global, s0=0, Sblk=256)
# print("median(model/data) =", np.median(model_sum / np.maximum(1e-30, data_sum)))

# ------------------------------------------------------------------------------

def _energy_component_scales(
    x_cp: np.ndarray,
    t_vec: np.ndarray,
    *,
    E_cp: np.ndarray | None = None,
    minw: float = 1e-12,
) -> np.ndarray:
    """
    Compute per-component scale factors F[c] so that the energy-weighted
    usage vector s'_c is proportional to the target mixture t_vec, while
    preserving total mass in the chosen metric.

    If E_cp is provided, usage is s_c = sum_p X[c,p] * E[c,p].
    Otherwise usage is s_c = sum_p X[c,p].

    Returns
    -------
    F : ndarray, shape (C,)
        Multiplicative scale factors for each component row.
    """
    X = np.nan_to_num(np.asarray(x_cp, np.float64),
                      nan=0.0, posinf=0.0, neginf=0.0, copy=True)
    if X.ndim != 2:
        raise ValueError("x_cp must be 2-D (C,P).")
    C, P = X.shape

    # --- target fractions t_c (C,)
    t = np.asarray(t_vec, np.float64).ravel(order="C")
    if t.size == C * P:
        t = t.reshape(C, P, order="C").sum(axis=1)
    elif t.size != C:
        raise ValueError(
            f"[project] target len {t.size} not in {{C, C*P}} (C={C}, P={P})."
        )
    t = np.maximum(np.nan_to_num(t, nan=0.0, posinf=0.0, neginf=0.0),
                   float(minw))
    Tsum = float(t.sum() or 1.0)
    t /= Tsum  # unit-sum target fractions

    # --- current usage s_c in the chosen metric
    if E_cp is not None:
        E = np.asarray(E_cp, np.float64, order="C")
        if E.shape != (C, P):
            raise ValueError(f"E_cp shape {E.shape} != (C,P) {(C, P)}")
        s = (X * E).sum(axis=1)
    else:
        s = X.sum(axis=1)

    s = np.maximum(np.nan_to_num(s, nan=0.0, posinf=0.0, neginf=0.0), 0.0)
    S = float(s.sum() or 1.0)

    # sprinkle tiny mass into components that should be nonzero
    need = (s <= 0.0) & (t > 0.0)
    if np.any(need):
        eps = 1.0e-12 * S
        X[need, :] += (eps / float(P))
        if E_cp is not None:
            s = (X * E).sum(axis=1)
        else:
            s = X.sum(axis=1)
        s = np.maximum(np.nan_to_num(s, nan=0.0, posinf=0.0, neginf=0.0),
                       0.0)
        S = float(s.sum() or 1.0)

    # exact scales: s'_c = F_c * s_c = t_c * S  ⇒  F_c = t_c * S / s_c
    s_safe = np.maximum(s, 1.0e-30)
    F = (t * S) / s_safe
    F = np.nan_to_num(F, nan=0.0, posinf=0.0, neginf=0.0)

    return F

# ------------------------------------------------------------------------------

def project_to_component_weights(
    x_cp: np.ndarray,
    t_vec: np.ndarray,
    *,
    E_cp: np.ndarray | None = None,
    minw: float = 1e-12,
    beta: float = 0.2,
) -> None:
    """
    Gentle, mass-preserving projection of the global component mixture of
    x_cp toward t_vec in the chosen metric (plain or energy-weighted).

    This uses a convex blend:
        X <- (1 - beta) * X + beta * (F * X)

    where F are per-component scale factors computed so that, in the
    chosen metric, the *fully* scaled state would match t_vec exactly.
    For beta=1, this reduces to the strict projector.
    """
    if beta <= 0.0:
        return  # no-op

    X = np.asarray(x_cp, np.float64, order="C")
    C, P = X.shape

    F = _energy_component_scales(X, t_vec, E_cp=E_cp, minw=minw)  # (C,)

    # strict projected state
    X_proj = X * F[:, None]

    if beta >= 1.0:
        X_new = X_proj
    else:
        # convex blend between current and projected
        X_new = (1.0 - float(beta)) * X + float(beta) * X_proj

    np.maximum(X_new, 0.0, out=X_new)
    np.nan_to_num(X_new, copy=False, nan=0.0, posinf=0.0, neginf=0.0)

    x_cp[:] = X_new

# ------------------------------------------------------------------------------

def diagnose_zero_column_threshold(
    h5_path: str,
    rel_zero_candidates=None,
    abs_zero: float = 1.0e-24,
) -> None:
    """
    Diagnose the impact of the zero-column freeze threshold on this
    HyperCube.

    This inspects the global column energy array E_global(c,p) and reports
    how many columns would be considered "tiny" for various choices of
    rel_zero, using the same logic as the solver:

        tiny_col = max(abs_zero, rel_zero * median(E_global[E_global>0])),
        freeze_mask = (E_global <= tiny_col).

    Parameters
    ----------
    h5_path : str
        Path to the HDF5 file with /HyperCube and /HyperCube/col_energy.
    rel_zero_candidates : sequence of float, optional
        List of rel_zero values to test. If None, a default grid
        [1e-12, 1e-11, ..., 1e-6] is used.
    abs_zero : float, optional
        Absolute floor for tiny_col, same meaning as in the solver.
    """

    if rel_zero_candidates is None:
        rel_zero_candidates = np.logspace(-20, -2, num=int(19*2),
            dtype=np.float64)

    E_global = read_global_column_energy(h5_path)  # (C, P)
    E = np.asarray(E_global, dtype=np.float64).ravel()

    # Keep only positive, finite energies
    mask_good = np.isfinite(E) & (E > 0.0)
    E = E[mask_good]

    if E.size == 0:
        print(
            "[diagnose_zero_column_threshold] No positive finite column "
            "energies found."
        )
        return

    med_energy = float(np.median(E))
    q05 = float(np.quantile(E, 0.05))
    q25 = float(np.quantile(E, 0.25))
    q75 = float(np.quantile(E, 0.75))
    q95 = float(np.quantile(E, 0.95))

    print("[diagnose_zero_column_threshold] Global column energy stats:")
    print(f"  N_cols   = {E.size}")
    print(f"  min      = {float(E.min()):.3e}")
    print(f"  5%       = {q05:.3e}")
    print(f"  25%      = {q25:.3e}")
    print(f"  median   = {med_energy:.3e}")
    print(f"  75%      = {q75:.3e}")
    print(f"  95%      = {q95:.3e}")
    print(f"  max      = {float(E.max()):.3e}")
    print()

    for r in rel_zero_candidates:
        if not np.isfinite(r) or r <= 0.0:
            continue
        tiny_col = float(max(abs_zero, r * med_energy))
        frozen = int(np.count_nonzero(E <= tiny_col))
        frac = frozen / float(E.size)
        print(
            f"  rel_zero = {r:9.1e} -> "
            f"tiny_col = {tiny_col:.3e}, "
            f"freeze {frozen:6d}/{E.size:6d} "
            f"({100.0 * frac:6.3f} %)"
        )

# ------------------------------------------------------------------------------

def diagnose_tile_col_denom_threshold(
    h5_path: str,
    *,
    n_tiles: int = 8,
    n_comps: int = 8,
    seed: int = 12345,
    rel_zero_grid: np.ndarray | None = None,
    abs_zero: float | None = None,
    mask_dset: str = "/Mask",
    models_dset: str = "/HyperCube/models",
    lamw_dset: str = "/HyperCube/lambda_weights",
) -> dict:
    """
    Diagnose a reasonable ZERO_COL_REL based on tile-local denominators.

    This samples a subset of spatial tiles and components, computes the
    tile-local per-population denominator:

        col_denom[p] = sum_{s,λ} ( (sqrt(w_λ) * A[s,p,λ])^2 )

    and then evaluates freeze fractions for a grid of rel_zero values using:

        tiny_col = max(abs_zero, rel_zero * median(col_denom[col_denom>0]))

    Parameters
    ----------
    h5_path
        Path to the CubeFit HDF5 file.
    n_tiles
        Number of spatial tiles to sample (uniform random).
    n_comps
        Number of components to sample (uniform random).
    seed
        RNG seed for reproducibility.
    rel_zero_grid
        Grid of rel_zero values to test. If None, uses a log grid.
    abs_zero
        Absolute floor. If None, read from env default (1e-24 behavior).
    mask_dset
        Mask dataset path, if present.
    models_dset
        HyperCube models dataset path.
    lamw_dset
        Lambda-weights dataset path (optional). If missing, weights=1.

    Returns
    -------
    out : dict
        Contains sampled denom statistics and freeze fractions.

    Notes
    -----
    This is intended for choosing rel_zero. It does not run the solver.
    """
    if rel_zero_grid is None:
        rel_zero_grid = np.logspace(-15, -2, 25)
    rel_zero_grid = np.asarray(rel_zero_grid, dtype=np.float64)

    if abs_zero is None:
        abs_zero = 1.0e-24

    rng = np.random.default_rng(int(seed))

    with open_h5(h5_path, role="reader") as f:
        M = f[models_dset]
        S, C, P, L = map(int, M.shape)

        # Mask -> keep_idx
        keep_idx = None
        if mask_dset in f:
            mask = np.asarray(f[mask_dset][...], bool).ravel()
            keep_idx = np.flatnonzero(mask)
            Lk = int(keep_idx.size)
        else:
            Lk = L

        # Lambda weights -> sqrt weights on kept grid
        w_lam_sqrt = None
        if lamw_dset in f:
            w_full = np.asarray(f[lamw_dset][...], np.float64).ravel()
            if w_full.size == L:
                w = w_full[keep_idx] if keep_idx is not None else w_full
                w_lam_sqrt = np.sqrt(np.clip(w, 1e-12, None)).astype(np.float64)

        # Tile ranges from chunking (fallback 128)
        s_tile = int(M.chunks[0]) if (M.chunks and M.chunks[0] > 0) else 128
        s_ranges = [(s0, min(S, s0 + s_tile)) for s0 in range(0, S, s_tile)]

        # Sample tiles/components
        tile_idx = rng.choice(len(s_ranges),
                              size=min(n_tiles, len(s_ranges)),
                              replace=False)
        comp_idx = rng.choice(C, size=min(n_comps, C), replace=False)

        denoms = []

        for ti in tile_idx:
            s0, s1 = s_ranges[int(ti)]
            for c in comp_idx:
                A = np.asarray(M[s0:s1, int(c), :, :], np.float32, order="C")
                if keep_idx is not None:
                    A = A[:, :, keep_idx]  # (Sblk, P, Lk)

                if w_lam_sqrt is not None:
                    Aw = A * w_lam_sqrt[None, None, :]
                else:
                    Aw = A

                # col_denom over (s,λ)
                col_denom = np.sum(np.square(Aw, dtype=np.float64), axis=(0, 2))
                denoms.append(col_denom.astype(np.float64, copy=False))

        den = np.concatenate(denoms, axis=0) if denoms else np.zeros((0,))
        den_pos = den[den > 0.0]

        if den_pos.size == 0:
            med = 0.0
            q = {}
        else:
            med = float(np.median(den_pos))
            q = {
                "min": float(np.min(den_pos)),
                "p05": float(np.percentile(den_pos, 5)),
                "p25": float(np.percentile(den_pos, 25)),
                "median": float(np.median(den_pos)),
                "p75": float(np.percentile(den_pos, 75)),
                "p95": float(np.percentile(den_pos, 95)),
                "max": float(np.max(den_pos)),
            }

        rows = []
        for rz in rel_zero_grid:
            tiny_col = float(max(abs_zero, float(rz) * med))
            frz = int(np.count_nonzero(den <= tiny_col))
            tot = int(den.size)
            pct = 0.0 if tot == 0 else 100.0 * frz / tot
            rows.append((float(rz), tiny_col, frz, tot, pct))

    out = {
        "sampled_tiles": int(len(tile_idx)),
        "sampled_components": int(len(comp_idx)),
        "stats_pos": q,
        "rows": rows,
    }

    print("[diagnose_tile_col_denom_threshold] denom stats (positive only):")
    for k, v in q.items():
        print(f"  {k:>7s} = {v:.3e}")
    print("")
    for rz, tiny, frz, tot, pct in rows:
        print(f"  rel_zero={rz:9.2e} -> tiny_col={tiny:9.3e}, "
              f"freeze {frz:6d}/{tot:6d} ({pct:6.3f} %)")

    return out

# ------------------------------------------------------------------------------
</file>

<file path="pipeline_runner.py">
# -*- coding: utf-8 -*-
r"""
    pipeline_runner.py
    Adriano Poci
    University of Oxford
    2025

    Platforms
    ---------
    Unix, Windows

    Synopsis
    --------
    High-level CubeFit pipeline orchestration: runs per-aperture or global
    Kaczmarz NNLS fits, manages Zarr storage, provides reference and diagnostic
    NNLS fits, and supports continuum/velocity expansion and plotting.

    Notes
    -----
    * Uses the robust HDF5 open helper `open_h5(...)` everywhere to avoid
      SWMR/locking issues and double-opens.
    * No API changes: calls from kz_fitSpec remain the same.

    Authors
    -------
    Adriano Poci <adriano.poci@physics.ox.ac.uk>

History
-------
v1.0:   Initial pipeline design for CubeFit. 2025
v1.1:   Added global (full-cube) Kaczmarz and block constraint support. 2025
v1.2:   Supports continuum, velocity-shift, and reference fits. 2025
v1.3:   Full workflow Zarr integration and flexible test sub-selection. 2025
v1.4:   Complete re-write to use HDF5. 7 September 2025
v1.5:   Wrap `solve_global_kaczmarz_cchunk_mp` in `logger.capture_all_output`
            in `PipelineRunner.solve_all_mp_batched`. 4 December 2025
v1.6:   Updated `solve_all_mp_batched` `warm_start` options to match `solve_all`.
            12 December 2025
v1.7:   Read in NNLs `L2` ridge from environment variable in
            `solve_all_mp_batched`. 13 December 2025
v1.8:   Implemented mtime-based decision for sidecar vs main file loading in
            `solve_all*` resume logic. 18 December 2025
"""

from __future__ import annotations
import pathlib as plp
from typing import Optional, Tuple
import json, time, math, os
import numpy as np
from dataclasses import dataclass

from CubeFit.hdf5_manager import H5Manager, H5Dims, open_h5
from CubeFit.hypercube_builder import build_hypercube, read_global_column_energy
from CubeFit.hypercube_reader import HyperCubeReader, ReaderCfg
from CubeFit.kaczmarz_solver import solve_global_kaczmarz, SolverCfg
from CubeFit.kaczmarz_solver_cchunk_mp_nnls import (
    MPConfig, solve_global_kaczmarz_global_step_mp)
    # solve_global_kaczmarz_cchunk_mp)
from CubeFit.live_fit_dashboard import (
    render_aperture_fits_with_x, render_sfh_from_x, alpha_star_stats
)
from CubeFit.nnls_patch import run_patch as _nnls_patch_run,\
    apply_orbit_prior_to_seed
from CubeFit.fit_tracker import FitTracker, NullTracker, TrackerConfig
import CubeFit.cube_utils as cu
from CubeFit.cube_utils import RatioCfg
from CubeFit.logger import get_logger

logger = get_logger()

# ----------------------------------------------------------------------
# Jacobi seeder
# ----------------------------------------------------------------------

def _build_streaming_jacobi_seed(
    h5_path: str,
    Ns: int = 16,
    K: int = 256,
    rng_seed: int = 12345,
    max_bytes: int = 2_000_000_000,
) -> tuple[np.ndarray, dict]:
    """
    Chunk-friendly Jacobi initializer (robust + mask-safe) with:
      • feature-aware (mean-centred in λ) matching for the seed
      • tiny NNLS polish on a working set of columns

    Env knobs (optional):
      CUBEFIT_SEED_REL_FLOOR     : float, default 1e-6
      CUBEFIT_SEED_ABS_FLOOR     : float, default 1e-24
      CUBEFIT_SEED_SIMPLEX       : 0/1,   default 1     (only if norm.mode='data')
      CUBEFIT_RDCC_SLOTS/bytes/W0: HDF5 dataset cache
      CUBEFIT_JACOBI_NNLS_K      : int,   default 128   (working-set cap)
      CUBEFIT_JACOBI_NNLS_PER_C  : int,   default 8     (per-component cap)
      CUBEFIT_JACOBI_MU_ITERS    : int,   default 10    (fallback MU iters)
    """
    rng = np.random.default_rng(rng_seed)

    rel_floor  = float(os.environ.get("CUBEFIT_SEED_REL_FLOOR", "1e-6"))
    abs_floor  = float(os.environ.get("CUBEFIT_SEED_ABS_FLOOR", "1e-24"))
    use_simplex = os.environ.get("CUBEFIT_SEED_SIMPLEX", "1").lower() \
        not in ("0", "false", "no", "off")

    with open_h5(h5_path, role="reader") as f:
        M = f["/HyperCube/models"] # (S,C,P,L) float32
        Y = f["/DataCube"]         # (S,L)     float64
        Mask = f["/Mask"] if "/Mask" in f else None
        nm_attr = f["/HyperCube"].attrs.get("norm.mode", "")
        norm_mode = nm_attr.decode() if isinstance(
            nm_attr, (bytes, bytearray)) else str(nm_attr)

        try:
            M.id.set_chunk_cache(
                int(os.environ.get("CUBEFIT_RDCC_SLOTS", "100000")),
                int(os.environ.get("CUBEFIT_RDCC_BYTES",
                                   str(512 * 1024 * 1024))),
                float(os.environ.get("CUBEFIT_RDCC_W0", "0.75")),
            )
        except Exception:
            pass

        S, C, P, L = map(int, M.shape)
        CP = int(C * P)
        chunks = M.chunks or (S, 1, P, L)
        S_chunk = int(chunks[0])

        bytes_per_spax = int(C * P * L * 4)
        S_cap = int(np.max((1, int(max_bytes // np.max((1, bytes_per_spax))))))

        if S <= S_chunk:
            s0 = 0
        else:
            n_tiles = int(np.max((1, (S + S_chunk - 1) // S_chunk)))
            tile_id = int(rng.integers(0, n_tiles))
            s0 = int(np.min((tile_id * S_chunk, S - 1)))
            s0 = int((s0 // S_chunk) * S_chunk)
        S_eff = int(np.min((Ns, S_chunk, S_cap, S - s0)))
        s1 = int(s0 + S_eff)

        slab32 = np.asarray(M[s0:s1, :, :, :], np.float32, order="C")
        Y_tile = np.asarray(Y[s0:s1, :], np.float64, order="C")

        if Mask is not None:
            keep = np.asarray(Mask[...], dtype=bool).ravel()
            pool = np.flatnonzero(keep) if (keep.size == L and np.any(keep)) \
                   else np.arange(L, dtype=np.int64)
        else:
            pool = np.arange(L, dtype=np.int64)

        mK = int(np.min((K, pool.size)))
        if mK <= 0:
            raise RuntimeError("No wavelengths available for Jacobi seed.")
        ell = rng.choice(pool, size=mK, replace=False)
        ell_sorted = np.sort(ell)

        slab32 = slab32.reshape(S_eff, CP, L)
        A_sub  = slab32[:, :, ell_sorted]
        Y_sub  = Y_tile[:,  ell_sorted]

    Yc = Y_sub - np.mean(Y_sub, axis=1, keepdims=True)
    Ac = A_sub - np.mean(A_sub, axis=2, keepdims=True)

    d = np.einsum("spk,spk->p", Ac.astype(np.float64, copy=False),
                  Ac.astype(np.float64, copy=False),
                  dtype=np.float64, optimize=True)
    b = np.einsum("spk,sk->p",  Ac.astype(np.float64, copy=False),
                  Yc, dtype=np.float64, optimize=True)

    d2d = d.reshape(C, P)
    med_pos = np.zeros((C,), dtype=np.float64)
    for c in range(C):
        dc = d2d[c, :]
        pos = dc[dc > 0.0]
        med_pos[c] = float(np.median(pos)) if pos.size else 0.0
    floor_c = abs_floor + rel_floor * med_pos

    tiny_mask = d2d < floor_c[:, None]
    if np.any(tiny_mask):
        d2d[tiny_mask] = 1.0
        b.reshape(C, P)[tiny_mask] = 0.0
        d = d2d.ravel(order="C")

    eps = 1e-12
    x0 = np.maximum(0.0, b / (d + eps))

    if use_simplex and norm_mode == "data":
        X = x0.reshape(C, P)
        for c in range(C):
            s = float(np.sum(X[c, :]))
            if s > 0.0:
                X[c, :] /= s
        x0 = X.ravel(order="C")

    score = np.abs(b) / (np.sqrt(np.maximum(d, eps)) + eps)

    rows = int(S_eff * mK)
    bytes_per_col = int(rows * 8)
    K_mem = int(np.max((1, int(max_bytes // np.max((1, bytes_per_col))))))

    K_cap_env = int(os.environ.get("CUBEFIT_JACOBI_NNLS_K", "128"))
    per_c = int(np.max((1, int(os.environ.get("CUBEFIT_JACOBI_NNLS_PER_C", "8")))))
    K_cap = int(np.min((K_mem, K_cap_env, CP)))

    if K_cap > 0:
        score_cp = score.reshape(C, P)
        cand = []
        for c in range(C):
            sc = score_cp[c, :]
            k = int(np.min((per_c, np.count_nonzero(sc > 0.0))))
            if k > 0:
                top = np.argpartition(sc, -k)[-k:]
                top = top[np.argsort(sc[top])[::-1]]
                for p in top:
                    cand.append(int(c * P + int(p)))

        if len(cand) < K_cap:
            rem = K_cap - len(cand)
            order = np.argsort(score)[::-1]
            for g in order:
                if g not in cand:
                    cand.append(int(g))
                    if len(cand) >= K_cap:
                        break

        W_idx = np.array(cand[:K_cap], dtype=np.int64)

        B  = np.empty((rows, W_idx.size), dtype=np.float64)
        xW = x0[W_idx].astype(np.float64, copy=True)
        for j, g in enumerate(W_idx):
            B[:, j] = Ac[:, g, :].reshape(rows, order="C")
        y_flat = Yc.reshape(rows, order="C")

        try:
            from scipy.optimize import nnls as _scipy_nnls
            xW_new, _ = _scipy_nnls(B, y_flat)
        except Exception:
            xW_new = xW.copy()
            iters = int(np.max((0, int(os.environ.get("CUBEFIT_JACOBI_MU_ITERS",
                                                  "10")))))
            for _ in range(iters):
                By   = B @ xW_new
                BTy  = B.T @ y_flat
                BTBy = B.T @ By + 1e-12
                xW_new = np.maximum(0.0, xW_new * (BTy / BTBy))

        x0[W_idx] = xW_new

        if use_simplex and norm_mode == "data":
            X = x0.reshape(C, P)
            for c in range(C):
                s = float(np.sum(X[c, :]))
                if s > 0.0:
                    X[c, :] /= s
            x0 = X.ravel(order="C")

    stats = {
        "Ns_used": int(S_eff),
        "K_used": int(np.min((K, A_sub.shape[2]))),
        "s0": int(s0),
        "s1": int(s1),
        "S_chunk": int(S_chunk),
        "frozen_cols": int(np.sum(tiny_mask)) if 'tiny_mask' in locals() else 0,
    }
    return x0.astype(np.float64, copy=False), stats

# ----------------------------------------------------------------------
# Pipeline
# ----------------------------------------------------------------------

class PipelineRunner:
    """
    Orchestrates HyperCube build & global Kaczmarz fitting from an HDF5
    file.

    You can construct this class **before or after** building the
    HyperCube:
      - If /HyperCube/models is missing, we still read dimensions from
        LOSVD, DataCube, and Templates and you can call
        `build_hypercube(...)`.
      - If models exist, you can go straight to `solve_all(...)`.

    Parameters
    ----------
    h5_path : str | pathlib.Path
        Path to the HDF5 file that holds:
          * /LOSVD            (nSpat, nVel, nComp)            [required]
          * /DataCube         (nSpat, nLSpec)                 [required]
          * /Templates or /TemplatesFFT(_R) (nPop, N/TSpec)   [required]
            to build
          * /HyperCube/models (nSpat, nComp, nPop, nLSpec)    [required]
            to solve
    *_, **__ :
        Extra positional/keyword args are accepted and ignored for
        backward-compatibility with older call sites.
    """
    def __init__(self, h5_path: str | plp.Path):
        self.h5_path = str(h5_path)
        with open_h5(h5_path, "reader") as f:
            dims = {}
            if "/".encode() == b"/":
                pass
            if "dims_json" in f["/"].attrs:
                val = f["/"].attrs["dims_json"]
                if isinstance(val, bytes):
                    val = val.decode("utf-8")
                try:
                    dims = {k: int(v) for k, v in json.loads(val).items()}
                except Exception:
                    dims = {}
            if not dims:
                for k in ("nSpat", "nLSpec", "nTSpec", "nVel", "nComp",
                          "nPop"):
                    key = f"dims.{k}"
                    if key in f["/"].attrs:
                        dims[k] = int(f["/"].attrs[key])

            self.nSpat = int(dims.get("nSpat", f["/DataCube"].shape[0]))
            self.nLSpec = int(dims.get("nLSpec", f["/DataCube"].shape[1]))
            self.nComp  = int(dims.get("nComp",  f["/LOSVD"].shape[2]))
            self.nPop   = int(dims.get("nPop",   f["/Templates"].shape[0]))
            self.nVel   = int(dims.get("nVel",   f["/LOSVD"].shape[1]))
            self.nTSpec = int(dims.get("nTSpec",
                                 f["/Templates"].shape[1])) if "/Templates" \
                                 in f else None
            self.has_mask = ("/Mask" in f)
            self.has_models = ("/HyperCube/models" in f)
            self.complete = bool(f["/HyperCube"].attrs.get("complete",
                                                           False)) \
                            if "/HyperCube" in f else False

        logger.log(
            "[Pipeline] Initialized from HDF5: "
            f"S={self.nSpat}, C={self.nComp}, P={self.nPop}, L={self.nLSpec}, "
            f"V={self.nVel}, T={self.nTSpec if self.nTSpec is not None else 'NA'}; "
            f"mask={'yes' if self.has_mask else 'no'}; "
            f"models={'yes' if self.has_models else 'no'}; "
            f"complete={self.complete}"
        )

        self.zarr_path = None
        self.zarr_store = None
        self.models_path = self.h5_path

    def build_inputs(self) -> None:
        H5Manager(self.h5_path).init_base(H5Dims(
            nSpat=self.nSpat, nLSpec=self.nLSpec, nTSpec=int(self.nTSpec or 0),
            nVel=self.nVel, nComp=self.nComp, nPop=self.nPop
        ))

    def build_hypercube(self, *, S=16, C=1, P=256, galaxy=None, check="auto",
                        extra_manifest=None) -> None:
        nS, nC, nP = 128, 1, 360
        build_hypercube(
            self.h5_path, S_chunk=nS, C_chunk=nC, P_chunk=nP,
        )

        with open_h5(self.h5_path, "reader") as f:
            self.has_models = ("/HyperCube/models" in f)
            self.complete = bool(f["/HyperCube"].attrs.get("complete", False))

    @staticmethod
    def _read_latest_from_sidecar(sidecar_path: str, N_expected: int):
        """
        Read the most recent solution vector from a FitTracker sidecar file.

        For *resume semantics* we prefer the latest checkpoint, not the
        best-so-far checkpoint. So we try, in order:

        1) /Fit/x_last
        2) /Fit/x_epoch_last
        3) /Fit/x_snapshots[-1]
        4) /Fit/x_best
        5) /Fit/x_hist[-1] (legacy)

        Parameters
        ----------
        sidecar_path : str
            Path to the sidecar HDF5 file: <main>.fit.<pid>.<ts>.h5
        N_expected : int
            Expected flattened size (C*P).

        Returns
        -------
        x : ndarray[float64] or None
            Flattened solution vector of length N_expected, if found.
        src : str or None
            Dataset label used to load x (for logging).
        """
        if (sidecar_path is None) or (not os.path.exists(sidecar_path)):
            return None, None

        def _read_flat(g, name: str):
            if name not in g:
                return None
            ds = g[name]
            try:
                if ds.ndim == 2 and ds.shape[0] > 0:
                    v = np.asarray(ds[-1, :], np.float64, order="C")
                else:
                    v = np.asarray(ds[...], np.float64, order="C")
            except Exception:
                return None

            v = np.asarray(v, np.float64).ravel(order="C")
            if v.size != int(N_expected):
                return None
            return v

        with open_h5(sidecar_path, role="reader", swmr=True) as g:
            # Prefer latest progress for resume.
            v = _read_flat(g, "/Fit/x_last")
            if v is not None:
                return v, "/Fit/x_last"

            v = _read_flat(g, "/Fit/x_epoch_last")
            if v is not None:
                return v, "/Fit/x_epoch_last"

            if "/Fit/x_snapshots" in g and g["/Fit/x_snapshots"].shape[0] > 0:
                ds = g["/Fit/x_snapshots"]
                try:
                    v = np.asarray(ds[-1, :], np.float64, order="C")
                    v = v.ravel(order="C")
                    if v.size == int(N_expected):
                        return v, "/Fit/x_snapshots[-1]"
                except Exception:
                    pass

            # Fallback to best-so-far.
            v = _read_flat(g, "/Fit/x_best")
            if v is not None:
                return v, "/Fit/x_best"

            # Legacy fallback.
            if "/Fit/x_hist" in g and g["/Fit/x_hist"].shape[0] > 0:
                ds = g["/Fit/x_hist"]
                try:
                    v = np.asarray(ds[-1, :], np.float64, order="C")
                    v = v.ravel(order="C")
                    if v.size == int(N_expected):
                        return v, "/Fit/x_hist[-1]"
                except Exception:
                    pass

        return None, None

    @staticmethod
    def _read_latest_from_main(h5_path: str, N_expected: int):
        """
        Read the best available solution vector from the *main* HDF5.

        Priority is "most resume-correct" first:
        1) /Fit/x_last
        2) /Fit/x_epoch_last
        3) /X_global
        4) /Fit/x_best
        5) legacy fallbacks

        Returns
        -------
        x : ndarray[float64] or None
            Flattened solution vector (length N_expected), if found.
        src : str or None
            Dataset label used to load x (for logging).
        """
        if (h5_path is None) or (not os.path.exists(h5_path)):
            return None, None

        def _read_flat(f, name: str):
            if name not in f:
                return None
            ds = f[name]
            try:
                if ds.ndim == 2 and ds.shape[0] > 0:
                    v = np.asarray(ds[-1, :], np.float64, order="C")
                else:
                    v = np.asarray(ds[...], np.float64, order="C")
            except Exception:
                return None

            v = np.asarray(v, np.float64).ravel(order="C")
            if v.size != int(N_expected):
                return None
            if not np.all(np.isfinite(v)):
                v = np.nan_to_num(v, nan=0.0, posinf=0.0, neginf=0.0, copy=False)
            return v

        with open_h5(h5_path, role="reader", swmr=True) as f:
            # Resume-correct (if present)
            v = _read_flat(f, "/Fit/x_last")
            if v is not None:
                return v, "/Fit/x_last"

            v = _read_flat(f, "/Fit/x_epoch_last")
            if v is not None:
                return v, "/Fit/x_epoch_last"

            # Canonical committed solution
            v = _read_flat(f, "/X_global")
            if v is not None:
                return v, "/X_global"

            # Best-so-far fallback (if you keep it in main)
            v = _read_flat(f, "/Fit/x_best")
            if v is not None:
                return v, "/Fit/x_best"

            # Legacy candidates (only if they exist in your file)
            for name in ("/X_best", "/X_last", "/Fit/x_hist"):
                v = _read_flat(f, name)
                if v is not None:
                    return v, name

        return None, None

    def _read_seed_from_h5(self,
                        h5_path: str,
                        N_expected: int,
                        dset: str = "/Seeds/x0_nnls_patch",
                        project_nonneg: bool = True) \
                        -> tuple[np.ndarray | None, str | None]:
        """
        Read a seed solution from the main HDF5 file.
        Accepts either flat (N_expected,) or 2-D (C,P) and flattens
        C-order. If the size mismatches, trims or zero-pads with a
        warning.

        Returns
        -------
        (x0, src_label) or (None, None)
        """
        try:
            with open_h5(h5_path, role="reader") as f:
                if dset not in f:
                    return None, None
                arr = np.asarray(f[dset][...], dtype=np.float64, order="C")
                if arr.ndim == 2:
                    arr = arr.reshape(-1, order="C")
                x0 = arr.ravel(order="C")

                if x0.size != N_expected:
                    import warnings
                    warnings.warn(
                        f"[Pipeline] Seed at {dset} has length {x0.size} "
                        f"!= expected {N_expected}; "
                        f"{'trimming' if x0.size > N_expected else 'zero-padding'} "
                        f"to match.",
                        RuntimeWarning
                    )
                    if x0.size > N_expected:
                        x0 = x0[:N_expected].copy()
                    else:
                        tmp = np.zeros(N_expected, dtype=np.float64)
                        tmp[:x0.size] = x0
                        x0 = tmp

                if project_nonneg:
                    np.maximum(x0, 0.0, out=x0)

                return x0, f"{dset} (main)"
        except Exception:
            return None, None

    # ---------------------------- Solve (single-process) ----------------

    def solve_all(
        self,
        *,
        epochs: int = 1,
        pixels_per_aperture: int = 256,
        lr: float = 0.25,
        project_nonneg: bool = True,
        row_order: str = "random",
        # Reader knobs
        reader_s_tile: int | None = None,
        reader_c_tile: int | None = None,
        reader_p_tile: int | None = None,
        reader_dtype_models: str = "float32",
        reader_apply_mask: bool = True,
        # BLAS / compute
        blas_threads: int | None = None,
        block_rows: int | None = None,
        block_norm: str | None = None,
        # Priors / ratio
        orbit_weights: np.ndarray | None = None,
        ratio_use: bool | None = None,
        ratio_anchor: str | int | None = None,
        ratio_eta: float | None = None,
        ratio_prob: float | None = None,
        ratio_batch: int | None = None,
        ratio_min_weight: float | None = None,
        # Warm start
        x0: np.ndarray | None = None,
        warm_start: str = "nnls",  # "resume"|"jacobi"|"nnls"|
                                     # "zeros"|"seed"|"none"
        seed_cfg: dict | None = None,     # for Jacobi / nnls_patch seeding
        # Tracking
        tracker_mode: str = "sidecar",    # "sidecar" | "off"
        progress_interval_sec: float = 300.0,
        verbose: bool = True,
        seed: int | None = None,
    ):
        """
        Run the global Kaczmarz fit with a unified tracker interface.

        Requires FitTracker to expose:
        - set_orbit_weights(w_c)
        - maybe_save(x, epoch, now)
        - on_batch(rmse)
        - finalize(x)

        Returns
        -------
        x_global : (C*P,) float64
        stats    : dict
        """
        t0 = time.perf_counter()

        # ---------------- Warm-start ----------------
        N_expected = self.nComp * self.nPop

        if x0 is not None:
            x0_effective = np.asarray(x0, dtype=np.float64, order="C")

        elif warm_start == "seed":
            path = os.environ.get("CUBEFIT_SEED_PATH", "/Seeds/x0_nnls_patch")
            x_seed, src_seed = self._read_seed_from_h5(self.h5_path,
                N_expected, dset=path)
            if x_seed is not None:
                x0_effective = x_seed
                if verbose:
                    logger.log(f"[Pipeline] Warm-start from seed {src_seed} "
                               f"(n={x0_effective.size}).")
            else:
                x0_effective = None
                if verbose:
                    logger.log(f"[Pipeline] No seed found at {path}; "
                               f"continuing without warm-start.")

        elif warm_start == "resume":
            sidecar = cu._find_latest_sidecar(self.h5_path)

            # Always define these (avoids UnboundLocalError patterns).
            x_side, src_side = (None, None)
            x_main, src_main = (None, None)
            x_seed, src_seed = (None, None)

            # Candidate 1: newest sidecar (by filename/mtime), but may not exist.
            if sidecar is not None and os.path.exists(sidecar):
                x_side, src_side = self._read_latest_from_sidecar(
                    sidecar, N_expected
                )

            # Candidate 2: main file (committed solution).
            x_main, src_main = self._read_latest_from_main(
                self.h5_path, N_expected
            )

            def _safe_mtime(path: str | None) -> float:
                if not path:
                    return -np.inf
                try:
                    return float(os.path.getmtime(path))
                except Exception:
                    return -np.inf

            def _try_epoch(path: str | None, dset: str | None) -> float | None:
                if (path is None) or (dset is None):
                    return None
                try:
                    with open_h5(path, role="reader", swmr=True) as f:
                        if dset in f:
                            e = f[dset].attrs.get("epoch", None)
                            if e is None:
                                return None
                            e = float(e)
                            return e if np.isfinite(e) else None
                except Exception:
                    return None
                return None

            # Decide: newest progress (prefer higher epoch if both have it;
            # otherwise prefer newer file mtime).
            choose_side = False
            if x_side is not None and x_main is None:
                choose_side = True
            elif x_side is None and x_main is not None:
                choose_side = False
            elif x_side is not None and x_main is not None:
                e_side = _try_epoch(sidecar, src_side)
                e_main = _try_epoch(self.h5_path, src_main)

                if (e_side is not None) and (e_main is not None) and (e_side != e_main):
                    choose_side = (e_side > e_main)
                else:
                    choose_side = (_safe_mtime(sidecar) > _safe_mtime(self.h5_path))

            x0_effective, src_label, src_file = (
                (x_side, src_side, sidecar) if choose_side
                else (x_main, src_main, self.h5_path)
            )

            # Fallback: seed (optional but robust).
            seed_used = False
            if x0_effective is None:
                seed_path = os.environ.get("CUBEFIT_SEED_PATH", "/Seeds/x0_nnls_patch")
                x_seed, src_seed = self._read_seed_from_h5(
                    self.h5_path, N_expected, dset=seed_path
                )
                if x_seed is not None:
                    x0_effective = x_seed
                    src_label = src_seed
                    src_file = self.h5_path
                    choose_side = False
                    seed_used = True
                    if verbose:
                        logger.log(
                            "[Pipeline] Warm-start fallback from seed "
                            f"{src_seed} (n={x0_effective.size})."
                        )

            if x0_effective is not None and verbose and (not seed_used):
                t_side = _safe_mtime(sidecar) if sidecar else -np.inf
                t_main = _safe_mtime(self.h5_path)
                logger.log(
                    f"[Pipeline] Warm-start from {src_label} "
                    f"({'sidecar' if choose_side else 'main'}: {src_file}) "
                    f"(n={x0_effective.size}); "
                    f"mtime(sidecar)={t_side:.0f}, mtime(main)={t_main:.0f}."
                )

        elif warm_start == "nnls":
            if verbose:
                logger.log("[Pipeline] Warm-start mode: nnls_patch seed (exact semantics)")

            # Mirror nnls_patch defaults: mask+lambda on, nnls solver, normalized columns, zero ridge
            res = _nnls_patch_run(
                h5_path=self.h5_path,
                s_sel=None,# like nnls_patch default: first min(32, S) spaxels
                k_per_comp=24, # same default as CLI
                pick_mode="energy",
                solver="nnls",
                ridge=0.0,
                use_mask=True,
                use_lambda=True,
                lam_dset="/HyperCube/lambda_weights",
                out_dir=plp.Path(self.h5_path).parent/'figures',
                write_seed=True,
                seed_path="/Seeds/x0_nnls_patch",
                normalize_columns=True,
                orbit_weights=orbit_weights,
            )
            Xcp = np.asarray(res["x_CP"], np.float64, order="C")

            x0_effective = Xcp.ravel(order="C")
            if verbose:
                meta = res.get("meta", {})
                logger.log(f"[Pipeline] nnls_patch seed: rows={meta.get('rows')}, cols={meta.get('cols')}, "
                        f"mask={meta.get('mask_used')}, lambda={meta.get('lambda_used')}, "
                        f"solver={meta.get('solver')}, normcols={True}")

        elif warm_start == "jacobi":
            if verbose:
                logger.log("[Pipeline] Warm-start mode: fresh Jacobi")
            x0_effective, jacobi_stats = _build_streaming_jacobi_seed(
                self.h5_path, **(seed_cfg or {})
            )

        elif warm_start == "zeros":
            x0_effective = np.zeros(N_expected, np.float64)

        else:
            x0_effective = None

        diag_on = os.environ.get("CUBEFIT_JACOBI_DIAG", "1").lower() \
            not in ("0", "false", "no", "off")
        if warm_start == "jacobi" and x0_effective is not None and diag_on:
            try:
                s0 = int(jacobi_stats.get("s0", 0))
                s1 = int(jacobi_stats.get(
                    "s1", s0 + int(np.max([1, jacobi_stats.get("Ns_used", 4)]))
                ))
                Ns_used = int(jacobi_stats.get("Ns_used", 4))

                astats = alpha_star_stats(self.h5_path, x0_effective,
                    n_spax=int(np.min([6, Ns_used])), tile=(s0, s1))
                with open_h5(self.h5_path, role="writer") as fwr:
                    gj = fwr.require_group("/Fit/JacobiDiag")
                    for k in ("median", "min", "max", "n"):
                        gj.attrs[f"alpha_star.{k}"] = float(astats[k])

                with open_h5(self.h5_path, role="reader",
                             swmr=True) as fro:
                    S = int(fro["/DataCube"].shape[0])
                n_plot = int(np.min([6, S]))
                if n_plot > 0:
                    sample = np.linspace(0, S - 1, n_plot,
                                         dtype=int).tolist()

                    base = plp.Path(self.h5_path).parent/'figures'
                    out_fits = base/'jacobi_fits.png'
                    render_aperture_fits_with_x(self.h5_path, x0_effective,
                        out_fits, apertures=sample, show_residual=True,
                        title=f"Jacobi seed overlays (spaxels={sample})")

                    out_sfh = base/'jacobi_sfh.png'
                    render_sfh_from_x(self.h5_path, x0_effective, out_sfh)

            except Exception as e:
                logger.log("[JacobiDiag] WARNING: could not run Jacobi "
                           "diagnostics:", e)


        # ---------------- Reader ----------------
        reader_cfg = ReaderCfg(
            s_tile=reader_s_tile,
            c_tile=reader_c_tile,
            p_tile=reader_p_tile,
            dtype_models=reader_dtype_models,
            apply_mask=reader_apply_mask,
        )
        reader = HyperCubeReader(self.h5_path, cfg=reader_cfg)

        # ---------------- Tracker ----------------
        tracker = NullTracker()
        if tracker_mode != "off":
            tracker_cfg = TrackerConfig()
            tracker = FitTracker(self.h5_path, cfg=tracker_cfg)

        # Push priors into tracker if provided
        if orbit_weights is not None:
            w_c = np.asarray(orbit_weights, dtype=np.float64).ravel()
            if w_c.size not in (reader.nComp, reader.nComp * reader.nPop):
                raise ValueError(
                    "orbit_weights len must be C or C*P; got "
                    f"{w_c.size} for C={reader.nComp}, P={reader.nPop}"
                )
            if w_c.size == reader.nComp * reader.nPop:
                w_c = w_c.reshape(reader.nComp, reader.nPop).sum(axis=1)
            tracker.set_orbit_weights(w_c)

        if ratio_anchor is None:
            ratio_anchor = "auto"

        cfg = SolverCfg(
            epochs=epochs,
            pixels_per_aperture=pixels_per_aperture,
            lr=lr,
            project_nonneg=project_nonneg,
            row_order=row_order,
            blas_threads=blas_threads,
            block_rows=block_rows,
            block_norm=block_norm,
            ratio_use=ratio_use,
            ratio_anchor=ratio_anchor,
            ratio_eta=ratio_eta,
            ratio_prob=ratio_prob,
            ratio_batch=ratio_batch,
            ratio_min_weight=ratio_min_weight,
            verbose=verbose,
            seed=seed,
        )

        def _on_batch_rmse(rmse: float):
            try:
                tracker.on_batch_rmse(rmse, block=False)
            except Exception:
                pass

        def _on_progress(epoch, stats_epoch):
            try:
                if tracker is not None and hasattr(tracker, "on_progress"):
                    tracker.on_progress(epoch=epoch, stats=stats_epoch)
            except Exception:
                pass

        def _on_epoch_end(epoch: int, stats: dict):
            try:
                tracker.on_epoch_end(epoch, stats)
            except Exception:
                pass

        try:
            x, stats = solve_global_kaczmarz(
                reader,
                cfg=cfg,
                orbit_weights=orbit_weights,
                x0=x0_effective,
                on_epoch_end=_on_epoch_end,
                on_progress=_on_progress,
                progress_interval_sec=progress_interval_sec,
                on_batch_rmse=_on_batch_rmse,
            )
        finally:
            reader.close()

        with open_h5(self.h5_path, role="writer") as f_wr:
            if "/X_global" in f_wr:
                del f_wr["/X_global"]
            f_wr.create_dataset("/X_global", data=x, dtype=np.float64)

        tracker.maybe_save(x, stats)
        try:
            tracker.close()
        except Exception:
            pass

        stats = dict(stats or {})
        stats["elapsed_total_sec"] = time.perf_counter() - t0
        return x, stats

    # ------------------------- Solve (multi-process) -------------------

    def solve_all_mp_batched(
        self,
        epochs=1,
        lr=0.25,
        project_nonneg=True,
        reader_s_tile=128,
        reader_c_tile=1,
        reader_p_tile=360,
        reader_dtype_models="float32",
        reader_apply_mask=True,
        processes=2,
        blas_threads=12,
        orbit_weights=None,
        x0=None,
        warm_start="nnls",  # default to the new seed
        seed_cfg=None,
        tracker_mode="on",
        verbose=True,
        ratio_cfg: RatioCfg | None = None,
    ):

        # --------------- Warm-start (same policy as SP path) -----------
        N_expected = int(self.nComp * self.nPop)

        if x0 is not None:
            x0_effective = np.asarray(x0, dtype=np.float64, order="C")

        elif warm_start == "seed":
            path = os.environ.get("CUBEFIT_SEED_PATH", "/Seeds/x0_nnls_patch")
            x_seed, src_seed = self._read_seed_from_h5(self.h5_path,
                N_expected, dset=path)
            if x_seed is not None:
                x0_effective = x_seed
                if verbose:
                    logger.log(f"[Pipeline] Warm-start from seed {src_seed} "
                               f"(n={x0_effective.size}).")
            else:
                x0_effective = None
                if verbose:
                    logger.log(f"[Pipeline] No seed found at {path}; "
                               f"continuing without warm-start.")

        elif warm_start == "resume":
            sidecar = cu._find_latest_sidecar(self.h5_path)

            # Always define these (avoids UnboundLocalError patterns).
            x_side, src_side = (None, None)
            x_main, src_main = (None, None)
            x_seed, src_seed = (None, None)

            # Candidate 1: newest sidecar (by filename/mtime), but may not exist.
            if sidecar is not None and os.path.exists(sidecar):
                x_side, src_side = self._read_latest_from_sidecar(
                    sidecar, N_expected
                )

            # Candidate 2: main file (committed solution).
            x_main, src_main = self._read_latest_from_main(
                self.h5_path, N_expected
            )

            def _safe_mtime(path: str | None) -> float:
                if not path:
                    return -np.inf
                try:
                    return float(os.path.getmtime(path))
                except Exception:
                    return -np.inf

            def _try_epoch(path: str | None, dset: str | None) -> float | None:
                if (path is None) or (dset is None):
                    return None
                try:
                    with open_h5(path, role="reader", swmr=True) as f:
                        if dset in f:
                            e = f[dset].attrs.get("epoch", None)
                            if e is None:
                                return None
                            e = float(e)
                            return e if np.isfinite(e) else None
                except Exception:
                    return None
                return None

            # Decide: newest progress (prefer higher epoch if both have it;
            # otherwise prefer newer file mtime).
            choose_side = False
            if x_side is not None and x_main is None:
                choose_side = True
            elif x_side is None and x_main is not None:
                choose_side = False
            elif x_side is not None and x_main is not None:
                e_side = _try_epoch(sidecar, src_side)
                e_main = _try_epoch(self.h5_path, src_main)

                if (e_side is not None) and (e_main is not None) and (e_side != e_main):
                    choose_side = (e_side > e_main)
                else:
                    choose_side = (_safe_mtime(sidecar) > _safe_mtime(self.h5_path))

            x0_effective, src_label, src_file = (
                (x_side, src_side, sidecar) if choose_side
                else (x_main, src_main, self.h5_path)
            )

            # Fallback: seed (optional but robust).
            seed_used = False
            if x0_effective is None:
                seed_path = os.environ.get("CUBEFIT_SEED_PATH", "/Seeds/x0_nnls_patch")
                x_seed, src_seed = self._read_seed_from_h5(
                    self.h5_path, N_expected, dset=seed_path
                )
                if x_seed is not None:
                    x0_effective = x_seed
                    src_label = src_seed
                    src_file = self.h5_path
                    choose_side = False
                    seed_used = True
                    if verbose:
                        logger.log(
                            "[Pipeline] Warm-start fallback from seed "
                            f"{src_seed} (n={x0_effective.size})."
                        )

            if x0_effective is not None and verbose and (not seed_used):
                t_side = _safe_mtime(sidecar) if sidecar else -np.inf
                t_main = _safe_mtime(self.h5_path)
                logger.log(
                    f"[Pipeline] Warm-start from {src_label} "
                    f"({'sidecar' if choose_side else 'main'}: {src_file}) "
                    f"(n={x0_effective.size}); "
                    f"mtime(sidecar)={t_side:.0f}, mtime(main)={t_main:.0f}."
                )

        elif warm_start == "nnls":
            if verbose:
                logger.log("[Pipeline] Warm-start mode: nnls_patch seed (exact semantics)")

            with logger.capture_all_output():
                nnls_l2 = float(os.environ.get("CUBEFIT_NNLS_L2", "0.0"))
                if not np.isfinite(nnls_l2) or nnls_l2 < 0.0:
                    nnls_l2 = 0.0
                res = _nnls_patch_run(
                    h5_path=self.h5_path,
                    s_sel=None, # first min(32, S) spaxels
                    k_per_comp=24,
                    pick_mode="energy",
                    solver="nnls",
                    ridge=float(nnls_l2),
                    use_mask=True,
                    use_lambda=True,
                    lam_dset="/HyperCube/lambda_weights",
                    out_dir=plp.Path(self.h5_path).parent/'figures',
                    write_seed=bool((seed_cfg or {}).get("write_seed", True)),
                    seed_path="/Seeds/x0_nnls_patch",
                    normalize_columns=True,
                    orbit_weights=orbit_weights,
                )
            Xcp = np.asarray(res["x_CP"], np.float64, order="C")

            x0_effective = Xcp.ravel(order="C")
            if verbose:
                meta = res.get("meta", {})
                logger.log(f"[Pipeline] nnls_patch seed: rows={meta.get('rows')}, cols={meta.get('cols')}, "
                        f"mask={meta.get('mask_used')}, lambda={meta.get('lambda_used')}, "
                        f"solver={meta.get('solver')}, normcols={True}")

        elif warm_start == "jacobi":
            if verbose:
                logger.log("[Pipeline] Warm-start mode: fresh Jacobi")
            x0_effective, jacobi_stats = _build_streaming_jacobi_seed(
                self.h5_path, **(seed_cfg or {})
            )

        elif warm_start == "zeros":
            x0_effective = np.zeros(N_expected, np.float64)

        else:
            x0_effective = None

        diag_on = os.environ.get("CUBEFIT_JACOBI_DIAG", "1").lower() \
            not in ("0", "false", "no", "off")
        if warm_start == "jacobi" and x0_effective is not None and diag_on:
            try:
                s0 = int(jacobi_stats.get("s0", 0))
                s1 = int(jacobi_stats.get(
                    "s1", s0 + int(np.max([1, jacobi_stats.get("Ns_used", 4)]))
                ))
                Ns_used = int(jacobi_stats.get("Ns_used", 4))

                astats = alpha_star_stats(self.h5_path, x0_effective,
                    n_spax=int(np.min([6, Ns_used])), tile=(s0, s1))
                with open_h5(self.h5_path, role="writer") as fwr:
                    gj = fwr.require_group("/Fit/JacobiDiag")
                    for k in ("median", "min", "max", "n"):
                        gj.attrs[f"alpha_star.{k}"] = float(astats[k])

                with open_h5(self.h5_path, role="reader",
                             swmr=True) as fro:
                    S = int(fro["/DataCube"].shape[0])
                n_plot = int(np.min([6, S]))
                if n_plot > 0:
                    sample = np.linspace(0, S - 1, n_plot,
                                         dtype=int).tolist()

                    base = plp.Path(self.h5_path).parent/'figures'
                    out_fits = base/'jacobi_fits.png'
                    render_aperture_fits_with_x(self.h5_path, x0_effective,
                        out_fits, apertures=sample, show_residual=True,
                        title=f"Jacobi seed overlays (spaxels={sample})")

                    out_sfh = base/'jacobi_sfh.png'
                    render_sfh_from_x(self.h5_path, x0_effective, out_sfh)
            except Exception as e:
                logger.log('[JacobiDiag] WARNING: could not run Jacobi '
                    'diagnostics:')
                logger.log_exc(e)

        # ---------------- Reader ----------------
        reader_cfg = ReaderCfg(
            s_tile=reader_s_tile,
            c_tile=reader_c_tile,
            p_tile=reader_p_tile,
            dtype_models=(reader_dtype_models or "float32"),
            apply_mask=bool(reader_apply_mask),
        )
        reader = HyperCubeReader(self.h5_path, cfg=reader_cfg)
        if verbose:
            logger.log(
                "[Pipeline] Initialized from HDF5:"
                f" S={reader.nSpat}, C={reader.nComp}, P={reader.nPop}, "
                f"L={reader.nLSpec}; "
                f"mask={'yes' if reader.has_mask else 'no'}; "
                f"models={'yes' if reader.has_models else 'no'}; "
                f"complete={reader.models_complete}"
            )

        # ---------------- Tracker wiring ----------------
        tracker = NullTracker()
        if tracker_mode != "off":
            tracker = FitTracker(self.h5_path)
            logger.log("[Pipeline] Using tracker with mode:", tracker_mode)
            logger.log('[Pipeline] Need to infer shapes...')
            with open_h5(self.h5_path, role="reader") as f:
                g = f.get("/HyperCube", None)
                if g is not None:
                    shp = g.attrs.get("shape")
                    if shp is not None and len(shp) == 4:
                        _, C, P, _ = map(int, shp)
                if "/HyperCube/models" in f:
                    _, C, P, _ = map(int, f["/HyperCube/models"].shape)
                if "/LOSVD" in f and "/Templates" in f:
                    _, _, C = map(int, f["/LOSVD"].shape)
                    P = int(f["/Templates"].shape[0])
            logger.log(f"[Pipeline] Inferred C={C}, P={P} from HDF5")
            tracker.set_meta(N=int(C)*int(P))

        cfg = MPConfig(
            epochs=int(epochs),
            lr=float(lr),
            project_nonneg=bool(project_nonneg),
            processes=int(processes),
            blas_threads=int(blas_threads),
            apply_mask=bool(reader_apply_mask),
        )

        try:
            with logger.capture_all_output():
                x_global, stats = solve_global_kaczmarz_global_step_mp(
                    self.h5_path,
                    cfg,
                    orbit_weights=orbit_weights,
                    # ratio_cfg=ratio_cfg,
                    x0=x0_effective,
                    tracker=tracker,
                )

            # Do we need to project + polish?
            if orbit_weights is not None:
                logger.log("[Pipeline] Initial MP solve done; starting quick polish...")
                E_global = read_global_column_energy(self.h5_path)   # (C,P) or None
                # Tile budget for quick polish (no extra I/O; use the reader’s known shapes)
                # --- STRICT projection (mass preserving, exact per-component)
                logger.log('[Pipeline] Enforcing orbit weights via strict '\
                    'projection.')
                cu.project_to_component_weights(
                    x_global, orbit_weights, E_cp=E_global, minw=1e-10,
                    beta=1.0
                )
                logger.log('[Pipeline] Post-projection total mass: '\
                    f'{float(np.sum(x_global)):.6e}')
                total_tiles = (
                    math.ceil(reader.nSpat / reader_s_tile)
                    * math.ceil(reader.nComp / reader_c_tile)
                    * math.ceil(reader.nPop  / reader_p_tile)
                )
                # --- QUICK POLISH (cheap 1-epoch touch-up, no projector/softbox)
                # Use the solver's own API; seed it with the just-projected x_global.
                # Keep it lightweight: a small fraction of tiles, no ratio projector, no softbox.
                try:
                    small_tile_budget = np.max((1, int(0.1 * total_tiles)))# ~10%
                except Exception:
                    small_tile_budget = 0  # fall back to solver default if your solver ignores max_tiles
                logger.log(f"[Pipeline] Running quick polish with tile budget: "
                    f"{small_tile_budget} tiles (~10% of total {total_tiles}).")

                cfg_polish = MPConfig(
                    epochs=1,
                    lr=float(lr),
                    project_nonneg=bool(project_nonneg),
                    processes=int(processes),
                    blas_threads=int(blas_threads),
                    apply_mask=bool(reader_apply_mask),
                    max_tiles=int(small_tile_budget)
                )

                x_global, _ = solve_global_kaczmarz_global_step_mp(
                    self.h5_path,
                    cfg_polish,
                    orbit_weights=None,
                    # keep the enforced mixture; no projector here
                    x0=np.asarray(x_global, np.float64, order="C"),
                    tracker=NullTracker(), # silent + cheap
                    # ratio_cfg=None, # disable ratio projector
                )
                logger.log("[Pipeline] Quick polish done.")

        finally:
            try:
                reader.close()
            except Exception:
                pass

        with open_h5(self.h5_path, role="writer") as f_wr:
            if "/X_global" in f_wr:
                del f_wr["/X_global"]
            x1d = np.asarray(x_global, np.float64).ravel(order="C")
            f_wr.create_dataset("/X_global", data=x1d, dtype=np.float64,
                chunks=(min(8192, x1d.size),), compression=None,
                shuffle=True)

        if tracker is not None:
            try: tracker.close()
            except Exception: pass

        return x_global, stats
</file>

<file path="kz_fitSpec.py">
# -*- coding: utf-8 -*-
r"""
    kz_fitSpec.py
    Adriano Poci
    University of Oxford
    2025

    Platforms
    ---------
    Unix, Windows

    Synopsis
    --------
    Master script to prepare data products and execute the workflow of CubeFit.

    Authors
    -------
    Adriano Poci <adriano.poci@physics.ox.ac.uk>

History
-------
v1.0:   2025
v1.1:   Read `zarrDir` from `kwargs` instead of hardcoding it. 12 August 2025
v1.2:   Removed `cp_flux_ref` from `reconstruct_modelcube_fast`, since now all
            public `x` API is scaled to physical units. 5 December 2025
"""
# need to set up the logger before any other imports
import pathlib as plp
from CubeFit.logger import get_logger
print("[CubeFit] Initializing CubeFit logger...")
curdir = plp.Path(__file__).parent
lfn = curdir/'kz_run.log'
logger = get_logger(lfn, mode='w')
logger.log(f"[CubeFit] CubeFit logger initialised to {logger.logfile}",
    flush=True)

import os, pdb, math, ctypes, sys
import numpy as np
import hashlib
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import matplotlib.patheffects as PathEffects
from matplotlib.colors import Normalize
from copy import copy
import h5py
import subprocess
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed,\
    ThreadPoolExecutor
from typing import Tuple, Sequence
from plotbin.display_pixels import display_pixels as dbi

from CubeFit.hdf5_manager import H5Manager, H5Dims, open_h5,\
    live_prefit_snapshot_from_models, invalidate_done
from CubeFit.hypercube_builder import build_hypercube, assert_preflight_ok,\
    estimate_global_velocity_bias_prebuild
from CubeFit.pipeline_runner   import PipelineRunner
from CubeFit import cube_utils as cu
from muse import tri_fitSpec as tf
from muse import tri_utils as uu
from dynamics.IFU.Constants import Constants, Units, UnitStr
from dynamics.IFU.Functions import Plot, Geometric

mDir = curdir.parent/'muse'
dDir = uu._ddir()

UTS = UnitStr()
UTT = Units()
CTS = Constants()
POT = Plot()
GEO = Geometric()

divcmap = 'GECKOSdr'
moncmap = 'inferno'
moncmapr = 'inferno_r'

os.environ["FITTRACKER_START"] = "fork"

# ------------------------------------------------------------------------------

def _worker_reconstruct_readonly(h5_path: str, s0: int, s1: int, x_cp_2d, *, band_L: int = 128):
    """
    4-D only: compute Y for spaxels [s0:s1) without ever loading (ΔS,C,P,L).
    Compute the contraction in float64 as requested.
    """

    x64 = np.asarray(x_cp_2d, dtype=np.float64, order="C")  # (C,P)

    with open_h5(h5_path, role="reader") as f:
        M = f["/HyperCube/models"] # (S,C,P,L) float32
        S, C, P, L = map(int, M.shape)
        dS = s1 - s0
        Y  = np.empty((dS, L), dtype=np.float64)

        # λ-banding: never hold (ΔS,C,P,L) in RAM
        for i, s in enumerate(range(s0, s1)):
            y = np.empty(L, dtype=np.float64)
            for l0 in range(0, L, band_L):
                l1   = min(L, l0 + band_L)
                A32  = M[s, :, :, l0:l1][...] # (C,P,Lb), f32
                A64  = A32.astype(np.float64, copy=False) # cast just the band
                y[l0:l1] = np.tensordot(x64, A64, axes=([0,1],[0,1]))
            Y[i, :] = y

    if not Y.flags.c_contiguous:
        Y = np.ascontiguousarray(Y)
    return (s0, Y)

def _worker_compute_tile(h5_path, s0, s1, x_cp64):
    x_cp64 = np.asarray(x_cp64, dtype=np.float64, order="C")

    with open_h5(h5_path, role="reader") as f:
        M = f["/HyperCube/models"]                     # (S,C,P,L) f32
        _, C, P, L = map(int, M.shape)
        # infer P_chunk from dataset chunks; fallback to full P
        P_chunk = (M.chunks[2] if getattr(M, "chunks", None) else P)
        dS = s1 - s0
        Y  = np.zeros((dS, L), dtype=np.float64)

        for p0 in range(0, P, P_chunk):
            p1   = min(P, p0 + P_chunk)
            slab = M[s0:s1, :, p0:p1, :][...] # (dS,C,Pb,L) f32
            A64  = slab.astype(np.float64, copy=False) # cast once per P_chunk
            xblk = x_cp64[:, p0:p1] # (C,Pb) f64
            Y   += np.tensordot(A64, xblk, axes=([1, 2], [0, 1])) # (dS,L)
    return s0, Y

# ------------------------------------------------------------------------------

def load_xring_best(sidecar_path: str, ring_idx: int = 95, as_physical: bool = True,
                    cp_flux_ref: np.ndarray | None = None) -> np.ndarray:
    """
    Load x_ring[ring_idx] from sidecar /Fit/x_ring and return a 1-D float64
    vector suitable as x0 for genCubeFit. If as_physical=True and a
    cp_flux_ref array is provided, convert from solver-normalized (x_CP)
    -> physical (X_phys) before returning.
    """
    with open_h5(sidecar_path, role="reader") as f:
        fit = f.get("/Fit")
        if fit is None:
            raise KeyError(f"no /Fit group in {sidecar_path!r}")
        if "x_ring" not in fit:
            raise KeyError(f"/Fit/x_ring not found in {sidecar_path!r}")

        x_ring = np.asarray(fit["x_ring"])  # shape (Nring, C*P) or (Nring, C, P)

    # Accept both flattened and (C,P) forms
    if x_ring.ndim == 1:
        # single-vector stored (no ring axis) — treat as the vector
        vec = x_ring.astype(np.float64).ravel(order="C")
    elif x_ring.ndim == 2:
        if ring_idx < 0 or ring_idx >= x_ring.shape[0]:
            raise IndexError(f"ring_idx {ring_idx} out of range (0..{x_ring.shape[0]-1})")
        vec = x_ring[ring_idx].astype(np.float64).ravel(order="C")
    elif x_ring.ndim == 3:
        # stored as (Nring, C, P) — flatten afterwards
        if ring_idx < 0 or ring_idx >= x_ring.shape[0]:
            raise IndexError(f"ring_idx {ring_idx} out of range (0..{x_ring.shape[0]-1})")
        vec = x_ring[ring_idx].astype(np.float64).ravel(order="C")
    else:
        raise RuntimeError(f"unexpected /Fit/x_ring shape {x_ring.shape}")

    # Optional conversion from normalized basis x_CP -> physical X_phys:
    if as_physical and (cp_flux_ref is not None):
        # cp_flux_ref expected shape (C, P)
        CtimesP = vec.size
        cp = np.asarray(cp_flux_ref, np.float64).ravel(order="C")
        if cp.size != CtimesP:
            # allow cp provided as (C,P) array
            cp = np.asarray(cp_flux_ref, np.float64).ravel(order="C")
            if cp.size != CtimesP:
                raise ValueError("cp_flux_ref size does not match x_ring length")
        inv_cp = 1.0 / np.maximum(cp, 1e-30)
        vec = (vec * inv_cp).astype(np.float64, order="C")

    return vec

# --- example use in genCubeFit ---
# sidecar_path = "/path/to/your/sidecar.h5"
# x0 = load_xring_best(sidecar_path, ring_idx=95, as_physical=True, cp_flux_ref=maybe_cp_flux)
# pass x0 into the routine that accepts initial x0 (shape (C*P,))

# ------------------------------------------------------------------------------

def genCubeFit(galaxy, mPath, decDir=None, nCuts=None, proj='i', SN=90,
    full=False, slope=1.30, IMF='KB', iso='pad', weighting='luminosity',
    lOrder=4, rescale=False, specRange=None, lsf=False, band='r', smask=None,
    method='fsf', varIMF=False, source='ppxf', redraw=False, runSwitch='gen',
    **kwargs):
    """
    _summary_

    Parameters
    ----------
    galaxy : str
        Name of the galaxy to process.
    mPath : str
        Path to the model directory.
    decDir : str, optional
        Decomposition directory name, by default None.
    nCuts : int, optional
        Number of cuts for decomposition, by default None.
    proj : str, optional
        Projection type, by default 'i'.
    SN : int, optional
        Signal-to-noise ratio, by default 90.
    full : bool, optional
        Whether to use full data or truncated, by default False.
    slope : float, optional
        Slope for the IMF, by default 1.30.
    IMF : str, optional
        Initial mass function type, by default 'KB'.
    iso : str, optional
        Isochrones type, by default 'pad'.
    weighting : str, optional
        Weighting scheme, by default 'luminosity'.
    nProcs : int, optional
        Number of processes to use, by default 1.
    lOrder : int, optional
        Polynomial order for the fit, by default 4.
    fit : str, optional
        Type of fit to perform, by default 'cube'.
    rescale : bool, optional
        Whether to rescale the data, by default False.
    specRange : tuple, optional
        Spectral range to consider, by default None.
    lsf : bool, optional
        Whether to apply LSF (Line Spread Function), by default False.
    band : str, optional
        Band to use for the fit, by default 'r'.
    smask : str, optional
        Mask for the spectra, by default None.
    method : str, optional
        Method to use for the fit, by default 'fsf'.
    varIMF : bool, optional
        Whether to use variable IMF, by default False.
    source : str, optional
        Source of the data, by default 'ppxf'.
    **kwargs : dict, optional
        Additional keyword arguments for the function.
    """

    # Directories
    bDir = mDir/'tri_models'/mPath
    pDir = curdir.parent/'pxf'
    figDir = curdir/galaxy/'figures'
    MKDIRS = [bDir, pDir, figDir]
    [plp.Path(DIR).mkdir(parents=True, exist_ok=True) for DIR in MKDIRS]
    if isinstance(decDir, type(None)):
        with open(bDir/'decomp.dir', 'r+') as dd:
            decDir = dd.readline().strip()
    if isinstance(nCuts, type(None)):
        direc = list(filter(lambda xd: xd.is_dir(),
            (bDir/decDir).glob('decomp_*')))[0]
    else:
        direc = bDir/decDir/f"decomp_{nCuts:d}"
    if 'fif' in method:
        IMF = 'FIF'
        iso = 'fif'
    if not full:
        tEnd = 'trunc'
    else:
        tEnd = 'full'
    w8Str = f"{weighting[0].upper()}W"
    tag = f"_SN{int(SN):02d}_{iso}_{IMF}{slope:.2f}_{w8Str}"
    # Filenames
    kin = pDir/galaxy/f"kinematics_SN{SN:02d}.xz"
    pfs = pDir/galaxy/f"pixels_SN{SN:02d}.xz"
    sfs =  pDir/galaxy/f"selection_SN{SN:02d}_{tEnd}.xz"
    vbSpec = pDir/galaxy/f"voronoi_SN{SN:02d}_{tEnd}.xz"
    mlfn = pDir/galaxy/f"ML{tag}.xz"
    infn = bDir/'infil.xz'
    gfn = curdir/'obsData'/f"{galaxy}.xz"

    INF = uu.Load.lzma(infn)
    PA = INF['angle'][0]

    xpix, ypix, sele, pixs = uu.Load.lzma(pfs)
    # saur,goods = uu.Load.lzma(sfs)
    # del saur
    xbix, ybix = GEO.rotate2D(xpix, ypix, PA)
    pfn = dDir.parent/'muse'/'obsData'/f"{galaxy}-poly-rot.xz"
    polyProps = dict(ec=POT.brown, linestyle='--', fill=False, zorder=100,
        lw=0.75, salpha=0.5)
    if pfn.is_file():
        aShape = uu.Load.lzma(pfn)
        aShape, pPatch = POT.polyPatch(POLYGON=aShape, Xpo=xbix, Ypo=ybix,
            **polyProps)
    else:
        aShape, pPatch = POT.polyPatch(Xpo=xbix, Ypo=ybix, **polyProps)
        uu.Write.lzma(pfn, aShape)
    xmin, xmax = np.amin(xbix), np.amax(xbix)
    ymin, ymax = np.amin(ybix), np.amax(ybix)
    xLen, yLen = np.ptp(xbix), np.ptp(ybix) # unmasked pixels

    saur, goods = uu.Load.lzma(pDir/galaxy/f"selection_SN{SN:02d}_{tEnd}.xz")
    xpix = np.compress(goods, xpix)
    ypix = np.compress(goods, ypix)
    xbix = np.compress(goods, xbix)
    ybix = np.compress(goods, ybix)

    # Data spectra
    if vbSpec.is_file():
        VB = uu.Load.lzma(vbSpec)
        binNum = VB['binNum']
        binCounts = VB['binCounts']
        binFlux = VB['binFlux']
        del VB
    else:
        raise RuntimeError(f"No binned spectra.\n{'': <4s}{vbSpec}")

    with logger.capture_all_output():
        decDir, cDirs, cKeys, nComp, teLL, lnGrid, histBinSize, dataVelScale,\
            RZ, spLL, laGrid, lmin, lmax, umetals, uages, ualphas, pixOff = \
            tf._oneTimeSpec(galaxy=galaxy, mPath=mPath, decDir=decDir,
            nCuts=nCuts, proj=proj, SN=SN, full=full, slope=slope, IMF=IMF,
            iso=iso, weighting=weighting, lOrder=lOrder, rescale=rescale,
            lsf=lsf, specRange=specRange, band=band, method=method,
            varIMF=varIMF, source=source, **kwargs)
    nLSpec, nSpat = laGrid.shape
    nTSpec, nMetals, nAges, nAlphas = lnGrid.shape
    nSSP = int(np.prod((nMetals, nAges, nAlphas), dtype=int))
    pred = f"0{len(repr(nComp)):d}"
    nComp = int(nComp)

    oDict = uu.Load.lzma(direc/f"decomp_{nCuts:d}.plt")
    if 'binFN' not in oDict.keys():
        oDict['binFN'] = 'bins_0.dat'
        oDict['apFN'] = 'aperture_0.dat'
        uu.Write.lzma(direc/f"decomp_{nCuts:d}.plt", oDict)
    binFN = oDict['binFN']
    apFN = oDict['apFN']
    dnPix, dgrid = uu.Read.bins(bDir/'infil'/binFN)
    dnbins = int(np.max(dgrid))
    dgrid -= 1
    dss = np.where(dgrid >= 0)[0]
    dx0, dx1, dnx, dy0, dy1, dny, dtheta = uu.Read.aperture(
        bDir/'infil'/apFN)
    ddx = np.abs((dx1-dx0)/dnx)
    ddy = np.abs((dy1-dy0)/dny)
    dpixs = np.min([ddx, ddy])
    dxr = np.arange(dnx)*dpixs + dx0 + 0.5*dpixs
    dyr = np.arange(dny)*dpixs + dy0 + 0.5*dpixs
    dxtss = uu._hash(dxr, np.full_like(dyr, 1)).ravel()[dss]
    dytss = uu._hash(np.full_like(dxr, 1), dyr).ravel()[dss]
    dtestX, dtestY = GEO.rotate2D(dxtss, dytss, dtheta)
    duPix, dpInverse, dpCounts = np.unique(dgrid[dss], return_inverse=True,
        return_counts=True)
    dpCount = dpCounts[dpInverse]

    biI = INF['bins'][0]
    bCount = biI['pCountsBin']
    # grid = np.array(biI['grid'], dtype=int).ravel()-1
    grid = np.array(biI['grid'], dtype=int).T.ravel()-1
    nbins = np.max(grid).astype(int)+1
    ss = np.where(grid >= 0)[0]

    if np.max(dpCount) > 1: # at least one bin contains more than one pixel
        # a quick way to check if the oberved scheme was used
        dgrid = grid
        dss = ss
        dnbins = nbins
        dpCount = bCount

    nzComp = np.array(oDict['nzComp'], dtype=int)
    nnOrb = plp.Path(*oDict['nnOrb'])
    oClass = plp.Path(*oDict['oClass'])
    obClass = plp.Path(*oDict['obClass'])
    bLKey = uu.keySep.join([nnOrb.parent.parent.name, nnOrb.parent.name])
    bLKey = uu.rReplace(bLKey, uu.keySep, os.sep, 1)
    nnOrb = plp.Path(bDir, decDir, nnOrb.parent.name, nnOrb.name)
    oClass = plp.Path(bDir, decDir, oClass.parent.name, oClass.name)
    obClass = plp.Path(bDir, decDir, obClass.parent.name, obClass.name)
    fpd = uu._deetExtr(bLKey)
    apDir = bDir/bLKey/'nn_aphist.out'
    maDir = (bDir/bLKey).parent/'datfil'/'mass_aper.dat'
    nnK = bDir/bLKey/'nn_kinem.out'

    NOrbs, inds, energs, I2s, I3s, regs, types, weights, lcuts =\
        uu.Read.orbits(nnOrb)
    cWeights = np.array([
        np.ma.sum(oDict['weights'][f"{comp:{pred}d}"]) for comp in nzComp])

    kiBin = INF['kin']['nbins'][0]
    assert nbins == kiBin, 'Output does not agree with input bins\nInput:'+\
        f"{kiBin}\nOutput: {nbins}"

    wbin, hN, histBinSize, hArr = uu.Read.apertureHist(apDir)
    logger.log(f"{'Mass outside of the histograms:': <45s}"\
          f"{np.sum(hArr[:, 0] + hArr[:, wbin * 2]):5.5}")

    fullBin, fullID, fullK0 = uu.Read.massAperture(maDir)
    logger.log(f"{'Mass normalisation is:': <45s}"\
        f"{np.sum(hArr) / np.sum(fullK0):5.5}")
    if isinstance(proj, list):
        pStr = ''.join([str(f) for f in proj])
    else:
        pStr = str(proj)
    plt.close('all')
    massNorm = fullK0

    apMassFile = direc/f"apMass_i{proj}_{nComp:{pred}d}.xz"
    if apMassFile.is_file():
        aperMass = uu.Load.lzma(apMassFile)
    else:
        aperMass = np.ma.ones((nSpat, nComp), dtype=float)*np.nan
        ERR = []
        for cn, cDir in tqdm(enumerate(cDirs), desc='Mass', total=nComp):
            try:
                maFile = cDir/'declib_apermass.out'
                nbin, ID, k0 = uu.Read.massAperture(maFile)
                aperMass[:, cn] = k0
            except:
                ERR += [cDir.name]
        if len(ERR) > 0:
            logger.log(ERR)
            breakpoint()
        uu.Write.lzma(apMassFile, aperMass)
    aperMass = np.ma.masked_invalid(aperMass)
    norma = np.sum(aperMass, axis=1)

    logger.log('Done.', flush=True)
    apFile = cDirs[0]/'declib_aphist.out'
    wbin, hN, histBinSize, hArr = uu.Read.apertureHist(apFile)
    # Load the parameters regardless
    apHistFile = direc/f"apHists_i{pStr}_{nComp:{pred}d}.jl"
    if apHistFile.is_file():
        logger.log('Reading histograms...', flush=True)
        apHists = uu.Load.jobl(apHistFile)
    else:
        apFile = cDirs[0]/'declib_aphist.out'
        wbin, hN, histBinSize, cArr = uu.Read.apertureHist(apFile)
        logger.log('Generating histograms...', flush=True)
        apHists = np.ma.ones((*cArr.shape, nComp))*np.nan
        ERR = []
        for cn, cDir in tqdm(enumerate(cDirs), desc='Components',
            total=nComp):
            try:
                apFile = cDir/'declib_aphist.out'
                wbin, hN, histBinSize, cArr = uu.Read.apertureHist(
                    apFile)
                apHists[:, :, cn] = cArr
            except:
                ERR += [cDir.stem]
        if len(ERR) > 0:
            logger.log(ERR)
            pdb.set_trace()
        uu.Write.jobw(apHistFile, apHists)
    logger.log('Done.')
    apHists = np.ma.masked_invalid(apHists)
    nApHists = (apHists*(massNorm/norma)[:, np.newaxis, np.newaxis])
    # nApHists /= binFlux[:, np.newaxis, np.newaxis]
    hbi = wbin*2 + 1
    vbins = (np.arange(hbi)-wbin)*histBinSize
    # (nSpat, nVel, nComp)

    logger.log('Generating spectral mask...', flush=True)
    spmask = np.ones(nLSpec, dtype=bool)
    with open(dDir/'emissionLines.txt', 'r+') as emlf:
        emMask = np.genfromtxt(emlf, usecols=(0, 1))
    for emm in emMask:
        smask += [[emm[0]-emm[1]/2.0, emm[0]+emm[1]/2.0]]
    if len(smask)>0:
        for pair in smask:
            spmask[(spLL>=np.log(pair[0])) & (spLL<=np.log(pair[1]))] = False
    logger.log('Done.', flush=True)

    # --- Setup HDF5 directory ---
    hdf5Dir = plp.Path(kwargs.pop('hdf5Dir', curdir/galaxy))
    hdf5Dir.mkdir(parents=True, exist_ok=True)
    hdf5Path = (hdf5Dir/f"{galaxy}_{nComp}_{lOrder:02d}").with_suffix('.h5')

    # --- Initialize and load data ---
    # mgr = H5Manager(hdf5Path, tem_pix=copy(teLL), obs_pix=copy(spLL))
    mgr = H5Manager(hdf5Path)
    # mgr.set_velocity_grid(copy(vbins))
    arDims = mgr.populate_from_arrays(
        losvd=nApHists,
        datacube=laGrid,
        templates=lnGrid,
        mask=spmask,
        tem_pix=copy(teLL), obs_pix=copy(spLL),
        vel_pix=copy(vbins),
        xpix=xpix, ypix=ypix,
        binnum=binNum,
        orbit_weights=cWeights,
    )
    mgr.ensure_rebin_and_resample()

    # --- 2. Precompute HyperCube ---
    if redraw and ('gen' in runSwitch):
        logger.log('[CubeFit] Calling `invalidate_done` to regenerate '\
            '/HyperCube.')
        invalidate_done(hdf5Path)

    nS, nC, nP = 128, 1, 360
    # --- Optional hard gate before any heavy work
    # Use small prefix slices if nothing is specified.
    with logger.capture_all_output():
        _ = assert_preflight_ok(
            hdf5Path,
            s_list=list(range(int(np.minimum(3, nS)))),
            c_list=list(range(int(np.minimum(2, nC)))),
            p_list=list(range(int(np.minimum(6, nP)))),
            # keep tolerances in sync with preflight defaults
            tol_rel=2e-3,
            tol_shift_px=0.5,
            tol_flat_valid=3e-8,
            require_rt_flat=True,
            rt_flat_tol=3e-8,
            verbose=True,
        )

        est = estimate_global_velocity_bias_prebuild(hdf5Path,
            n_spax=96, n_features=24, window_len=31, lag_px=12)

    logger.log(f"[CubeFit] Estimated global velocity bias (km/s): "\
        f"{est['vel_bias_kms']:.3f}")
    logger.log(f"[CubeFit] Building /HyperCube in {hdf5Path}...")
    with logger.capture_all_output():
        build_hypercube(
            hdf5Path,
            norm_mode="model", # choose "model" or "data"
            # "model" preserves relative contribution to both spaxel and components
            amp_mode="sum", # "sum" or "trapz"
            S_chunk=nS, C_chunk=nC, P_chunk=nP,
            vel_bias_kms=est["vel_bias_kms"]
        )
    # even if runSwitch is fit only, we want to ensure the HyperCube
    # is built, so we don't return early here.
    # Should be zero-cost if already built

    prefit_png = figDir / f"prefit_overlay_from_models_C{nComp:03d}.png"
    with logger.capture_all_output():
        live_prefit_snapshot_from_models(
            h5_path=str(hdf5Path),
            max_components=4,
            templates_per_pair=3,
            out_png=str(prefit_png),
        )
    logger.log(f"[Prefit] wrote {prefit_png}")
    if 'gen' in runSwitch:
        return

    if 'fit' not in runSwitch:
        logger.log(f"[CubeFit] runSwitch={runSwitch} is not understood; "
            "exiting.")
        raise RuntimeError("Invalid runSwitch")
    # --- 4) Run the global Kaczmarz fit (tiled; RAM-bounded) ---
    runner = PipelineRunner(hdf5Path)

    ncpuset, mask = cu.cpuset_count()
    print(f"[guard] cpuset mask: {mask}  cores: {ncpuset}")
    logger.log(f"cpuset cores: {len(os.sched_getaffinity(0))}")
    from threadpoolctl import threadpool_info
    logger.log(f"BLAS pools: {threadpool_info()}")
    # look for openblas 
    try:
        with open("/proc/self/cgroup") as f:
            print("[guard] cgroups:")
            for line in f:
                if "cpuset" in line or "cpu" in line:
                    print(" ", line.strip())
    except Exception:
        pass

    print("[env] SLURM_JOB_ID=", os.environ.get("SLURM_JOB_ID"),
        " SLURM_STEP_ID=", os.environ.get("SLURM_STEP_ID"))

    ########################
    # Non-batched Kaczmarz #
    ########################
    # x_global, stats = runner.solve_all(
    #     epochs=1,
    #     pixels_per_aperture=4096,
    #     lr=0.25,
    #     project_nonneg=True,
    #     # orbit_weights=cWeights,
    #     orbit_weights=None,
    #     ratio_use=False,
    #     reader_s_tile=nS, reader_c_tile=nC, reader_p_tile=nP,
    #     verbose=True,
    #     warm_start='jacobi',  # 'zeros', 'random', 'jacobi'
    #     row_order='sequential',
    #     block_rows=2048,
    #     blas_threads=48,
    #     progress_interval_sec=900,
    #     tracker_mode='off',
    # # Optional ratio controls
    # # ratio_use=True, ratio_anchor="auto", ratio_eta=0.05, ratio_prob=0.02,
    # # ratio_batch=2, ratio_min_weight=1e-4,
    # # progress_interval_sec=60,  # if you want periodic on_progress ticks
    # )

    # sidecar = cu._find_latest_sidecar(hdf5Path)
    # ridx = 95
    # x0 = load_xring_best(
    #     sidecar,
    #     ring_idx=ridx,
    #     as_physical=True,
    #     cp_flux_ref=None,
    # )
    # logger.log(f"[CubeFit] Loaded `x0` from sidecar {sidecar} at ring {ridx}.")
    
    #####################################
    # Multi-processing Batched Kaczmarz #
    #####################################
    RC = cu.RatioCfg(
        anchor="target",   # pull toward /CompWeights
        eta=0.8,           # per-tile multiplicative strength
        gamma=2.0,         # clamp for multiplicative factors
        prob=1.0,          # apply every tile
        batch=0,           # consider all active components
        minw=1e-5,         # ignore ~zero-weight targets
        # epoch-level projection (gentle, blended)
        epoch_project=True,
        epoch_eta=0.6,     # how hard the epoch projection pushes
        epoch_gamma=3.0,   # clamp at epoch projection
        epoch_beta=0.35,   # 0..1 blend toward the projected state
        epoch_renorm=True, # keep target sum=1
        tile_every=1,      # apply per tile
        use=True           # actually enable the ratio term
    )
    x_global, stats = runner.solve_all_mp_batched(
        epochs=3,
        # x0=x0,
        lr=0.01,
        project_nonneg=True,
        orbit_weights=None, # or None for “free” fit
        ratio_cfg=None,
        # orbit_weights=cWeights, # Make sure to enable `CUBEFIT_ORBIT_BETA`
        # ratio_cfg=RC,
        processes=4, # 4 workers
        blas_threads=12, # 12 BLAS threads each → 48 total
        reader_s_tile=128, # match /HyperCube/models chunking on S
        verbose=True,
        warm_start='nnls',  # 'zeros', 'resume', 'jacobi', 'nnls'
        seed_cfg=dict(Ns=128, L_sub=1200, K_cols=768, per_comp_cap=24),
    )

    logger.log("[CubeFit] Global fit completed.")
    # # --- Save the global solution vector
    # logger.log("[CubeFit] Saving fit results...")
    # with open_h5(hdf5Path, role="writer") as f:
    #     ds = f.get("/X_global", None)
    #     if ds is not None and ds.shape == (x_global.size,):
    #         ds[...] = np.asarray(x_global, np.float64)
    #         logger.log("[CubeFit] Dataset already exists; overwritten.")
    #     else:
    #         if "/X_global" in f:
    #             del f["/X_global"]
    #         f.create_dataset(
    #             "/X_global",
    #             data=np.asarray(x_global, np.float64),
    #             chunks=(min(8192, x_global.size),),
    #             compression="gzip", compression_opts=4, shuffle=True,
    #         )
    #         logger.log("[CubeFit] Results stored.")

# ------------------------------------------------------------------------------
# HDF5 helpers (added)
# ----------------------------------------------------------------------

def _coerce_h5_path(h5_or_path) -> str:
    """
    Return a filesystem path to the HDF5 file. Accepts str/Path or an
    h5py.File (uses .filename). Raises on unsupported input.
    """
    if isinstance(h5_or_path, (str, os.PathLike, plp.Path)):
        return str(h5_or_path)
    if isinstance(h5_or_path, h5py.File):
        return str(h5_or_path.filename)
    raise TypeError(f"Expected HDF5 file path or h5py.File; got {type(h5_or_path)}")

def _h5_exists(h5_path: str, key: str) -> bool:
    with open_h5(h5_path, role="reader") as f:
        return key in f

# ----------------------------------------------------------------------
# Rewritten HDF5-native functions
# ----------------------------------------------------------------------

def compute_model_batch_global(
    h5_or_path,
    batch_idx: int,
    x_global,
    nSpat: int,
):
    """
    Reconstruct a batch of model spectra from /HyperCube/models in HDF5.
    Uses the dataset's spatial chunk size as the batch length.

    Returns
    -------
    (start, Y) : (int, ndarray (m, nLSpec))
    """
    h5_path = _coerce_h5_path(h5_or_path)
    with open_h5(h5_path, role="reader") as f:
        models = f["/HyperCube/models"]
        if models.ndim == 5:
            nB, B, nC, nP, nL = models.shape
            start = batch_idx * B
            if start >= nSpat:
                return start, np.empty((0, nL), dtype=np.float64)
            m = int(min(B, nSpat - start))
            Y = np.empty((m, nL), dtype=np.float64)
            x2 = (x_global.reshape(nC, nP) if getattr(x_global, "ndim", 1) == 1 else x_global)
            x2 = np.asarray(x2, dtype=np.float64, order="C")
            for s in range(m):
                b, i = divmod(start + s, B)
                spec = np.asarray(models[b, i, :, :, :], order="C")
                Y[s, :] = np.tensordot(spec, x2, axes=([0, 1], [0, 1]))
            return start, Y
        elif models.ndim == 4:
            S, nC, nP, nL = models.shape
            B = (models.chunks[0] if getattr(models, "chunks", None) else 32)
            start = batch_idx * B
            if start >= nSpat:
                return start, np.empty((0, nL), dtype=np.float64)
            m = int(min(B, nSpat - start))
            slab = np.asarray(models[start:start + m, :, :, :], order="C")
            x2 = (x_global.reshape(nC, nP) if getattr(x_global, "ndim", 1) == 1 else x_global)
            x2 = np.asarray(x2, dtype=np.float64, order="C")
            Y = np.tensordot(slab, x2, axes=([1, 2], [0, 1]))
            return start, Y
        else:
            raise RuntimeError(f"Unexpected /HyperCube/models rank {models.ndim}")

def parallel_model_cube_global_batched(
    h5_or_path,
    x_global,
    nSpat: int,
    nLSpec: int,
    n_workers: int = 1,
    array_name: str = "ModelCube",
    spat_tile: int | None = None,
    compression: str | None = None,
    compression_opts: int | None = None,
    shuffle: bool = False,
):
    """
    Reconstruct full model cube from a global x and store (nSpat, nLSpec) f64
    into HDF5. **Single-writer** pattern:
      - Single-process path: one "r+" handle does read & write.
      - Multi-process path: main process holds the only "r+" handle and writes;
        workers open read-only and return their (s0, Y) blocks.
    """
    os.environ.setdefault("HDF5_USE_FILE_LOCKING", "FALSE")
    h5_path = str(h5_or_path)

    # decide compression kwargs once
    comp_kwargs = {}
    if compression is None or compression is False:
        comp_kwargs = {}
    elif compression == "lzf":
        comp_kwargs = dict(compression="lzf", shuffle=shuffle)
    elif compression == "gzip":
        level = 1 if compression_opts is None else int(compression_opts)
        comp_kwargs = dict(compression="gzip", compression_opts=level, shuffle=shuffle)
    else:
        raise ValueError(f"Unsupported compression: {compression}")

    # Inspect dims/chunks via short-lived read handle
    with open_h5(h5_path, role="reader") as f_r:
        models0 = f_r["/HyperCube/models"]
        if models0.ndim == 4:
            S_disk, C_disk, P_disk, L_disk = models0.shape
            assert S_disk == nSpat, f"S mismatch: {S_disk} vs {nSpat}"
            assert L_disk == nLSpec, f"L mismatch: {L_disk} vs {nLSpec}"
            (S_chunk, C_chunk, P_chunk, L_chunk) = models0.chunks if getattr(models0, "chunks", None) else (128, C_disk, P_disk, L_disk)
        elif models0.ndim == 5:
            nB, B, C_disk, P_disk, L_disk = models0.shape
            assert nB * B == nSpat, f"S mismatch: {nB*B} vs {nSpat}"
            assert L_disk == nLSpec, f"L mismatch: {L_disk} vs {nLSpec}"
            S_chunk, C_chunk, P_chunk, L_chunk = (B, C_disk, P_disk, L_disk)
        else:
            raise RuntimeError(f"Unexpected HyperCube/models rank: {models0.ndim}")

    # Tile size along S
    if spat_tile is None:
        spat_tile = max(S_chunk * 4, 1)
    # Ensure destination dataset exists (writer)
    with open_h5(h5_path, role="writer") as f_w:
        if array_name in f_w:
            out = f_w[array_name]
            if out.shape != (nSpat, nLSpec) or str(out.dtype) != "float64":
                del f_w[array_name]
                out = f_w.create_dataset(array_name, shape=(nSpat, nLSpec),
                    dtype="f8", chunks=(min(spat_tile, nSpat), nLSpec),
                    **comp_kwargs)
        else:
            out = f_w.create_dataset(array_name, shape=(nSpat, nLSpec),
                dtype="f8", chunks=(min(spat_tile, nSpat), nLSpec),
                **comp_kwargs)

    # Precompute 2-D view of x (C,P) for GEMMs
    nC, nP = int(C_disk), int(P_disk)
    x_cp = (x_global.reshape(nC, nP) if getattr(x_global, "ndim", 1) == 1 else x_global)
    x_cp = np.asarray(x_cp, dtype=np.float64, order="C")

    # Build ranges
    ranges = []
    s = 0
    while s < nSpat:
        e = min(s + spat_tile, nSpat)
        ranges.append((s, e))
        s = e

    L_chunk = L_chunk
    print(f"[Reconstruct] S={nSpat} L={nLSpec} (L_band={L_chunk}) spat_tile={spat_tile} nTiles={len(ranges)} n_workers(requested)={n_workers}")

    # Single-process path
    if n_workers <= 1:
        with open_h5(h5_path, role="writer") as f:
            # source and destination in the same handle
            models = f["/HyperCube/models"]  # (S, C, P, L), stored as float32
            if array_name in f:
                out = f[array_name]
                if out.shape != (nSpat, nLSpec):
                    del f[array_name]
                    out = f.create_dataset(
                        array_name, shape=(nSpat, nLSpec), dtype="f8",
                        chunks=(spat_tile, nLSpec), **comp_kwargs)
            else:
                out = f.create_dataset(
                    array_name, shape=(nSpat, nLSpec), dtype="f8",
                    chunks=(spat_tile, nLSpec), **comp_kwargs)

            # reconstruct each spaxel tile without opening a second handle
            for (s0, s1) in tqdm(ranges, desc="[Reconstruct] tiles"):
                slab = np.asarray(models[s0:s1, :, :, :], dtype=np.float64, order="C")
                dS, C, P, L = slab.shape
                # A(s) is (N,L) with N=C*P.  Compute y_hat(s) = A(s)^T @ x.
                # AFTER (mirror the worker logic)
                # slab: (dS, C, P, L), x_cp: (C, P)
                Y = np.tensordot(slab, x_cp, axes=([1, 2], [0, 1])) # -> (dS, L)
                # (optional) ensure dtype/layout
                if Y.dtype != np.float64: Y = Y.astype(np.float64, copy=False)
                if not Y.flags["C_CONTIGUOUS"]: Y = np.ascontiguousarray(Y)
                out[s0:s1, :] = Y

        logger.log("[Reconstruct] Done (single-process).")
        return

    S_tile = S_chunk   # or 2*S_chunk if you have headroom

    # Build S-tiles (chunk aligned)
    ranges = [(s0, min(S_disk, s0+S_tile)) for s0 in range(0, S_disk, S_tile)]

    # Prepare output with tile-aligned chunks; uncompressed is fastest
    with open_h5(h5_path, role="writer") as f:
        if array_name in f:
            del f[array_name]
        out = f.create_dataset(array_name, shape=(S_disk, L_disk), dtype="f8",
                               chunks=(min(S_tile, S_disk), L_disk))

    # Don’t fork the parent’s big arrays
    ctx = mp.get_context("fork")
    with ProcessPoolExecutor(max_workers=min(n_workers, len(ranges)),
                             mp_context=ctx,
                             initializer=_init_worker) as pool:
        futs = [pool.submit(_worker_compute_tile, h5_path, s0, s1, x_cp)
                for (s0, s1) in ranges]
        for fut in tqdm(
            as_completed(futs),
            total=len(ranges),
            desc="[Reconstruct] tiles",
            unit="tile",
            dynamic_ncols=True,
            miniters=1,
            leave=True,
        ):
            s0, Y = fut.result()
            s1 = s0 + Y.shape[0]
            with open_h5(h5_path, role="writer") as f:
                f[array_name][s0:s1, :] = Y
    print("[Reconstruct] Done (multi-process).")

# ------------------------------------------------------------------------------

def reconstruct_model_cube_single(h5_path: str,
                                  x_global,
                                  array_name: str = "ModelCube",
                                  blas_threads: int = 8,
                                  S_tile: int | None = None,
                                  L_band: int = 128) -> None:
    """
    Chunk-aligned, streaming reconstruction of Y = (HyperCube) · x.

    Reads the model cube in the same order it is chunked:
      S in blocks of S_chunk, C in blocks of 1 (C_chunk=1), and P in
      blocks of P_chunk. For each (S_chunk, C=1) slab it contracts over
      P against x[c, :] using BLAS on small 2-D views, accumulating into
      a (S_chunk, L) float64 tile. Optionally processes L in bands to
      keep the GEMV working set small. Never materializes a (ΔS,C,P,L)
      float64 slab.

    Parameters
    ----------
    h5_path : str
        Path to the base HDF5 file.
    x_global : array-like
        Global weights, shape (C*P,) or (C,P). Internally used in f32
        for speed; accumulation into output stays f64.
    array_name : str
        Name of output dataset to create (S,L) float64.
    blas_threads : int
        Number of BLAS threads to use inside this process.
    S_tile : Optional[int]
        If given, caps the S block size. Otherwise uses S_chunk.
    L_band : int
        Optional λ-banding for GEMV working set (does not change how
        HDF5 reads; it just limits the temporary 2-D views).
    """

    # Try to limit BLAS threads for predictability on clusters
    try:
        from threadpoolctl import threadpool_limits
    except Exception:
        threadpool_limits = None

    t = int(os.environ.get("SLURM_CPUS_PER_TASK", blas_threads))
    os.environ["OMP_NUM_THREADS"] = os.environ["MKL_NUM_THREADS"] = \
        os.environ["OPENBLAS_NUM_THREADS"] = str(t)
    if threadpool_limits:
        try:
            threadpool_limits(t)
        except Exception:
            pass

    # Small helper to return free heap to the OS (keeps RSS flat).
    try:
        libc = ctypes.CDLL("libc.so.6")
        def trim_heap(): libc.malloc_trim(0)
    except Exception:
        def trim_heap(): pass

    # Optional dataset chunk-cache tuning (local to this dataset)
    rdcc_slots = int(os.environ.get("CUBEFIT_RDCC_SLOTS", "1000003"))
    rdcc_bytes = int(os.environ.get("CUBEFIT_RDCC_BYTES", str(256 * 1024**2)))
    rdcc_w0    = float(os.environ.get("CUBEFIT_RDCC_W0", "0.90"))

    # Force unbuffered writes so tqdm appears immediately on HPC logs
    try:
        sys.stdout.reconfigure(line_buffering=True)  # py3.7+
    except Exception:
        pass
    os.environ.setdefault("PYTHONUNBUFFERED", "1")

    with open_h5(h5_path, role="writer") as f:
        M = f["/HyperCube/models"]              # (S,C,P,L) float32
        S, C, P, L = map(int, M.shape)
        try:
            M.id.set_chunk_cache(rdcc_slots, rdcc_bytes, rdcc_w0)
        except Exception:
            pass

        chunks = M.chunks or (S, 1, P, L)
        S_chunk, C_chunk, P_chunk, L_chunk = map(int, chunks)

        # Choose S block aligned to storage chunk (or user cap).
        S_blk = S_chunk if S_tile is None else max(1, int(S_tile))
        S_blk = min(S_blk, S_chunk, S)

        # Prepare output (tile-aligned; uncompressed is fastest).
        if array_name in f:
            del f[array_name]
        out = f.create_dataset(array_name, shape=(S, L), dtype="f8",
                               chunks=(S_blk, L))

        # Weights as (C,P) — math in f32 for speed; sum in f64.
        x_cp = np.asarray(x_global)
        if x_cp.ndim == 1:
            x_cp = x_cp.reshape(C, P)
        if x_cp.shape != (C, P):
            raise ValueError(f"x shape {x_cp.shape} != (C,P)=({C},{P})")
        x32 = np.asarray(x_cp, dtype=np.float32, order="C")

        # Progress — write to stdout & flush so it shows immediately
        n_tiles = math.ceil(S / S_blk)
        header = (f"[Recon] S_chunk={S_chunk} C_chunk={C_chunk} "
                  f"P_chunk={P_chunk} L_chunk={L_chunk} S_blk={S_blk} "
                  f"L_band={L_band}")
        print(header, flush=True)

        pbar = tqdm(total=n_tiles,
                    desc="[Reconstruct] tiles",
                    unit="tile",
                    dynamic_ncols=True,
                    mininterval=2.0,
                    miniters=1,
                    smoothing=0.0,
                    file=sys.stdout,
                    leave=True)
        pbar.refresh(); sys.stdout.flush()

        # --- main loop: S in storage-aligned tiles -------------------
        for s0 in range(0, S, S_blk):
            s1 = min(S, s0 + S_blk)
            dS = s1 - s0

            # Tile accumulator in f64
            Y_tile = np.zeros((dS, L), dtype=np.float64, order="C")

            # Iterate components in storage order (C_chunk == 1)
            for c0 in range(0, C, max(1, C_chunk)):
                c1 = min(C, c0 + max(1, C_chunk))
                c = c0  # C_chunk is 1 in our files

                # Optional λ-banding for small GEMV views
                for l0 in range(0, L, max(1, L_band)):
                    l1  = min(L, l0 + max(1, L_band))
                    Lb  = l1 - l0

                    # Accumulator for this (c, L band)
                    band_acc = np.zeros((dS, Lb), dtype=np.float64, order="C")

                    # Contract over P in P_chunk steps
                    for p0 in range(0, P, max(1, P_chunk)):
                        p1  = min(P, p0 + max(1, P_chunk))
                        Pb  = p1 - p0

                        # Read one storage-aligned slab: (dS, 1, Pb, Lb) f32
                        A32 = M[s0:s1, c:c1, p0:p1, l0:l1][...]
                        A32 = np.asarray(A32, dtype=np.float32, order="C")

                        # Make a (dS*Lb, Pb) 2-D view for GEMV
                        A2D = A32[:, 0, :, :].swapaxes(1, 2).reshape(dS * Lb, Pb)

                        # Multiply by weights for this (c, p-block) in f32
                        w32 = x32[c, p0:p1]          # (Pb,)
                        tmp = A2D @ w32              # (dS*Lb,) f32

                        # Accumulate into f64 band
                        band_acc += tmp.reshape(dS, Lb).astype(np.float64, copy=False)

                    # Add this component's band into the tile
                    Y_tile[:, l0:l1] += band_acc

            # Write the finished S-tile
            out[s0:s1, :] = Y_tile

            # Make progress visible right now
            pbar.update(1)
            pbar.refresh()
            try:
                out.id.flush()    # SWMR-friendly: expose new tile
            except Exception:
                pass
            try:
                f.flush()
            except Exception:
                pass
            sys.stdout.flush()
            trim_heap()

        pbar.close()

# ------------------------------------------------------------------------------

def reconstruct_modelcube_fast(
    h5_path: str,
    x_cp: np.ndarray,
    out_dset: str = "/ModelCube",
    s_chunk: int | None = None,
    l_band: int | None = None,
    use_sparse: bool = True,
    out_dtype: str = "float64",
    rdcc_slots: int = 1_000_003,
    rdcc_bytes: int = 512 * 1024**2,
    rdcc_w0: float = 0.90,
) -> None:
    """
    Reconstruct /ModelCube = sum_{c,p} X[c,p] * models[s,c,p,λ] efficiently,
    using a single writer handle (read + write) to avoid HDF5 lock thrashing.

    Parameters
    ----------
    h5_path : str
        Path to the CubeFit HDF5 file.
    x_cp : ndarray
        Solution weights as shape (C, P) or flat (C*P,). These must be in the
        same physical normalization as `/HyperCube/models`.
    out_dset : str, optional
        Destination dataset name. Default is "/ModelCube".
    s_chunk : int or None, optional
        Spatial tile size. If None, uses models' S chunk on disk.
    l_band : int or None, optional
        Wavelength band size. If None, uses models' λ chunk on disk.
    use_sparse : bool, optional
        If True, skip zero populations per component. Default True.
    out_dtype : {"float64","float32"}, optional
        Output dtype (math is accumulated in float64). Default "float64".
    rdcc_slots, rdcc_bytes, rdcc_w0 : int, int, float
        HDF5 raw chunk cache knobs for faster streaming.

    Returns
    -------
    None

    Raises
    ------
    RuntimeError
        If required datasets are missing or incompatible.
    ValueError
        If `x_cp` cannot be reshaped to (C, P).

    Examples
    --------
    >>> reconstruct_modelcube_fast(h5_path, x_global)
    """
    # Keep tqdm visible on HPC logs
    try:
        sys.stdout.reconfigure(line_buffering=True)  # py3.7+
    except Exception:
        pass
    os.environ.setdefault("PYTHONUNBUFFERED", "1")

    # Helper: return free heap to OS (keeps RSS flat during long loops)
    try:
        libc = ctypes.CDLL("libc.so.6")
        def trim_heap(): libc.malloc_trim(0)
    except Exception:
        def trim_heap(): pass

    want_dtype = np.float64 if str(out_dtype) == "float64" else np.float32

    # Single writer handle: read models + write ModelCube with the same file
    with open_h5(h5_path, role="writer") as f:
        if "/HyperCube/models" not in f:
            raise RuntimeError("No /HyperCube/models; build the HyperCube first.")

        M = f["/HyperCube/models"]           # (S, C, P, L) f32 on disk
        try:
            M.id.set_chunk_cache(int(rdcc_slots), int(rdcc_bytes), float(rdcc_w0))
        except Exception:
            pass

        if M.ndim != 4:
            raise RuntimeError(f"Unexpected /HyperCube/models rank {M.ndim}")
        S, C, P, L = map(int, M.shape)

        # Choose tile sizes aligned to on-disk chunking unless overridden
        m_chunks = M.chunks or (S, 1, P, L)
        S_chunk_file, _, _, L_chunk_file = map(int, m_chunks)
        S_blk  = S_chunk_file if s_chunk is None else int(s_chunk)
        L_band = L_chunk_file if l_band is None else int(l_band)
        S_blk  = max(1, min(S_blk,  S))   # clamp
        L_band = max(1, min(L_band, L))   # clamp

        # Prepare /ModelCube with compatibility check (shape/dtype/chunks)
        ds = f.get(out_dset, None)
        if ds is not None:
            ok = (tuple(ds.shape) == (S, L) and str(ds.dtype) == str(want_dtype))
            ch = getattr(ds, "chunks", None)
            if ch is None or tuple(ch) != (S_blk, L_band):
                ok = False
            if not ok:
                del f[out_dset]
                ds = None
        if ds is None:
            ds = f.create_dataset(
                out_dset,
                shape=(S, L),
                dtype=want_dtype,
                chunks=(S_blk, L_band),
                compression=None,
                shuffle=False,
            )
        out = ds  # alias

        # Accept (C,P) or flat (C*P,)
        x_in = np.asarray(x_cp, np.float64).ravel(order="C")
        if x_in.size != C * P:
            raise ValueError(f"x_cp length {x_in.size} != C*P={C*P}")
        x_cp = np.ascontiguousarray(x_in.reshape(C, P), dtype=np.float64)

        # Optional sparse speedup: precompute nonzeros per component
        nz_per_c = None
        if use_sparse:
            nz_per_c = [np.flatnonzero(x_cp[c, :] != 0.0) for c in range(C)]

        # Progress
        n_tiles = math.ceil(S / S_blk) * math.ceil(L / L_band)
        pbar = tqdm(total=n_tiles, desc="[Reconstruct]", mininterval=2.0)
        pbar.refresh(); sys.stdout.flush()

        # Main loops: S tiles × λ bands; accumulate per component in f64
        for s0 in range(0, S, S_blk):
            s1 = min(S, s0 + S_blk)
            dS = s1 - s0

            # Reset accumulator for this S-tile (full λ span)
            Y_tile = np.zeros((dS, L), dtype=np.float64, order="C")

            for l0 in range(0, L, L_band):
                l1 = min(L, l0 + L_band)

                # Accumulate this λ band
                band = np.zeros((dS, l1 - l0), dtype=np.float64, order="C")

                for c in range(C):
                    if use_sparse:
                        nz = nz_per_c[c]
                        if nz.size == 0:
                            continue
                        w = x_cp[c, nz]  # (P_sub,)
                        A_c = np.asarray(
                            M[s0:s1, c, nz, l0:l1], dtype=np.float32, order="C"
                        )
                    else:
                        w = x_cp[c, :]    # (P,)
                        A_c = np.asarray(
                            M[s0:s1, c, :, l0:l1], dtype=np.float32, order="C"
                        )

                    # y += sum_p w[p] * A[:, p, :]
                    band += np.tensordot(
                        w.astype(np.float64, copy=False),
                        A_c.astype(np.float64, copy=False),
                        axes=(0, 1),
                    )

                # Drop the band into the tile buffer
                Y_tile[:, l0:l1] += band

                # Progress per band
                pbar.update(1)
                pbar.refresh()
                sys.stdout.flush()

            # Write the finished S-tile
            out[s0:s1, :] = Y_tile

            # Try to flush to disk eagerly for SWMR friendliness
            try:
                out.id.flush()
            except Exception:
                pass
            try:
                f.flush()
            except Exception:
                pass
            sys.stdout.flush()
            trim_heap()

        pbar.close()

# ------------------------------------------------------------------------------

def _x_digest(x) -> str:
    x64 = np.ascontiguousarray(x, dtype=np.float64)
    h   = hashlib.sha1()
    h.update(x64.tobytes())
    h.update(str(x64.shape).encode("utf-8"))
    return h.hexdigest()

def modelcube_status(h5_path: str, x_global=None, require_float64: bool = True,
    redraw: bool = False):
    """
    Returns (ok: bool, msg: str). ok=True means you can safely skip rebuild.
    Checks presence, shape, dtype; if 'x_digest' attr is present, also checks currency vs x_global.
    """
    if redraw:
        return (False, "Forced re-creation")
    with open_h5(h5_path, role="reader") as f:
        if "/ModelCube" not in f:
            return (False, "missing /ModelCube")

        ds = f["/ModelCube"]
        # Determine expected (S,L) from DataCube if possible; else fall back to HyperCube/models
        if "/DataCube" in f:
            S, L = map(int, f["/DataCube"].shape)
        elif "/HyperCube/models" in f:
            M = f["/HyperCube/models"]
            S, L = int(M.shape[0]), int(M.shape[-1])
        else:
            return (False, "cannot infer (S,L) — missing /DataCube and /HyperCube/models")

        if ds.shape != (S, L):
            return (False, f"wrong shape {ds.shape} != ({S},{L})")

        if require_float64 and ds.dtype != np.float64:
            return (False, f"dtype {ds.dtype} is not float64")

        # If present, verify mask length is consistent
        if "/Mask" in f:
            mask_len = int(f["/Mask"].shape[0])
            if mask_len != L:
                return (False, f"/Mask length {mask_len} != L={L}")

        # If the dataset has a digest, compare with current x (if provided)
        ds_digest = ds.attrs.get("x_digest", None)
        ds_xshape = tuple(ds.attrs.get("x_shape", ()))
        if (x_global is not None) and (ds_digest is not None):
            xshape = np.asarray(x_global).shape
            if ds_xshape and tuple(ds_xshape) != xshape:
                return (False, f"x_shape mismatch: file {tuple(ds_xshape)} vs current {xshape}")
            cur = _x_digest(x_global)
            if cur != ds_digest:
                return (False, "digest mismatch: /ModelCube built with different x")
            return (True, "present, shape/dtype ok, digest matches")

        # No digest to check; accept but note it’s unverified against x
        return (True, "present, shape/dtype ok (no digest to verify)")

# ------------------------------------------------------------------------------

def parallel_spectrum_plots(
    h5_or_path: str,
    chi2: np.ndarray,
    n: int,
    plot_dir: str,
    n_workers: int,
    tag: str,
    mask: np.ndarray | None = None,
):
    """
    Memory-safe plotting:
      - Reads only needed rows from /DataCube and /ModelCube.
      - Never touches /HyperCube/models.
      - Closes every figure immediately.
      - Small thread pool (I/O bound).

    Style:
      - Data in black, model in red (lw=0.8).
      - Residuals (data - model) as green diamonds at every pixel,
        vertically offset (same offset policy as before) so they don’t
        overlap the spectra.
      - A solid green line at the residual zero (i.e., the offset
        baseline), and thin dashed green lines at ±1σ (σ computed on
        masked residuals).
      - If /Mask exists (or 'mask' provided), masked regions are shaded
        with semi-transparent grey bands.
    """
    import os
    import pathlib as plp
    from concurrent.futures import ThreadPoolExecutor

    plp.Path(plot_dir).mkdir(parents=True, exist_ok=True)

    n = int(np.maximum(1, n))
    chi2 = np.asarray(chi2, dtype=np.float64)
    S = int(chi2.shape[0])

    # Pick indices (worst/best by chi^2)
    order_desc = np.argsort(-chi2)
    order_asc  = np.argsort( chi2)
    idx_worst  = order_desc[:n]
    idx_best   = order_asc[:n]
    picks      = np.unique(np.concatenate([idx_worst, idx_best])).astype(int)

    # Read only the selected rows + metadata
    with open_h5(str(h5_or_path), role="reader") as f:
        if "/ModelCube" not in f:
            raise RuntimeError("Expected /ModelCube (S,L) for plotting. Reconstruct first.")
        data_ds  = f["/DataCube"]    # (S,L)
        model_ds = f["/ModelCube"]   # (S,L)

        L = int(model_ds.shape[1])
        obs = f["/ObsPix"][...] if "/ObsPix" in f else np.arange(L, dtype=np.float64)

        # Prefer provided mask; else load /Mask; else keep-all
        if mask is None and "/Mask" in f:
            m = np.asarray(f["/Mask"][...], dtype=bool).ravel()
            mask = m if int(m.size) == L else None
        if mask is None:
            mask = np.ones(L, dtype=bool)
        else:
            mask = np.asarray(mask, dtype=bool)
            if int(mask.size) != L:
                raise ValueError(f"Mask length {mask.size} != L={L}")

        print(
            "[Plots] picks={} L={} mem≈{:.1f} MB for data+model rows"
            .format(int(picks.size), L, float(picks.size * L * 16.0 / 1e6))
        )

        data_sel  = np.empty((int(picks.size), L), dtype=np.float64)
        model_sel = np.empty((int(picks.size), L), dtype=np.float64)
        for j, s in enumerate(picks):
            data_sel[j, :]  = data_ds[int(s), :]
            model_sel[j, :] = model_ds[int(s), :]

    # Precompute masked bands as contiguous intervals where mask == False
    masked = ~mask
    if np.any(masked):
        pad = np.concatenate((
            np.array([0], dtype=np.int8),
            masked.view(np.int8),
            np.array([0], dtype=np.int8)
        ))
        edges = np.diff(pad)
        starts = np.nonzero(edges == 1)[0]
        ends   = np.nonzero(edges == -1)[0]
        mask_spans = list(zip(starts, ends))  # intervals [start, end)
    else:
        mask_spans = []

    # Small plotting worker: operates on compact row views
    def _plot_one(s_idx: int, rank_tag: str):
        j = int(np.where(picks == s_idx)[0][0])
        dat = data_sel[j, :]
        mod = model_sel[j, :]
        res = dat - mod

        # Keep the SAME residual offset policy as before
        y_lo = float(np.nanmin(np.concatenate((dat[mask], mod[mask]))))
        y_hi = float(np.nanmax(np.concatenate((dat[mask], mod[mask]))))
        y_rng = float(np.maximum(y_hi - y_lo, 1.0))
        y_off = y_lo - 0.25 * y_rng

        # σ from masked residuals
        sigma = float(np.nanstd(res[mask])) if np.any(mask) else 0.0

        fig = plt.figure(figsize=(8, 3.5))
        ax  = fig.add_subplot(111)

        # Data/model lines (thin)
        ax.plot(obs[mask], dat[mask], lw=0.8, color="k", label="data")
        ax.plot(obs[mask], mod[mask], lw=0.8, color="r", label="model")

        # Residuals as green diamonds at every pixel (offset)
        ax.scatter(
            obs[mask], (res[mask] + y_off),
            s=8, marker="D", edgecolors="none", color="g", alpha=0.9,
            label="residual (offset)"
        )

        # Residual baseline (solid) and ±1σ (dashed)
        ax.axhline(y_off,             ls="-",  lw=0.7, color="g")          # zero residual
        if sigma > 0.0 and np.isfinite(sigma):
            ax.axhline(y_off + sigma, ls="--", lw=0.6, color="g")
            ax.axhline(y_off - sigma, ls="--", lw=0.6, color="g")

        # Shade masked regions as semi-transparent grey bands
        if mask_spans:
            y0, y1 = ax.get_ylim()
            for a, b in mask_spans:
                x0 = float(obs[int(a)])
                x1 = float(obs[int(np.maximum(a, b - 1))])
                if int(b) < L:
                    x1 = float(obs[int(b)])
                ax.axvspan(x0, x1, color="0.2", alpha=0.12, zorder=0)
            ax.set_ylim(y0, y1)

        # ax.set_title(f"spaxel {int(s_idx)}  χ={chi2[int(s_idx)]:.3f}")
        ax.set_xlabel("log(\u03bb [\u212B])")
        ax.set_ylabel("$F_\lambda$ (arb. units)")
        # ax.legend(loc="best", fontsize=8)
        fig.savefig(os.path.join(
            plot_dir, f"{rank_tag}_{tag}_spax{int(s_idx):05d}.png"
        ), dpi=120)
        plt.close(fig)

    # Tiny pool; ≤4 for I/O friendliness (NumPy math, not builtins)
    pool_n = int(np.minimum(np.maximum(1, int(n_workers)), 4))
    jobs = [(int(s), "worst") for s in idx_worst] + \
           [(int(s), "best")  for s in idx_best]

    with ThreadPoolExecutor(max_workers=pool_n) as pool:
        list(pool.map(lambda args: _plot_one(*args), jobs))

    del data_sel, model_sel

# ------------------------------------------------------------------------------

def ceil_div(a,b): return (a + b - 1)//b
def round_down_to_multiple(x,m): return (x//m)*m

def choose_spat_tile_fast(S, n_workers, s_chunk, k=2):
    """
    Try for ~k*n_workers tiles. If chunk-aligning would give too few tiles,
    drop alignment to keep all cores busy.
    """
    target_tiles = max(1, k * n_workers)
    raw = max(1, S // target_tiles)          # integer floor

    # First try: chunk-aligned (round DOWN)
    tile = round_down_to_multiple(raw, s_chunk)
    if tile < s_chunk:
        tile = s_chunk
    n_tiles = ceil_div(S, tile)

    # If alignment leaves us with fewer tiles than workers, drop alignment
    if n_tiles < n_workers:
        tile = max(1, S // target_tiles)     # non-aligned raw
        n_tiles = ceil_div(S, tile)

    return tile, n_tiles

def _init_worker():
    import os
    os.environ["OMP_NUM_THREADS"] = "1"
    os.environ["MKL_NUM_THREADS"] = "1"
    os.environ["OPENBLAS_NUM_THREADS"] = "1"
    os.environ["NUMEXPR_MAX_THREADS"] = "1"
    try:
        # runtime guard in case threads were already initialized
        from threadpoolctl import threadpool_limits
        threadpool_limits(1)  # BLAS/OpenMP libraries → 1 thread
    except Exception:
        pass

# ------------------------------------------------------------------------------

def loadCubeFit(galaxy, mPath, decDir=None, nCuts=None, proj='i', SN=90,
    full=False, slope=1.30, IMF='KB', iso='pad', weighting='luminosity',
    nProcs=1, lOrder=4, rescale=False, specRange=None, lsf=False,
    band='r', smask=None, method='fsf', varIMF=False,
    source='ppxf', pplots=['sfh', 'spec', 'mw'], redraw=False, **kwargs):
    """
    Load the CubeFit data for a given galaxy and model path.
    """
    # Directories
    bDir = mDir/'tri_models'/mPath
    pDir = curdir.parent/'pxf'
    figDir = curdir/galaxy/'figures'
    MKDIRS = [bDir, pDir, figDir]
    [plp.Path(DIR).mkdir(parents=True, exist_ok=True) for DIR in MKDIRS]
    if isinstance(decDir, type(None)):
        with open(bDir/'decomp.dir', 'r+') as dd:
            decDir = dd.readline().strip()
    if isinstance(nCuts, type(None)):
        direc = list(filter(lambda xd: xd.is_dir(),
            (bDir/decDir).glob('decomp_*')))[0]
    else:
        direc = bDir/decDir/f"decomp_{nCuts:d}"
    if 'fif' in method:
        IMF = 'FIF'
        iso = 'fif'
    if not full:
        tEnd = 'trunc'
    else:
        tEnd = 'full'
    w8Str = f"{weighting[0].upper()}W"
    tag = f"_SN{int(SN):02d}_{iso}_{IMF}{slope:.2f}_{w8Str}"
 
    pfs = pDir/galaxy/f"pixels_SN{SN:02d}.xz"
    vbSpec = pDir/galaxy/f"voronoi_SN{SN:02d}_{tEnd}.xz"
    infn = bDir/'infil.xz'

    INF = uu.Load.lzma(infn)
    PA = INF['angle'][0]
    if vbSpec.is_file():
        VB = uu.Load.lzma(vbSpec)
        binNum = VB['binNum']
        binCounts = VB['binCounts']
        del VB
    else:
        raise RuntimeError(f"No binned spectra.\n{'': <4s}{vbSpec}")

    xpix, ypix, sele, pixs = uu.Load.lzma(pfs)
    # saur,goods = uu.Load.lzma(sfs)
    # del saur
    xbix, ybix = GEO.rotate2D(xpix, ypix, PA)
    pfn = dDir.parent/'muse'/'obsData'/f"{galaxy}-poly-rot.xz"
    polyProps = dict(ec=POT.brown, linestyle='--', fill=False, zorder=100,
        lw=0.75, salpha=0.5)
    if pfn.is_file():
        aShape = uu.Load.lzma(pfn)
        aShape, pPatch = POT.polyPatch(POLYGON=aShape, Xpo=xbix, Ypo=ybix,
            **polyProps)
    else:
        aShape, pPatch = POT.polyPatch(Xpo=xbix, Ypo=ybix, **polyProps)
        uu.Write.lzma(pfn, aShape)
    xmin, xmax = np.amin(xbix), np.amax(xbix)
    ymin, ymax = np.amin(ybix), np.amax(ybix)
    xLen, yLen = np.ptp(xbix), np.ptp(ybix) # unmasked pixels

    saur, goods = uu.Load.lzma(pDir/galaxy/f"selection_SN{SN:02d}_{tEnd}.xz")
    xpix = np.compress(goods, xpix)
    ypix = np.compress(goods, ypix)
    xbix = np.compress(goods, xbix)
    ybix = np.compress(goods, ybix)

    with logger.capture_all_output():
        decDir, cDirs, cKeys, nComp, teLL, lnGrid, histBinSize, dataVelScale,\
            RZ, spLL, laGrid, lmin, lmax, umetals, uages, ualphas, pixOff = \
            tf._oneTimeSpec(galaxy=galaxy, mPath=mPath, decDir=decDir,
            nCuts=nCuts, proj=proj, SN=SN, full=full, slope=slope, IMF=IMF,
            iso=iso, weighting=weighting, lOrder=lOrder, rescale=rescale,
            lsf=lsf, specRange=specRange, band=band, method=method,
            varIMF=varIMF, source=source, **kwargs)
    nLSpec, nSpat = laGrid.shape
    nTSpec, nMetals, nAges, nAlphas = lnGrid.shape
    nSSP = int(np.prod((nMetals, nAges, nAlphas), dtype=int))
    pred = f"0{len(repr(nComp)):d}"
    nComp = int(nComp)

    oDict = uu.Load.lzma(direc/f"decomp_{nCuts:d}.plt")
    if 'binFN' not in oDict.keys():
        oDict['binFN'] = 'bins_0.dat'
        oDict['apFN'] = 'aperture_0.dat'
        uu.Write.lzma(direc/f"decomp_{nCuts:d}.plt", oDict)
    binFN = oDict['binFN']
    apFN = oDict['apFN']
    dnPix, dgrid = uu.Read.bins(bDir/'infil'/binFN)
    dnbins = int(np.max(dgrid))
    dgrid -= 1
    dss = np.where(dgrid >= 0)[0]
    dx0, dx1, dnx, dy0, dy1, dny, dtheta = uu.Read.aperture(
        bDir/'infil'/apFN)
    ddx = np.abs((dx1-dx0)/dnx)
    ddy = np.abs((dy1-dy0)/dny)
    dpixs = np.min([ddx, ddy])
    dxr = np.arange(dnx)*dpixs + dx0 + 0.5*dpixs
    dyr = np.arange(dny)*dpixs + dy0 + 0.5*dpixs
    dxtss = uu._hash(dxr, np.full_like(dyr, 1)).ravel()[dss]
    dytss = uu._hash(np.full_like(dxr, 1), dyr).ravel()[dss]
    dtestX, dtestY = GEO.rotate2D(dxtss, dytss, dtheta)
    duPix, dpInverse, dpCounts = np.unique(dgrid[dss], return_inverse=True,
        return_counts=True)
    dpCount = dpCounts[dpInverse]

    biI = INF['bins'][0]
    bCount = biI['pCountsBin']
    # grid = np.array(biI['grid'], dtype=int).ravel()-1
    grid = np.array(biI['grid'], dtype=int).T.ravel()-1
    nbins = np.max(grid).astype(int)+1
    ss = np.where(grid >= 0)[0]

    if np.max(dpCount) > 1: # at least one bin contains more than one pixel
        # a quick way to check if the oberved scheme was used
        dgrid = grid
        dss = ss
        dnbins = nbins
        dpCount = bCount

    nzComp = np.array(oDict['nzComp'], dtype=int)
    nnOrb = plp.Path(*oDict['nnOrb'])
    oClass = plp.Path(*oDict['oClass'])
    obClass = plp.Path(*oDict['obClass'])
    bLKey = uu.keySep.join([nnOrb.parent.parent.name, nnOrb.parent.name])
    bLKey = uu.rReplace(bLKey, uu.keySep, os.sep, 1)
    nnOrb = plp.Path(bDir, decDir, nnOrb.parent.name, nnOrb.name)
    oClass = plp.Path(bDir, decDir, oClass.parent.name, oClass.name)
    obClass = plp.Path(bDir, decDir, obClass.parent.name, obClass.name)
    fpd = uu._deetExtr(bLKey)
    apDir = bDir/bLKey/'nn_aphist.out'
    maDir = (bDir/bLKey).parent/'datfil'/'mass_aper.dat'

    NOrbs, inds, energs, I2s, I3s, regs, types, weights, lcuts =\
        uu.Read.orbits(nnOrb)
    cWeights = np.zeros(nComp)
    for jk, comp in enumerate(nzComp):
        cWeights[jk] = np.ma.sum(weights[oDict['wheres'][f"{comp:{pred}d}"]])

    kiBin = INF['kin']['nbins'][0]
    assert nbins == kiBin, 'Output does not agree with input bins\nInput:'+\
        f"{kiBin}\nOutput: {nbins}"

    wbin, hN, histBinSize, hArr = uu.Read.apertureHist(apDir)
    logger.log(f"{'Mass outside of the histograms:': <45s}"\
          f"{np.sum(hArr[:, 0] + hArr[:, wbin * 2]):5.5}")

    fullBin, fullID, fullK0 = uu.Read.massAperture(maDir)
    logger.log(f"{'Mass normalisation is:': <45s}"\
        f"{np.sum(hArr) / np.sum(fullK0):5.5}")
    if isinstance(proj, list):
        pStr = ''.join([str(f) for f in proj])
    else:
        pStr = str(proj)
    plt.close('all')
    massNorm = fullK0

    apMassFile = direc/f"apMass_i{proj}_{nComp:{pred}d}.xz"
    if apMassFile.is_file():
        aperMass = uu.Load.lzma(apMassFile)
    else:
        aperMass = np.ma.ones((nSpat, nComp), dtype=float)*np.nan
        ERR = []
        for cn, cDir in tqdm(enumerate(cDirs), desc='Mass', total=nComp):
            try:
                maFile = cDir/'declib_apermass.out'
                nbin, ID, k0 = uu.Read.massAperture(maFile)
                aperMass[:, cn] = k0
            except:
                ERR += [cDir.name]
        if len(ERR) > 0:
            logger.log(ERR)
            breakpoint()
        uu.Write.lzma(apMassFile, aperMass)
    aperMass = np.ma.masked_invalid(aperMass)
    norma = np.sum(aperMass, axis=1)

    logger.log('Done.', flush=True)
    apFile = cDirs[0]/'declib_aphist.out'
    wbin, hN, histBinSize, hArr = uu.Read.apertureHist(apFile)
    # Load the parameters regardless
    apHistFile = direc/f"apHists_i{pStr}_{nComp:{pred}d}.jl"
    if apHistFile.is_file():
        logger.log('Reading histograms...', flush=True)
        apHists = uu.Load.jobl(apHistFile)
    else:
        apFile = cDirs[0]/'declib_aphist.out'
        wbin, hN, histBinSize, cArr = uu.Read.apertureHist(apFile)
        logger.log('Generating histograms...', flush=True)
        apHists = np.ma.ones((*cArr.shape, nComp))*np.nan
        ERR = []
        for cn, cDir in tqdm(enumerate(cDirs), desc='Components',
            total=nComp):
            try:
                apFile = cDir/'declib_aphist.out'
                wbin, hN, histBinSize, cArr = uu.Read.apertureHist(
                    apFile)
                apHists[:, :, cn] = cArr
            except:
                ERR += [cDir.stem]
        if len(ERR) > 0:
            logger.log(ERR)
            pdb.set_trace()
        uu.Write.jobw(apHistFile, apHists)
    logger.log('Done.')
    apHists = np.ma.masked_invalid(apHists)
    nApHists = (apHists*(massNorm/norma)[:, np.newaxis, np.newaxis])

    # --- Setup HDF5 directory ---
    hdf5Dir = plp.Path(kwargs.pop('hdf5Dir', curdir/galaxy))
    hdf5Dir.mkdir(parents=True, exist_ok=True)
    hdf5Path = (hdf5Dir/f"{galaxy}_{nComp}_{lOrder:02d}").with_suffix('.h5')
    
    # Read dims & X_global using robust reader
    with open_h5(hdf5Path, role="reader") as f:
        if "/X_global" not in f:
            raise RuntimeError("No /X_global found — run the fit first.")
        x_global = f["/X_global"][...]

        if "/HyperCube/models" not in f:
            raise RuntimeError("No /HyperCube/models found — build the HyperCube first.")
        models = f["/HyperCube/models"]
        models_chunks = models.chunks  # may be None
        if models.ndim == 4:
            nSpat, nComp, nPop, nLSpec = map(int, models.shape)
            s_chunk = (models_chunks[0] if models_chunks is not None else 32)
        elif models.ndim == 5:
            nB, B, nComp, nPop, nLSpec = map(int, models.shape)
            nSpat = nB * B
            s_chunk = B
        else:
            raise RuntimeError(f"Unexpected /HyperCube/models rank {models.ndim}")

        # optional input data for plots
        has_mask = ("/Mask" in f)
        mask_arr = f["/Mask"][...] if has_mask else None
        obs = f["/ObsPix"][...] if "/ObsPix" in f else np.arange(nLSpec)

    spat_tile, nTiles = choose_spat_tile_fast(nSpat, nProcs, s_chunk, k=2)
    nProcs = min(nProcs, nTiles, 12)  # don’t spawn more processes than tiles

    ok, why = modelcube_status(str(hdf5Path), x_global=x_global, require_float64=True, redraw=redraw)
    logger.log(f"[ModelCube] status: {why}")
    if ok:
        logger.log("[ModelCube] Skipping reconstruction.")
    else:
        logger.log("[ModelCube] Reconstructing…")
        # reconstruct_model_cube_single(  # or your parallel version
        reconstruct_modelcube_fast(
            h5_path=str(hdf5Path),
            x_cp=x_global,
            # array_name="ModelCube",
            # blas_threads=nProcs,
        )
        # Stamp digest so future runs can skip confidently
        try:
            with open_h5(str(hdf5Path), role="writer") as f:
                xdig = _x_digest(x_global)
                ds = f["/ModelCube"]
                ds.attrs["x_digest"] = xdig
                ds.attrs["x_shape"]  = np.asarray(x_global).shape
                ds.attrs["dtype_math"] = "float64"
                ds.attrs["generator"] = "reconstruct_modelcube_fast"
        except Exception as e:
            logger.log(f"[ModelCube] Warning: could not stamp digest ({e})")

    with open_h5(hdf5Path, role="reader") as f:
        data_cube = f["/DataCube"][...]
        model_cube = f["/ModelCube"][...]  # (nSpat, nLSpec)

    # chi^2 per spaxel
    if mask_arr is None:
        mask_arr = np.ones(nLSpec, dtype=bool)
    resid = (data_cube - model_cube)[:, mask_arr]
    rchi2 = np.sqrt((resid * resid).mean(axis=1))

    plt.figure(figsize=(6, 4))
    plt.hist(rchi2, bins=40, alpha=0.7)
    plt.xlabel(r"${\rm Norm}/\sqrt{N_{\rm pix}}$")
    plt.ylabel("Number of apertures")
    plt.title("Distribution of fit quality")
    plt.savefig(figDir/"chi2_hist.png")
    plt.close()

    if 'spec' in pplots:
        logger.log("Generating spectrum plots...")
        with logger.capture_all_output():
            parallel_spectrum_plots(
                h5_or_path=str(hdf5Path),
                chi2=rchi2,
                n=10,
                plot_dir=str(figDir),
                n_workers=min(12, max(1, nProcs)),
                tag=f"C{nComp:04d}",
                mask=mask_arr,
            )

    if 'mw' in pplots:
        data_cube = np.ma.masked_less_equal(data_cube, 0.0)
        model_cube = np.ma.masked_less_equal(model_cube, 0.0)
        flux = np.ma.masked_invalid(
            # (np.ma.sum(laGrid, axis=0)/binCounts)[binNum], 0.))
            (np.ma.sum(data_cube[:, mask_arr], axis=1)/binCounts))
        modSB = np.ma.masked_array( # re-scale to original data levels
            (np.ma.sum(model_cube[:, mask_arr], axis=1)/binCounts),
            # (np.ma.sum(model_cube, axis=0)*laScales/binCounts)[binNum],
            mask=np.ma.getmaskarray(flux))
        fmin, fmax = np.log10(np.ma.min(flux)), np.log10(np.ma.max(flux))
        pren = 2
        miText = POT.prec(pren, fmin)
        maText = POT.prec(pren, fmax)
        gs = gridspec.GridSpec(3, 1, hspace=0., wspace=0.)
        fig = plt.figure(figsize=plt.figaspect((yLen*3.)/xLen)*0.75)
        ax = fig.add_subplot(gs[0])
        cnt = dbi(xpix, ypix, np.log10(flux[binNum]), pixelsize=pixs, angle=PA,
            cmap='gist_heat', vmin=fmin, vmax=fmax)
        ax.set_xticklabels([])
        ax.set_xlim(xmin, xmax)
        ax.set_ylim(ymin, ymax)
        # ax.add_patch(copy(pPatch))
        cax = POT.attachAxis(ax, 'top', 0.1)
        cb = plt.colorbar(cnt, cax=cax, orientation='horizontal')
        lT = cax.text(0.5, 0.5, fr"$L\ [{UTS.lsun}]$", va='center', ha='center',
            color=POT.pgreen, transform=cax.transAxes)
        lT.set_path_effects([PathEffects.withStroke(linewidth=1.5,
            foreground='k')])
        cax.text(1e-3, 0.5, miText, va='center', ha='left', color='white',
            transform=cax.transAxes)
        cax.text(1.0-1e-3, 0.5, maText, va='center', ha='right', color='black',
            transform=cax.transAxes)
        cb.set_ticks([])
        ax = fig.add_subplot(gs[1])
        dbi(xpix, ypix, np.log10(modSB[binNum]), pixelsize=pixs, angle=PA,
            cmap='cet_fire', vmin=fmin, vmax=fmax)
        ax.set_xticklabels([])
        ax.set_xlim(xmin, xmax)
        ax.set_ylim(ymin, ymax)
        # ax.add_patch(copy(pPatch))

        delta = (flux-modSB)/flux
        ax = fig.add_subplot(gs[2])
        cnt = dbi(xpix, ypix, delta[binNum], pixelsize=pixs, angle=PA,
            cmap=divcmap, vmin=-0.1, vmax=0.1)
        cax = POT.attachAxis(ax, 'top', 0.1, mid=True)
        cb = plt.colorbar(cnt, cax=cax, orientation='horizontal')
        lT = cax.text(0.5, 0.5, r'$(D-M)/D$', va='center', ha='center',
            color=POT.pgreen, transform=cax.transAxes)
        lT.set_path_effects([PathEffects.withStroke(linewidth=1.5,
            foreground='k')])
        cax.text(1e-3, 0.5, '-0.1', va='center', ha='left', color='white',
            transform=cax.transAxes)
        cax.text(1.0-1e-3, 0.5, '0.1', va='center', ha='right', color='white',
            transform=cax.transAxes)
        cb.set_ticks([])
        ax.set_xlim(xmin, xmax)
        ax.set_ylim(ymin, ymax)
        # ax.add_patch(copy(pPatch))

        BIG = fig.add_subplot(gs[:])
        BIG.set_frame_on(False)
        BIG.set_xticks([])
        BIG.set_yticks([])
        BIG.set_xlabel(r'$x\ [{\rm arcsec}]$', labelpad=25)
        BIG.set_ylabel(r'$y\ [{\rm arcsec}]$', labelpad=25)

        plt.savefig(figDir/\
            f"modelCube_{nComp:{pred}d}_i{proj}{tag}_{lOrder:02d}.png")

    if 'otype' not in oDict['cutOn']:
        return # only do orbital SFH if orbital decomposition
    if len(oDict['cuts'])>0:
        # determine which components belong to which orbital categories
        allCuts = np.array([oDict['cuts'][key] for key in oDict['cuts'].keys()])
        uCuts, uCounts = np.unique(allCuts, axis=0, return_counts=True)
        # assert that every elemnt of uCounts is equal
        assert np.unique(uCounts).size == 1
        notypes = np.max(uCounts)
        obins = np.arange(1, notypes+1) * uCuts.shape[0]
        otypes = np.digitize(nzComp, bins=obins, right=True)
    else:
        otypes = copy(nzComp)

    if 'sfh' in pplots:
        satube = (otypes == 0) # group short-axis tubes
        latube = (otypes == 1)
        boxess = (otypes == 2)
        arSOL = x_global.reshape(nComp, nMetals, nAges, nAlphas, order='C')
        coSFH = arSOL[satube, :, :, :].sum(axis=0)
        laSFH = arSOL[latube, :, :, :].sum(axis=0)
        boSFH = arSOL[boxess, :, :, :].sum(axis=0)

        minT, maxT = np.min(uages), np.max(uages)
        minZ, maxZ = np.min(umetals), np.max(umetals)

        wmax = np.max(np.log10(np.array([coSFH, laSFH, boSFH])))
        sfhMin = np.min(np.log10((
            np.min(coSFH[coSFH>0]),
            np.min(laSFH[laSFH>0]),
            np.min(boSFH[boSFH>0]))))
        wmin = np.max((sfhMin, -12))
        print(f"SFH plot limits: {wmin:.2f} ({sfhMin:.2f}) to {wmax:.2f}")

        fig = plt.figure(figsize=plt.figaspect(3./4.))
        gs = gridspec.GridSpec(3, nAlphas, hspace=0., wspace=0.)
        # one column per alpha, 3 orbit types
        print(nAlphas, ualphas)
        for ali in range(nAlphas):
            ax = fig.add_subplot(gs[0, ali])
            cnt = ax.imshow(np.log10(coSFH[:, :, ali]),
                extent=[minT, maxT, minZ, maxZ],
                aspect='auto', interpolation='none', origin='lower',
                cmap=moncmapr, norm=Normalize(vmin=wmin, vmax=wmax))
            if not ax.get_subplotspec().is_last_row():
                ax.set_xticklabels([])
            if not ax.get_subplotspec().is_first_col():
                ax.set_yticklabels([])
            if ax.get_subplotspec().is_first_col():
                lT = ax.text(1e-2, 1e-2, r'$z$ Tubes', va='bottom', ha='left',
                    color=POT.pgreen, transform=ax.transAxes)
                lT.set_path_effects([PathEffects.withStroke(linewidth=1.5,
                    foreground='k')])
            if nAlphas > 1:
                lT = ax.text(0.5, 1.05, rf"$[\alpha/Fe]={ualphas[ali]:.2f}$",
                    va='bottom', ha='center', color=POT.pgreen,
                    transform=ax.transAxes)
                lT.set_path_effects(
                    [PathEffects.withStroke(linewidth=1.5, foreground='k')])
            ax = fig.add_subplot(gs[1, ali])
            ax.imshow(np.log10(laSFH[:, :, ali]),
                extent=[minT, maxT, minZ, maxZ],
                aspect='auto', interpolation='none', origin='lower',
                cmap=moncmapr, norm=Normalize(vmin=wmin, vmax=wmax))
            if not ax.get_subplotspec().is_last_row():
                ax.set_xticklabels([])
            if not ax.get_subplotspec().is_first_col():
                ax.set_yticklabels([])
            if ax.get_subplotspec().is_first_col():
                lT = ax.text(1e-2, 1e-2, r'$x$ Tubes', va='bottom', ha='left',
                    color=POT.pgreen, transform=ax.transAxes)
                lT.set_path_effects([PathEffects.withStroke(linewidth=1.5,
                    foreground='k')])
            ax = fig.add_subplot(gs[2, ali])
            ax.imshow(np.log10(boSFH[:, :, ali]),
                extent=[minT, maxT, minZ, maxZ],
                aspect='auto', interpolation='none', origin='lower',
                cmap=moncmapr, norm=Normalize(vmin=wmin, vmax=wmax))
            if not ax.get_subplotspec().is_last_row():
                ax.set_xticklabels([])
            if not ax.get_subplotspec().is_first_col():
                ax.set_yticklabels([])
            if ax.get_subplotspec().is_first_col():
                lT = ax.text(1e-2, 1e-2, r'Box', va='bottom', ha='left',
                    color=POT.pgreen, transform=ax.transAxes)
                lT.set_path_effects([PathEffects.withStroke(linewidth=1.5,
                    foreground='k')])

        BIG = fig.add_subplot(gs[:])
        BIG.set_frame_on(False)
        BIG.set_xticks([])
        BIG.set_yticks([])
        BIG.set_xlabel(r'$t\ [{\rm Gyr}]$', labelpad=20)
        BIG.set_ylabel(r'$[Z/H]$', labelpad=35)
        cax = POT.attachAxis(BIG, 'right', 0.05)
        cb = plt.colorbar(cnt, cax=cax, orientation='vertical')
        lT = cax.text(0.5, 0.5, r'$\log_{10}($Mass Weight$)$',
            va='center', ha='center', color=POT.pgreen,
            transform=cax.transAxes, rotation=270)
        lT.set_path_effects([PathEffects.withStroke(linewidth=1.5,
            foreground='k')])
        cax.text(0.45, 1.0-1e-3, f"{wmax:.2f}", va='top', ha='center',
            color='w', transform=cax.transAxes, rotation=270)
        cax.text(0.45, 1e-3, f"{wmin:.2f}", va='bottom', ha='center',
            color='k', transform=cax.transAxes, rotation=270)
        cb.set_ticks([])
        plt.savefig(figDir/\
            f"orbitSFH_{nComp:{pred}d}_i{proj}{tag}_{lOrder:02d}.png")

    # 7. Print summary
    print(f"Mean reduced χ²: {np.mean(rchi2):.2f} ± {np.std(rchi2):.2f}")
    worst = np.argmax(rchi2)
    best = np.argmin(rchi2)
    print(f"Worst fit: aperture {worst} (χ² = {rchi2[worst]:.2f})")
    print(f"Best fit:  aperture {best} (χ² = {rchi2[best]:.2f})")
    print(f"[CubeFit] All plots and maps saved in {str(figDir)}")

# ------------------------------------------------------------------------------

def plot_sparse_spectra_from_x(
    h5_or_path: str,
    x_global: np.ndarray | None = None,
    *,
    picks: np.ndarray | list[int] | None = None,
    chi2: np.ndarray | None = None,
    n: int = 6,
    plot_dir: str = ".",
    tag: str = "",
    mask: np.ndarray | None = None,
):
    """
    Plot a few diagnostic spectra without building /ModelCube.
    Computes y_hat for selected spaxels directly from /HyperCube/models and x_global.

    Args
    ----
    h5_or_path : str
        Path to HDF5 with /HyperCube/models and /DataCube.
    x_global : array-like
        Global weights (C*P,) or (C,P). Internally cast to float32.
    picks : array-like of int, optional
        Explicit spaxel indices to plot. If None, use `chi2` & `n`.
    chi2 : array-like, optional
        Per-spaxel RMSE/chi2 to pick best/worst examples from.
    n : int
        If using `chi2`, number of best and worst to show (unique combined).
    plot_dir : str
        Where to save PNGs.
    tag : str
        Small tag to include in filenames.
    mask : 1-D bool array, optional
        Wavelength mask to apply to both data & model for plotting.
    """
    os.makedirs(plot_dir, exist_ok=True)

    with open_h5(h5_or_path, role="reader") as f:
        M = f["/HyperCube/models"]      # (S, C, P, L) float32
        DC = f["/DataCube"]             # (S, L)
        S, C, P, L = map(int, M.shape)
        obs = f["/ObsPix"][...] if "/ObsPix" in f else np.arange(L, dtype=int)

        # Load x_global if not provided
        if x_global is None:
            if "/X_global" not in f:
                raise RuntimeError("x_global not provided and /X_global not found in file.")
            x_global = np.asarray(f["/X_global"][...], dtype=np.float64)

        # Choose picks if not explicitly provided
        if picks is None:
            if chi2 is None:
                raise ValueError("Provide `picks` or (`chi2` and `n`).")
            chi2 = np.asarray(chi2, dtype=np.float64)
            if chi2.shape[0] != S:
                raise ValueError(f"chi2 length {chi2.shape[0]} != S={S}.")
            worst = np.argsort(-chi2)[:int(max(1, n))]
            best  = np.argsort( chi2)[:int(max(1, n))]
            picks = np.unique(np.concatenate([worst, best])).astype(int)
        else:
            picks = np.asarray(picks, dtype=int)

        # Mask sanity
        if mask is None:
            mask = np.ones(L, dtype=bool)
        else:
            mask = np.asarray(mask, dtype=bool)
            if mask.shape[0] != L:
                raise ValueError(f"Mask length {mask.shape[0]} != L={L}.")

        # Weights as (C,P) float32 for speed; accumulation stays float64
        x_cp = np.asarray(x_global)
        if x_cp.ndim == 1:
            if x_cp.size != C * P:
                raise ValueError(f"x_global length {x_cp.size} != C*P={C*P}.")
            x_cp = x_cp.reshape(C, P)
        elif x_cp.shape != (C, P):
            raise ValueError(f"x_global shape {x_cp.shape} != (C,P)=({C},{P}).")
        x32 = np.asarray(x_cp, dtype=np.float32, order="C")

        # Respect storage layout to keep I/O small
        chunks = M.chunks or (min(S, 32), 1, min(P, 256), L)
        S_chunk, C_chunk, P_chunk, L_chunk = map(int, chunks)

        print(f"[DiagSparse] S={S} C={C} P={P} L={L} | chunks={chunks}")
        print(f"[DiagSparse] picks={picks.size} → reads per pick ≈ C·ceil(P/P_chunk)={C*math.ceil(P/max(1,P_chunk))}")

        def _predict_row(s_idx: int) -> np.ndarray:
            y = np.zeros(L, dtype=np.float64, order="C")
            for c0 in range(0, C, max(1, C_chunk)):
                c1 = min(C, c0 + max(1, C_chunk))
                c = c0  # C_chunk==1 in our files
                for p0 in range(0, P, max(1, P_chunk)):
                    p1  = min(P, p0 + max(1, P_chunk))
                    A32 = M[s_idx:s_idx+1, c:c1, p0:p1, :][...].astype(np.float32, copy=False)
                    A2D = A32[0, 0, :, :]          # (Pb, L)
                    w32 = x32[c, p0:p1]            # (Pb,)
                    y  += (A2D.T @ w32).astype(np.float64, copy=False)
            return y

        for s in tqdm(picks, desc="[DiagSparse] spaxels", dynamic_ncols=True, mininterval=1.5):
            s = int(s)
            data  = np.asarray(DC[s, :], dtype=np.float64, order="C")
            model = _predict_row(s)

            fig = plt.figure(figsize=(8, 3.5))
            ax  = fig.add_subplot(111)
            ax.plot(obs[mask], data[mask],  lw=1.0, label="data")
            ax.plot(obs[mask], model[mask], lw=1.0, alpha=0.9, label="model (sparse)")
            ax.set_title(f"spaxel {s}")
            ax.set_xlabel("λ (log space)")
            ax.set_ylabel("flux")
            ax.legend(loc="best", fontsize=8)
            fn = os.path.join(plot_dir, f"diag_sparse_{tag}_spax{int(s):05d}.png")
            fig.savefig(fn, dpi=120)
            plt.close(fig)

        print(f"[DiagSparse] wrote {picks.size} plots to {plot_dir}")

# ------------------------------------------------------------------------------

def compare_orbit_vs_solution(
    h5_path: str,
    *,
    orbit_weights: np.ndarray | None = None,   # shape (C,), raw or normalized
    x_global: np.ndarray | None = None,        # shape (C*P,) or (C,P)
    title: str | None = None,
    save: str | None = None,
    show: bool = True,
):
    """
    Visualize how the learned per-component mass (sum over P) compares to the
    input orbit_weights used by the ratio penalty.
    """
    # --- read C,P and x_global if needed
    with open_h5(h5_path, role="reader") as f:
        M = f["/HyperCube/models"]
        S, C, P, L = map(int, M.shape)
        if x_global is None:
            if "/X_global" not in f:
                raise RuntimeError("No /X_global in HDF5 and x_global not provided.")
            x_global = np.asarray(f["/X_global"][...], dtype=np.float64)

    x = np.asarray(x_global, dtype=np.float64)
    if x.ndim == 1:
        if x.size != C*P:
            raise ValueError(f"x_global has length {x.size}, expected C*P={C*P}.")
        x = x.reshape(C, P)  # (C,P)
    elif x.ndim == 2:
        if x.shape != (C, P):
            raise ValueError(f"x_global shape {x.shape}, expected (C,P)=({C},{P}).")
    else:
        raise ValueError("x_global must be 1-D or 2-D")

    # --- per-component totals and normalization
    eps = 1e-18
    sol_tot = np.maximum(0.0, x).sum(axis=1)       # (C,)
    sol_sum = float(sol_tot.sum()) or 1.0
    sol_pdf = (sol_tot / sol_sum)

    # ------------------- ratio penalty setup (simple) -------------------
    have_ratio = False
    w_full = None  # per-component step scaling

    # If not passed explicitly, read from HDF5 only (no other fallbacks)
    if orbit_weights is None:
        ow_dset = os.environ.get("CUBEFIT_ORBIT_WEIGHTS_DSET",
                                 "/HyperCube/norm/orbit_weights")
        with open_h5(h5_path, role="reader") as f:
            if ow_dset not in f:
                raise RuntimeError(f"orbit_weights requested but dataset "
                                   f"'{ow_dset}' not found in {h5_path}")
            w_in = np.asarray(f[ow_dset][...], dtype=np.float64).ravel(order="C")
    else:
        w_in = np.asarray(orbit_weights, dtype=np.float64).ravel(order="C")

    if w_in.size != C:
        raise ValueError(f"orbit_weights length {w_in.size} != C={C}")
    if not np.all(np.isfinite(w_in)):
        raise ValueError("orbit_weights contains non-finite values")
    w_sum = float(np.sum(w_in))
    if w_sum <= 0.0:
        raise ValueError("orbit_weights sum must be > 0")

    # Normalized component prior (probabilities)
    w_c = (w_in / w_sum).astype(np.float64, copy=False)

    # Enable penalty and keep your existing knobs
    have_ratio   = True
    _ratio_eta   = 0.02
    _ratio_prob  = 0.02
    _ratio_batch = 2
    _ratio_minw  = 1e-4
    rng = np.random.default_rng()

    # Per-component step scaling (mean -> 1.0), same semantics you had
    m = float(np.mean(w_c)) or 1.0
    w_full = (w_c / m).astype(np.float64, copy=False)

    def _ratio_update_in_place(x_mat: np.ndarray) -> None:
        s = x_mat.sum(axis=1)
        eps = 1e-12
        active = (w_c >= _ratio_minw) | (s > 0)
        if not np.any(active):
            return
        a = int(np.argmax(w_c * active))
        sa = float(np.max((s[a], eps)))
        wa = float(np.max((w_c[a], eps)))
        pool = np.flatnonzero(active & (np.arange(C) != a))
        if pool.size == 0:
            return
        sel = pool[rng.choice(pool.size, size=np.min((_ratio_batch, pool.size)), replace=False)]
        if sel.size > 1 and _ratio_prob < 1.0:
            keep_mask = rng.random(sel.size) < _ratio_prob
            sel = sel[keep_mask]
            if sel.size == 0:
                return
        for c in sel:
            sc = float(np.max((s[c], eps)))
            rc = float(np.max((w_c[c], eps)))
            e = math.log((sc/sa) / (rc/wa))
            if e == 0.0:
                continue
            delta_sc = -_ratio_eta * e * sc
            delta_sa = +_ratio_eta * e * sa
            pc = x_mat[c, :] / sc
            pa = x_mat[a, :] / sa
            if not np.all(np.isfinite(pc)):
                pc = np.full(P, 1.0 / P, dtype=np.float64)
            if not np.all(np.isfinite(pa)):
                pa = np.full(P, 1.0 / P, dtype=np.float64)
            x_mat[c, :] += delta_sc * pc
            x_mat[a, :] += delta_sa * pa
        if cfg.project_nonneg:
            np.maximum(x_mat, 0.0, out=x_mat)


    # --- diagnostics
    l1 = float(np.sum(np.abs(w_in - sol_pdf)))
    cos = float(np.dot(w_in, sol_pdf) / (np.linalg.norm(w_in) * np.linalg.norm(sol_pdf) + eps))
    # KLs with epsilon-smoothing
    p = np.clip(w_in, eps, 1.0); p /= p.sum()
    q = np.clip(sol_pdf, eps, 1.0); q /= q.sum()
    kl_pq = float(np.sum(p * (np.log(p) - np.log(q))))
    kl_qp = float(np.sum(q * (np.log(q) - np.log(p))))
    print(f"[ratio vs solution] C={C}, P={P}")
    print(f"  L1 distance        : {l1:.4f}")
    print(f"  Cosine similarity  : {cos:.6f}")
    print(f"  KL(p||q), KL(q||p) : {kl_pq:.6f}, {kl_qp:.6f}")

    # --- plotting
    idx = np.arange(C)
    width = 0.45

    fig = plt.figure(figsize=(10, 4.2))
    ax1 = fig.add_subplot(1,2,1)
    ax1.bar(idx - width/2, w_in,  width, label="input prior (orbit_weights)")
    ax1.bar(idx + width/2, sol_pdf, width, label="solution ∑_P x[c,p]")
    ax1.set_xlabel("component c")
    ax1.set_ylabel("normalized mass / probability")
    ttl = title or "Component weights: prior vs solution"
    ax1.set_title(ttl)
    ax1.legend(frameon=False, fontsize=9)

    ax2 = fig.add_subplot(1,2,2)
    ax2.scatter(w_in, sol_pdf, s=14)
    lim = (0.0, max(1.0/C*5, float(max(w_in.max(), sol_pdf.max()))*1.05))
    ax2.plot(lim, lim, lw=1.0)
    ax2.set_xlim(lim); ax2.set_ylim(lim)
    ax2.set_xlabel("input prior w_in[c]")
    ax2.set_ylabel("solution mass fraction")
    ax2.set_title(f"scatter vs y=x  (cos={cos:.3f}, L1={l1:.3f})")

    if save:
        fig.savefig(save, dpi=140)
        print(f"[saved] {save}")
    if show:
        plt.show()
    else:
        plt.close(fig)

    # return raw arrays if you want to post-process
    return dict(
        prior=w_in, solution=sol_pdf, sol_tot=sol_tot,
        L1=l1, cosine=cos, KL_pq=kl_pq, KL_qp=kl_qp
    )
</file>

<file path="kaczmarz_solver_cchunk_mp_nnls.py">
# -*- coding: utf-8 -*-
r"""
    kaczmarz_solver_cchunk_mp_nnls.py
    Adriano Poci
    University of Oxford
    2025

    Platforms
    ---------
    Unix, Windows

    Synopsis
    --------
    Modestly-MP, chunk-friendly solver aligned to /HyperCube/models chunking.
    `x` is 1-D (length C*P) in/out. Warm start handled by caller; we accept `x0`.
    `orbit_weights` is accepted (optional); free fit if None

    Authors
    -------
    Adriano Poci <adriano.poci@physics.ox.ac.uk>

History
-------
v1.0:   Fixed bug in computing `nprocs`;
        Wrapped entire `solve_global_kaczmarz_cchunk_mp` in `try/except`. 4
            December 2025
v1.1:   Added column-flux scaling bypass (`cp_flux_ref=None`). 5 December 2025
v1.2:   Experimenting with RMSE cap. 11 December 2025
v1.3:   Introduced L2 into Kaczmarz solving to be consistent with NNLS
            initilisation;
        Disabled buggy `w_band` which was implemented incorrectly. 12 December
            2025
v1.4:   Use the seed vector as a numerical prior during the Kaczmarz solving;
        Implement global RMSE evaluation, and keep only globally-best solution;
        Optionally disable seed prior. 13 December 2025
v1.5:   Use RMSE proxy as guard for epoch solution. 15 December 2025
v2.0:   Implemented global Kaczmarz gradient instead of per-tile updates; added
            `_worker_tile_global_grad_band` and
            `solve_global_kaczmarz_global_step_mp`. 16 December 2025
v2.1:   Added tiny-column freeze inside `_worker_tile_global_grad_band`. 18
            December 2025
v2.2:   Consolidated two tiny-column freeze env var names into one;
        Added fairer max-tile rather than bias to brighter spaxels. 25 December
            2025
v2.3:   Stripped global Kaczmarz solver diagnostics to single gradient and NNLS
            constraint. 26 December 2025
v2.4:   Pre-check gradient before each epoch;
        Pre-check RMSE proxy before each epoch to allow for early exit. 28
            December 2025
v2.5:   Implemented backtracking and step-size reduction based on RMSE proxy. 29
            December 2025
v2.6:   Replaced expensive backtracking RMSE evaluations with O(1) quadratic
            coefficients. 30 December 2025
v3.0:   Switched to diagonal-preconditioned Spectral Projected Gradient method
            to replace Kaczmarz updates. 31 December 2025
"""

from __future__ import annotations, print_function

import os, sys, traceback
import math
import time
from dataclasses import dataclass
from typing import Iterable, Tuple, Optional, List
from contextlib import contextmanager

import multiprocessing as mp
import numpy as np
import h5py
from tqdm.auto import tqdm

from CubeFit.hdf5_manager import open_h5
from CubeFit.hypercube_builder import read_global_column_energy
from CubeFit import cube_utils as cu

# ----------------------------- Config ---------------------------------

@dataclass
class MPConfig:
    epochs: int = 1
    lr: float = 0.25
    project_nonneg: bool = True
    processes: int = 2              # modest parallelism (2–4 recommended)
    blas_threads: int = 8           # per-process BLAS threads
    apply_mask: bool = True
    # HDF5 *dataset* chunk cache (not RDCC): keep local & harmless
    dset_slots: int = 1_000_003
    dset_bytes: int = 256 * 1024**2
    dset_w0: float = 0.90
    s_tile_override: Optional[int] = None
    pixels_per_aperture: int = 256
    max_tiles: Optional[int] = None

# ------------------------- Norm mode guard ----------------------------

def _assert_norm_mode(h5_path: str, expect: Optional[str] = None) -> str:
    r"""
    Read /HyperCube/norm.mode and optionally assert a specific mode.

    Parameters
    ----------
    h5_path : str
        Path to HDF5 file.
    expect : str, optional
        If given ('data' or 'model'), raise if the stored mode differs.

    Returns
    -------
    str
        The stored normalization mode ('data' or 'model').

    Exceptions
    ----------
    RuntimeError
        If the attribute is missing or does not match `expect`.
    """
    with open_h5(h5_path, role="reader") as f:
        g = f["/HyperCube"]
        mode = str(g.attrs.get("norm.mode", "model")).lower()
    if expect and mode != expect:
        raise RuntimeError(
            f"Hypercube is in norm.mode='{mode}', but solver expects "
            f"'{expect}'. Convert first with convert_hypercube_norm(...)."
        )
    if mode not in ("data", "model"):
        raise RuntimeError(f"Unexpected norm.mode='{mode}'.")
    return mode

# ---------------------- Small pool utilities --------------------------

def _pool_ping() -> int:
    return 1

def _pool_ok(pool, timeout: float = 5.0) -> bool:
    """
    Returns True if a trivial task round-trips within `timeout`.
    If it times out or raises, the pool is considered unhealthy.
    """
    try:
        res = pool.apply_async(_pool_ping)
        return res.get(timeout=timeout) == 1
    except Exception:
        return False

def _worker_init(blas_threads: int) -> None:
    os.environ["OMP_NUM_THREADS"] = str(blas_threads)
    os.environ["OPENBLAS_NUM_THREADS"] = str(blas_threads)
    os.environ["MKL_NUM_THREADS"] = str(blas_threads)
    os.environ["NUMEXPR_NUM_THREADS"] = str(np.max((1, blas_threads // 2)))
    os.environ.setdefault("KMP_INIT_AT_FORK", "FALSE")

# ---------------------- Optional shift diagnostic ---------------------

def _xcorr_int_shift(a: np.ndarray, b: np.ndarray) -> int:
    """
    Integer-lag that best aligns b to a by full cross correlation.
    Positive → b is to the right.
    """
    aa = np.asarray(a, np.float64).ravel()
    bb = np.asarray(b, np.float64).ravel()
    n = int(aa.size)
    fa = np.fft.rfft(aa, n=2*n)
    fb = np.fft.rfft(bb, n=2*n)
    cc = np.fft.irfft(fa * np.conj(fb))
    j = int(np.argmax(cc))
    if j > n:
        j -= 2*n
    return j

# ------------------------------------------------------------------------------

def _choose_tiles_fair_spread(
    s_ranges: list[tuple[int, int]],
    k: int,
    seed: int = 12345,
) -> list[tuple[int, int]]:
    """
    Pick k tile ranges with a roughly uniform spread over the spatial index.

    This avoids brightness bias: it sorts by s0 and stratifies the list into k bins,
    selecting one tile per bin (random within each bin).

    Parameters
    ----------
    s_ranges
        List of (s0, s1) tile ranges.
    k
        Number of tiles to select.
    seed
        RNG seed.

    Returns
    -------
    list[tuple[int, int]]
        Selected tile ranges (k or fewer if s_ranges is smaller).
    """
    if k <= 0 or not s_ranges:
        return []
    if k >= len(s_ranges):
        return list(s_ranges)

    s_sorted = sorted(s_ranges, key=lambda t: int(t[0]))
    n = len(s_sorted)
    k = min(k, n)

    rng = np.random.default_rng(seed)
    edges = np.linspace(0, n, k + 1, dtype=int)

    out: list[tuple[int, int]] = []
    for i in range(k):
        a = int(edges[i])
        b = int(edges[i + 1])
        if b <= a:
            idx = a
        else:
            idx = int(rng.integers(a, b))
        out.append(s_sorted[idx])

    return out

# ------------------------------------------------------------------------------

def _safe_scalar_rmse(val: float, label: str) -> float:
    """
    Ensure a scalar RMSE used for epoch comparison is finite.

    If 'val' is NaN, Inf, or negative, print a warning and return +inf
    so that this value will never be considered an improvement over a
    finite best RMSE.
    """
    if not np.isfinite(val) or val < 0.0:
        print(
            f"[Kaczmarz-MP] WARNING: {label} RMSE={val!r} is non-finite "
            f"or negative; treating as +inf.",
            flush=True,
        )
        return float("inf")
    return float(val)

# ------------------------------------------------------------------------------

def rmse_proxy_subset(
    h5_path,
    x_CP,
    tile_ranges,
    keep_idx,
    inv_cp_flux_ref,
    w_lam_sqrt,
):
    ssq = 0.0
    n = 0

    with open_h5(h5_path, role="reader") as f:
        DC = f["/DataCube"]
        M  = f["/HyperCube/models"]

        for (s0, s1) in tile_ranges:
            Y = np.asarray(DC[s0:s1, :], np.float64)
            if keep_idx is not None:
                Y = Y[:, keep_idx]

            yhat = np.zeros_like(Y)
            for c in range(x_CP.shape[0]):
                A = np.asarray(M[s0:s1, c, :, :], np.float32)
                if keep_idx is not None:
                    A = A[:, :, keep_idx]
                if inv_cp_flux_ref is not None:
                    A = A * inv_cp_flux_ref[c, :][None, :, None]

                yhat += np.tensordot(x_CP[c, :], A, axes=(0, 1))

            R = Y - yhat
            if w_lam_sqrt is not None:
                R = R * w_lam_sqrt

            ssq += float(np.sum(R * R))
            n   += int(R.size)

    return np.sqrt(ssq / max(n, 1))

# ------------------------------------------------------------------------------

def _tiny_col_freeze_inplace(col_denom, grad, rel_zero, abs_zero):
    """
    Freeze numerically tiny / unsupported columns for a given tile-band.

    Parameters
    ----------
    col_denom : (P,) float array
        Per-column denominator computed on the SAME weighted/scaled A used for
        the gradient (e.g. after cp_flux_ref scaling and λ-weighting).
    grad : (P,) float array
        Per-column gradient/numerator (same space as col_denom).
    rel_zero : float
        Relative threshold multiplier applied to median positive denom.
    abs_zero : float
        Absolute floor threshold.
    """
    # robust median on strictly-positive finite entries
    good = np.isfinite(col_denom) & (col_denom > 0.0)
    if np.any(good):
        med_energy = float(np.median(col_denom[good]))
    else:
        med_energy = 0.0

    tiny_col = float(max(abs_zero, rel_zero * med_energy))

    # freeze non-finite or tiny denom
    freeze = (~np.isfinite(col_denom)) | (col_denom <= tiny_col)
    if np.any(freeze):
        grad[freeze] = 0.0
        col_denom[freeze] = np.inf

    return freeze, tiny_col, med_energy

# ---------------------------- Worker ---------------------------------

def _worker_tile_global_grad_band(args):
    r"""
    Compute global Kaczmarz-style gradient contributions for one spatial
    tile and one contiguous band of components.

    This worker does **not** update x or R. It only returns the
    band-local contributions to the global gradient and diagonal
    preconditioner:

        g_band[c,p]  = sum_{s,λ} (√w_λ A_{s,c,p,λ}) * (√w_λ R_{s,λ})
        D_band[c,p]  = sum_{s,λ} (√w_λ A_{s,c,p,λ})^2,

    where R = Y - ŷ is the residual for the current epoch's global x_CP,
    and A already includes any column-flux normalization in the "model"
    basis.

    Additionally, this worker can apply a tile-local "tiny-column freeze"
    safety guard: columns with numerically tiny denom are zeroed in the
    numerator and excluded from the denom accumulator for this tile. This
    prevents large/noisy updates later when the coordinator forms
    invD = 1/max(D, eps).

    Control knobs (env)
    -------------------
    CUBEFIT_ZERO_COL_FREEZE : {0,1} (default 1)
        Enable/disable tile-local freeze.
    CUBEFIT_ZERO_COL_REL : float (default 1e-12)
    CUBEFIT_ZERO_COL_ABS : float (default 1e-24)

    Returns
    -------
    g_band : ndarray, shape (band_size, P), float64
    D_band : ndarray, shape (band_size, P), float64
    """
    (h5_path, s0, s1, keep_idx,
     c_start, c_stop,
     x_band,
     R_tile,
     w_lam_sqrt,
     inv_ref_band,
     dset_slots, dset_bytes, dset_w0) = args

    eps = float(os.environ.get("CUBEFIT_EPS", "1e-12"))

    freeze_enable = os.environ.get(
        "CUBEFIT_ZERO_COL_FREEZE", "1"
    ).lower() not in ("0", "false", "no", "off")
    rel_zero = float(os.environ.get("CUBEFIT_ZERO_COL_REL", "1e-12"))
    abs_zero = float(os.environ.get("CUBEFIT_ZERO_COL_ABS", "1e-24"))

    # Shapes
    R_tile = np.asarray(R_tile, dtype=np.float64, order="C")
    Sblk, Lk = R_tile.shape

    # λ-weights (optional)
    if w_lam_sqrt is not None:
        wvec = np.asarray(w_lam_sqrt, np.float64).ravel()
        if wvec.size != Lk:
            raise RuntimeError(
                f"_worker_tile_global_grad_band: w_lam_sqrt length "
                f"{wvec.size} != Lk={Lk}"
            )
        Rw = R_tile * wvec[None, :]
    else:
        wvec = None
        Rw = R_tile

    band_size, P = x_band.shape

    g_band = np.zeros((band_size, P), dtype=np.float64)
    D_band = np.zeros((band_size, P), dtype=np.float64)

    with open_h5(h5_path, role="reader") as f:
        M = f["/HyperCube/models"]  # (S, C, P, L)
        try:
            M.id.set_chunk_cache(dset_slots, dset_bytes, dset_w0)
        except Exception:
            pass

        for bi, c in enumerate(range(c_start, c_stop)):
            A = np.asarray(
                M[s0:s1, c, :, :], dtype=np.float32, order="C"
            )  # (Sblk, P, L_full)
            if keep_idx is not None:
                A = A[:, :, keep_idx]  # (Sblk, P, Lk)

            # Apply column-flux normalization if present
            if inv_ref_band is not None:
                inv_ref = np.asarray(
                    inv_ref_band[bi, :], dtype=np.float64, copy=False
                )  # (P,)
                A = A * inv_ref[None, :, None]

            # Promote to float64 for safe dot products
            A = A.astype(np.float64, copy=False)

            if wvec is not None:
                A_w = A * wvec[None, None, :]  # (Sblk, P, Lk)
            else:
                A_w = A

            # g_row[p] = Σ_{s,λ} A_w[s,p,λ] * Rw[s,λ]
            g_row = np.tensordot(A_w, Rw, axes=([0, 2], [0, 1]))

            # D_row[p] = Σ_{s,λ} (A_w[s,p,λ])^2
            D_row = np.sum(np.square(A_w, dtype=np.float64), axis=(0, 2))

            # ---- Tiny-column freeze (tile-local) -------------------------
            # Use the existing helper. It sets denom[frozen]=inf; for a
            # global accumulator we instead want to *exclude* this tile’s
            # contribution, so we revert denom[frozen] -> 0 before storing.
            if freeze_enable:
                freeze, _, _ = _tiny_col_freeze_inplace(
                    D_row, g_row, rel_zero, abs_zero
                )
                if np.any(freeze):
                    D_row[freeze] = 0.0

            g_band[bi, :] = g_row
            D_band[bi, :] = D_row

    # Numerical guards
    if not np.all(np.isfinite(g_band)):
        g_band = np.nan_to_num(
            g_band, nan=0.0, posinf=0.0, neginf=0.0, copy=False
        )

    # For D: keep zeros (they are meaningful), but sanitize NaN/Inf.
    if not np.all(np.isfinite(D_band)):
        D_band = np.nan_to_num(
            D_band, nan=0.0, posinf=0.0, neginf=0.0, copy=False
        )

    # Ensure no negative denom from numerical noise
    np.maximum(D_band, 0.0, out=D_band)

    # Optional hard floor (only if you later invert without g-masking);
    # here we do NOT force a floor, because g has been frozen where denom
    # is tiny/invalid.
    if not np.isfinite(eps) or eps <= 0.0:
        eps = 1e-12

    return g_band, D_band

# ------------------------------------------------------------------------------

def _canon_orbit_weights(h5_path: str,
                         orbit_weights,
                         C: int,
                         P: int) -> np.ndarray | None:
    """
    Return a (C,) float64 prior vector for components, or None if unavailable.
    Accepts:
      - orbit_weights == None: try reading '/CompWeights' from HDF5.
      - orbit_weights shape == (C,): use as-is.
      - orbit_weights shape == (C*P,): sum over populations -> (C,).
    Raises if a provided vector has incompatible size.
    """
    w = None
    if orbit_weights is not None:
        w = np.asarray(orbit_weights, dtype=np.float64).ravel(order="C")
    else:
        with open_h5(h5_path, role="reader") as f:
            if "/CompWeights" in f:
                w = np.asarray(f["/CompWeights"][...], dtype=np.float64).ravel(order="C")
            else:
                return None  # no prior available

    if w.size == C:
        pass
    elif w.size == C * P:
        w = w.reshape(C, P).sum(axis=1)
    else:
        raise ValueError(f"orbit_weights length {w.size} incompatible with C={C}, P={P}. "
                         f"Expected C or C*P.")
    # normalize to a comparable scale (optional, keeps magnitudes sane)
    s = np.sum(w)
    if np.isfinite(s) and s > 0.0:
        w = w / s
    return w

# ------------------------------------------------------------------------------

def softbox_params_smooth(eq: int, E: int) -> tuple[float, float]:
    """
    Cosine ramp starting at epoch 2 (1-based):
      eq = 1 → (band, step) = (0.30, 0.20)
      eq = E → (band, step) = (0.15, 0.30)
      2..E ramps smoothly between the two.
    """
    eq = int(eq)
    E  = int(max(2, E))

    if eq <= 1:
        return 0.30, 0.20

    # t=0 at eq=2, t=1 at eq=E
    t = np.clip((eq - 2) / max(1, (E - 2)), 0.0, 1.0)
    s = 0.5 - 0.5 * np.cos(np.pi * t)

    band = (1.0 - s) * 0.30 + s * 0.15   # 0.30 → 0.15
    step = (1.0 - s) * 0.20 + s * 0.30   # 0.20 → 0.30
    return float(band), float(step)

# ------------------------------------------------------------------------------

def solve_global_kaczmarz_global_step_mp(
    h5_path: str,
    cfg: MPConfig,
    *,
    orbit_weights: Optional[np.ndarray] = None,
    x0: Optional[np.ndarray] = None,
    tracker: Optional[object] = None,
) -> tuple[np.ndarray, dict]:
    """
    SPG-class global NNLS solver with orbit-weight projection.

    This is a diagonal-preconditioned Spectral Projected Gradient method.
    It converges to the global NNLS optimum (for fixed templates) and
    enforces spaxel-independent orbit weights via a projection step.
    """

    t0 = time.perf_counter()

    # ------------------------------------------------------------
    # Load cube metadata
    # ------------------------------------------------------------
    with open_h5(h5_path, role="reader") as f:
        DC = f["/DataCube"]
        M  = f["/HyperCube/models"]

        S, L = map(int, DC.shape)
        _, C, P, Lm = map(int, M.shape)
        if Lm != L:
            raise RuntimeError("Model / data wavelength mismatch")

        mask = cu._get_mask(f) if cfg.apply_mask else None
        keep_idx = np.flatnonzero(mask) if mask is not None else None
        Lk = int(keep_idx.size) if keep_idx is not None else L

        s_tile = int(M.chunks[0]) if (M.chunks and M.chunks[0] > 0) else 128
        if cfg.s_tile_override is not None:
            s_tile = int(cfg.s_tile_override)

    s_ranges = [(s0, min(S, s0 + s_tile)) for s0 in range(0, S, s_tile)]

    print(
        f"[SPG] S={S}, L={L} (kept {Lk}), C={C}, P={P}, "
        f"s_tile={s_tile}, epochs={cfg.epochs}, lr0={cfg.lr}",
        flush=True,
    )

    # ------------------------------------------------------------
    # Global ||Y|| for trust region (compute once)
    # ------------------------------------------------------------
    Y_glob_norm2 = 0.0
    with open_h5(h5_path, role="reader") as f:
        DC = f["/DataCube"]
        for (s0, s1) in s_ranges:
            Yt = np.asarray(DC[s0:s1, :], np.float64, order="C")
            if keep_idx is not None:
                Yt = Yt[:, keep_idx]
            # guard against NaN/Inf
            if not np.all(np.isfinite(Yt)):
                Yt = np.nan_to_num(
                    Yt, nan=0.0, posinf=0.0, neginf=0.0, copy=False
                )
            Y_glob_norm2 += float(np.sum(Yt * Yt))

    Y_glob_norm = float(np.sqrt(Y_glob_norm2))
    print(
        f"[SPG] Global ||Y|| = {Y_glob_norm:.6e}",
        flush=True,
    )

    # ------------------------------------------------------------
    # λ-weights (optional)
    # ------------------------------------------------------------
    w_lam_sqrt = None
    if os.environ.get("CUBEFIT_LAMBDA_WEIGHTS_ENABLE", "1").lower() not in (
        "0", "false", "no", "off"
    ):
        try:
            w_full = cu.read_lambda_weights(h5_path)
            w_use = w_full[keep_idx] if keep_idx is not None else w_full
            w_lam_sqrt = np.sqrt(np.maximum(w_use, 1e-6)).astype(np.float64)
            print("[SPG] λ-weights enabled.", flush=True)
        except Exception:
            w_lam_sqrt = None
            print("[SPG] λ-weights unavailable; unweighted LS.", flush=True)

    # ------------------------------------------------------------
    # Global column energy (for preconditioning + orbit projection)
    # ------------------------------------------------------------
    E_global = read_global_column_energy(h5_path)  # (C,P)

    # ------------------------------------------------------------
    # Orbit-weight prior (spaxel independent)
    # ------------------------------------------------------------
    w_target = None
    orbit_beta = float(os.environ.get("CUBEFIT_ORBIT_BETA", "0.2"))
    if orbit_weights is not None:
        w_target = _canon_orbit_weights(h5_path, orbit_weights, C=C, P=P)
        print("[SPG] Orbit-weight projection enabled.", flush=True)

    # ------------------------------------------------------------
    # Initialise x
    # ------------------------------------------------------------
    if x0 is None:
        x = np.zeros((C, P), dtype=np.float64)
    else:
        x0 = np.asarray(x0, np.float64).ravel()
        if x0.size != C * P:
            raise ValueError("x0 has wrong size")
        x = x0.reshape(C, P).copy()

    if cfg.project_nonneg:
        np.maximum(x, 0.0, out=x)

    # ------------------------------------------------------------
    # Multiprocessing bands
    # ------------------------------------------------------------
    nprocs_req = max(1, int(cfg.processes))
    band_size = int(np.ceil(C / nprocs_req))
    bands = []
    c0 = 0
    for _ in range(nprocs_req):
        c1 = min(C, c0 + band_size)
        if c1 > c0:
            bands.append((c0, c1))
        c0 = c1

    use_pool = len(bands) > 1
    pool = None
    if use_pool:
        ctx = mp.get_context(
            os.environ.get("CUBEFIT_MP_CTX", "forkserver")
        )
        pool = ctx.Pool(
            processes=len(bands),
            initializer=_worker_init,
            initargs=(int(cfg.blas_threads),),
        )

    # ------------------------------------------------------------
    # SPG bookkeeping
    # ------------------------------------------------------------
    eps = float(os.environ.get("CUBEFIT_EPS", "1e-12"))
    lr = float(cfg.lr)

    x_prev = None
    g_prev = None

    best_x = x.copy()
    best_proxy = np.inf

    # --- Active set bookkeeping ---
    active_orbits = np.arange(C, dtype=np.int32)   # start fully active
    min_active = int(os.environ.get("CUBEFIT_MIN_ACTIVE_ORBITS", "8"))


    # ============================================================
    # Main epochs
    # ============================================================
    try:
        for ep in range(cfg.epochs):
            g_tot = np.zeros_like(x)
            D_tot = np.zeros_like(x)
            ssq = 0.0
            nres = 0

            pbar = tqdm(
                total=len(s_ranges),
                desc=f"[SPG] epoch {ep+1}/{cfg.epochs}",
                mininterval=2.0,
                dynamic_ncols=True,
            )

            # ---------------- Gradient accumulation ----------------
            for (s0, s1) in s_ranges:
                with open_h5(h5_path, role="reader") as f:
                    DC = f["/DataCube"]
                    M  = f["/HyperCube/models"]

                    Y = np.asarray(DC[s0:s1, :], np.float64)
                    if keep_idx is not None:
                        Y = Y[:, keep_idx]

                    # exact model prediction
                    yhat = np.zeros_like(Y)
                    for c in active_orbits:
                        A = np.asarray(M[s0:s1, c, :, :], np.float32)
                        if keep_idx is not None:
                            A = A[:, :, keep_idx]
                        yhat += np.tensordot(x[c], A, axes=(0, 1))

                    R = Y - yhat
                    if not np.all(np.isfinite(R)):
                        R = np.nan_to_num(R, copy=False)

                ssq += float(np.sum(R * R))
                nres += int(R.size)

                jobs = []
                for (c_start, c_stop) in bands:
                    mask = (active_orbits >= c_start) & (active_orbits < c_stop)
                    if not np.any(mask):
                        continue
                    sel = active_orbits[mask]
                    x_band = x[sel, :].copy(order="C")

                    jobs.append(
                        (
                            h5_path,
                            int(s0),
                            int(s1),
                            keep_idx,
                            sel[0], sel[-1] + 1,   # band range
                            x_band,
                            R.copy(order="C"),
                            w_lam_sqrt,
                            None,
                            cfg.dset_slots,
                            cfg.dset_bytes,
                            cfg.dset_w0,
                        )
                    )

                if use_pool:
                    results = pool.map(_worker_tile_global_grad_band, jobs)
                else:
                    results = [_worker_tile_global_grad_band(j) for j in jobs]

                for (c_start, c_stop), (g_band, D_band) in zip(bands, results):
                    g_tot[c_start:c_stop] += g_band
                    D_tot[c_start:c_stop] += D_band

                pbar.update(1)

            pbar.close()

            # ---------------- SPG step ----------------
            rmse_proxy = np.sqrt(ssq / max(nres, 1))
            if rmse_proxy < best_proxy:
                best_proxy = rmse_proxy
                best_x = x.copy()

            # diagonal preconditioner
            abs_zero = float(os.environ.get("CUBEFIT_ZERO_COL_ABS", "1e-24"))
            denom_floor_frac = float(os.environ.get("CUBEFIT_DENOM_FLOOR_FRAC",
                "1e-6"))
            D_pos = D_tot[D_tot > 0.0]
            D_scale = np.percentile(D_pos, 90.0) if D_pos.size else 1.0
            D_floor = max(abs_zero, denom_floor_frac * D_scale)
            D = np.maximum(D_tot, D_floor)

            g = g_tot

            # BB spectral step length
            if x_prev is not None and g_prev is not None:
                s = x - x_prev
                y = g - g_prev
                sy = float(np.sum(s * y))
                if sy > 0.0:
                    lr = float(np.sum(s * s) / sy)
                    lr = np.clip(lr, 1e-6, cfg.lr)
                else:
                    lr = cfg.lr

            # gradient descent step
            x_prev = x.copy()
            g_prev = g.copy()

            dx = -lr * (g / D)
            # ---- GLOBAL TRUST REGION ----
            tau_global = float(os.environ.get("CUBEFIT_GLOBAL_TAU", "0.5"))
            if E_global is not None and Y_glob_norm > 0.0:
                step_energy = float(np.sum((dx * dx) * E_global))
                cap = float(tau_global * Y_glob_norm)

                if step_energy > cap * cap:
                    scale = cap / max(np.sqrt(step_energy), 1.0e-12)
                    dx *= scale

            x += dx
            # projection: nonnegativity
            if cfg.project_nonneg:
                np.maximum(x, 0.0, out=x)

            # projection: orbit-weight prior
            if w_target is not None and orbit_beta > 0.0:
                cu.project_to_component_weights(
                    x,
                    w_target,
                    E_cp=E_global,
                    beta=orbit_beta,
                    minw=1e-12,
                )

            # ------------------------------------------------------------
            # Update active orbit set (NNLS-safe)
            # ------------------------------------------------------------
            x_row_l1 = np.sum(x, axis=1)
            g_row_inf = np.max(np.abs(g), axis=1)

            x_scale = max(float(np.sum(x_row_l1)), 1.0)
            g_scale = max(float(np.max(g_row_inf)), 1.0)

            eps_mass = 1e-12 * x_scale
            eps_grad = 1e-10 * g_scale

            new_active = np.nonzero(
                (x_row_l1 > eps_mass) | (g_row_inf > eps_grad)
            )[0]

            # Always keep a minimum number to avoid pathological empty sets
            if new_active.size < min_active:
                topg = np.argsort(g_row_inf)[-min_active:]
                new_active = np.unique(np.concatenate([new_active, topg]))

            active_orbits = new_active.astype(np.int32)

            print(
                f"[SPG] active orbits: {active_orbits.size}/{C}",
                flush=True,
            )

            if tracker is not None:
                tracker.maybe_snapshot_x(
                    x, epoch=ep + 1, rmse=rmse_proxy, force=True
                )

            print(
                f"[SPG] epoch {ep+1} RMSE(proxy)={rmse_proxy:.4e} "
                f"lr={lr:.3e} ||x||={np.linalg.norm(x):.3e}",
                flush=True,
            )

        elapsed = time.perf_counter() - t0
        return best_x.ravel(order="C"), dict(
            epochs=cfg.epochs,
            elapsed_sec=elapsed,
            rmse_proxy_best=float(best_proxy),
        )

    finally:
        if pool is not None:
            pool.close()
            pool.join()

# ------------------------------------------------------------------------------

def probe_kaczmarz_tile(
    h5_path: str,
    s0: int | None = None,
    s1: int | None = None,
    c: int | None = None,
    lr: float = 0.25,
    x_source: str = "auto",   # "auto" | "zeros"
    project_nonneg: bool = True,
):
    """
    Single-band probe that mirrors the worker math on one component.
    Uses the same λ-weighting and global energy blend, so scale matches.
    """

    bt_steps   = int(np.max((0, int(os.environ.get("CUBEFIT_BT_STEPS", "3")))))
    bt_factor  = float(os.environ.get("CUBEFIT_BT_FACTOR", "0.5"))
    tau_trust  = float(os.environ.get("CUBEFIT_TRUST_TAU", "0.7"))
    eps        = float(os.environ.get("CUBEFIT_EPS", "1e-12"))
    rel_zero   = float(os.environ.get("CUBEFIT_ZERO_COL_REL", "1e-12"))
    abs_zero   = float(os.environ.get("CUBEFIT_ZERO_COL_ABS", "1e-24"))
    tau_global = float(os.environ.get("CUBEFIT_GLOBAL_TAU", "0.5"))
    beta_blend = float(os.environ.get("CUBEFIT_GLOBAL_ENERGY_BLEND", "1e-2"))

    with h5py.File(h5_path, "r") as f:
        M  = f["/HyperCube/models"]  # (S,C,P,L)
        DC = f["/DataCube"]          # (S,L)
        S, Ctot, P, L = map(int, M.shape)
        chunks = M.chunks or (S, 1, P, L)
        S_chunk = int(chunks[0])

        if s0 is None or s1 is None:
            s0 = 0
            s1 = int(np.min((S, S_chunk)))
        if c is None:
            c = int(Ctot // 2)

        keep_idx = None
        if "/Mask" in f:
            m = np.asarray(f["/Mask"][...], bool).ravel()
            keep_idx = np.flatnonzero(m)
        Lk = int(L if keep_idx is None else keep_idx.size)

        # x source
        if x_source == "auto" and "/X_global" in f:
            x1d = np.asarray(f["/X_global"][...], np.float64, order="C")
            x_CP = x1d.reshape(Ctot, P)
        else:
            x_CP = np.zeros((Ctot, P), np.float64)

        # Y (tile), global ||Y||
        Y = np.asarray(DC[s0:s1, :], np.float64, order="C")
        if keep_idx is not None:
            Y = Y[:, keep_idx]  # (Sblk, Lk)
        Sblk = int(s1 - s0)

        Yglob2 = 0.0
        for t0 in range(0, S, S_chunk):
            t1 = int(np.min((S, t0 + S_chunk)))
            Yt = np.asarray(DC[t0:t1, :], np.float64, order="C")
            if keep_idx is not None:
                Yt = Yt[:, keep_idx]
            Yglob2 += float(np.sum(Yt * Yt))
        Y_glob_norm = float(np.sqrt(Yglob2))

        # yhat (tile) exactly like the solver
        yhat = np.zeros((Sblk, Lk), np.float64)
        for cc in range(Ctot):
            A_cc = np.asarray(M[s0:s1, cc, :, :], np.float32, order="C")
            if keep_idx is not None:
                A_cc = A_cc[:, :, keep_idx]
            xc = x_CP[cc, :].astype(np.float64, copy=False)
            for s in range(Sblk):
                yhat[s, :] += xc @ A_cc[s, :, :]

        R = Y - yhat

        # ---- worker-like band update on component c ----
        A = np.asarray(M[s0:s1, c, :, :], np.float32, order="C")
        if keep_idx is not None:
            A = A[:, :, keep_idx]  # (Sblk, P, Lk)
        cp_flux_ref = cu._ensure_cp_flux_ref(h5_path, keep_idx=None if keep_idx is None else np.arange(L)[keep_idx])
        A = A * (1.0 / cp_flux_ref[int(c), :])[None, :, None]

        # sanitize
        badR = ~np.isfinite(R); R[badR] = 0.0
        badA = ~np.isfinite(A); A[badA] = 0.0

        # λ-weights (mirror main solver)
        lamw_enable = os.environ.get(
            "CUBEFIT_LAMBDA_WEIGHTS_ENABLE", "1"
        ).lower() not in ("0", "false", "no", "off")
        if lamw_enable and "/HyperCube/lambda_weights" in f:
            w_full = np.asarray(f["/HyperCube/lambda_weights"][...],
                                np.float64)
            if keep_idx is not None:
                w_lam_sqrt = np.sqrt(np.maximum(w_full[keep_idx], 1e-6))
            else:
                w_lam_sqrt = np.sqrt(np.maximum(w_full, 1e-6))
        else:
            w_lam_sqrt = None

        # gradient (weighted)
        if w_lam_sqrt is not None:
            A_w = A * w_lam_sqrt[None, None, :]
            Rw  = R * w_lam_sqrt[None, :]
        else:
            A_w = A; Rw = R

        g = np.zeros((P,), np.float64)
        for s in range(Sblk):
            g += A_w[s, :, :].astype(np.float64, copy=False) @ Rw[s, :]

        # local per-column denom (weighted)
        col_denom = np.sum(np.square(A_w, dtype=np.float64), axis=(0, 2))

        # freeze near-zero columns (tile-local)
        med_energy = float(np.median(col_denom[col_denom > 0])) if np.any(col_denom > 0) else 0.0
        tiny_col = np.max((abs_zero, rel_zero * med_energy))
        freeze = col_denom <= tiny_col
        if np.any(freeze):
            g[freeze] = 0.0
            col_denom = np.where(freeze, np.inf, col_denom)

        # --- global energy blend
        E_global = read_global_column_energy(h5_path)  # (C,P)
        Eg_row = np.asarray(E_global[int(c), :], np.float64)  # (P,)
        col_denom = np.maximum(col_denom, float(beta_blend) * Eg_row)

        invD = 1.0 / np.maximum(col_denom, eps)
        dx_c = float(lr) * (g * invD)  # (P,)

        # ΔR for alpha=1 (unweighted)
        R_delta = np.zeros((Sblk, Lk), np.float64)
        for s in range(Sblk):
            R_delta[s, :] -= (
                A[s, :, :].astype(np.float64, copy=False).T @ dx_c
            )

        # trust region (tile, weighted)
        if w_lam_sqrt is not None:
            Rw_delta = R_delta * w_lam_sqrt[None, :]
            rn = float(np.linalg.norm(R * w_lam_sqrt[None, :]))
        else:
            Rw_delta = R_delta
            rn = float(np.linalg.norm(R))
        rd = float(np.linalg.norm(Rw_delta))
        alpha_max = 1.0 if rd == 0.0 else min(1.0, (tau_trust * rn) / rd)

        # backtracking
        alpha = alpha_max
        def _rmse_w(MAT):  # weighted RMSE helper
            if w_lam_sqrt is None:
                return float(np.sqrt(np.mean(MAT * MAT)))
            Z = MAT * w_lam_sqrt[None, :]
            return float(np.sqrt(np.mean(Z * Z)))

        rmse_before = _rmse_w(R)
        rmse_after  = _rmse_w(R + alpha * R_delta)
        if not (rmse_after < rmse_before):
            a = alpha
            for _ in range(bt_steps):
                a *= bt_factor
                if a <= 0.0:
                    break
                rmse_after = _rmse_w(R + a * R_delta)
                if rmse_after < rmse_before:
                    alpha = a
                    break
            else:
                alpha = a

        # global cap
        upd_energy_sq = float(np.sum((dx_c.astype(np.float64) ** 2) * Eg_row))
        if (upd_energy_sq > 0.0) and (Y_glob_norm > 0.0):
            step_norm_global = float(np.sqrt(upd_energy_sq)) * alpha
            cap = float(tau_global * Y_glob_norm)
            if step_norm_global > cap:
                alpha *= float(np.minimum(1.0, cap / np.maximum(1e-12, step_norm_global)))

        dx_c *= alpha
        if project_nonneg:
            over_neg = dx_c < -x_CP[c, :]
            if np.any(over_neg):
                dx_c[over_neg] = -x_CP[c, :][over_neg]
                R_delta.fill(0.0)
                for s in range(Sblk):
                    R_delta[s, :] -= (
                        A[s, :, :].astype(np.float64, copy=False).T @ dx_c
                    )
        else:
            if alpha != 1.0:
                R_delta *= alpha

        R_after = R + R_delta
        yhat_norm = float(np.linalg.norm(yhat))
        yhat_next_norm = float(np.linalg.norm(yhat - R_delta))

        out = {
            "rmse_before": float(np.sqrt(np.mean(R * R))),
            "rmse_after":  float(np.sqrt(np.mean(R_after * R_after))),
            "y_norm":      float(np.linalg.norm(Y)),
            "yhat_norm":   yhat_norm,
            "yhat_next_norm": yhat_next_norm,
            "g_norm":      float(np.linalg.norm(g)),
            "dx_norm":     float(np.linalg.norm(dx_c)),
            "global_upd_norm": float(np.sqrt(np.maximum(0.0, upd_energy_sq)) * alpha),
            "Y_glob_norm": Y_glob_norm,
            "Sblk":        Sblk,
            "Lk":          Lk,
            "c":           int(c),
            "alpha":       float(alpha),
            "frozen_cols": int(np.count_nonzero(freeze)),
        }
        print("[Probe]", out)
        return out
</file>

</files>
